## SoRA的架构与发展

过去十年里，AIGC 领域经历了三波重大跃迁：

| 时代      | 主力技术               | 代表事件                                        |
| --------- | ---------------------- | ----------------------------------------------- |
| 2014-2021 | GAN（对抗生成）        | Deepfake 换脸登上头条                           |
| 2021-2023 | 图像扩散               | Stable Diffusion 把“AI 画图”带进民用            |
| 2023-至今 | 视频扩散 + Transformer | OpenAI 发布 **Sora**，演示 1080p、15 s 的长镜头 |

Sora 之所以引发轰动，关键是一次性解决了困扰视频生成多年的三件事：

1. **长时一致**——人物、镜头运动、光照在十几秒内都稳定；
2. **原生多分辨率**——同一模型可直接输出竖屏、宽银幕或 4 K；
3. **一步到位**——用户只见最终高清帧，看不到“模糊→清晰”的显影过程，体验像实时渲染。



### 文生视频的实现流派（从推理角度）

到目前为止，将文本转化为连续视频的方法大致可被划分为两大主流思路：

**流派一：单帧内容扩展（逐帧预测方法）**

第一种方法首先根据文字提示生成一张图片帧，然后模型在此基础上继续预测下一帧画面，如此反复，最终生成连续的视频序列。这种方式最典型的代表是基于Stable Diffusion延伸的AnimateDiff模型。

这种逐帧接力的方法看似直接且直观，但存在显著的缺陷：

- 首先，每一帧的生成往往只参考了“前一帧”，极其缺乏整体性的语义理解和视频内容贯穿能力，这容易导致长时间视频生成时出现前后风格突变、人物细节连续性不足、运动轨迹不稳定甚至失去逻辑的问题；
- 其次，由于只关注当前生成的相邻前帧，这方法只对局部画面有效，缺乏对全局概念或构图的全面规划能力，导致整体画面稳定性、流畅性有限；
- 再次，由于逐帧预测对视频整体语义联贯缺乏深层理解，它更适合一些简单的视频转换任务，比如风格迁移、视频滤镜或背景替换等场景，而不适合高质量长篇生成。

因此，“逐帧法”通常适用场景较为狭窄，表现力、稳定性不足，更适合作为辅助工具，而不是主力的视频生成方式。

**流派二：用完整一段视频进行整体训练和生成（全局视频建模方法）**

与“逐帧扩展”不同，第二种思路将一整段视频作为训练和生成的基本单元。具体来讲，这种思路是：

- 在训练时，模型每次读入一个完整视频片段（比如4秒长短的视频）以及对应的文本描述，迫使模型学到“由文本整体生成这一段视频”的能力；
- 代表工具有Runway、Pika等模型。他们能够从完整的一小段视频出发进行生成，并获得相比逐帧扩展更高的视觉质量和短时内的连贯表现；

但是，这种方法虽然比起逐帧方法更优，但仍然存在着明显局限：

- 首先，这些模型通常训练和生成的视频长度局限在约4秒片段上下，更长视频生成能力明显不足；
- 其次，由于是“短段训练和拼接”的方法，在生成较长视频时容易出现较明显的片段痕迹，前后段落之间衔接不流畅、动作和场景突变，整体连贯性并未彻底解决；
- 同时，它们的学习方式本质上仍然停留在模仿训练数据表面，无法深入理解真实世界和视频内容呈现的内在逻辑与协调性，知识泛化能力和普适场景能力有限。

**流派的最新进展与革新者——Sora的出现**

上述逐帧方法和短段视频方法都难以真正胜任高质量、长视频的连贯生成任务。在这个背景下，OpenAI的Sora模型出现了，它进一步创新了视频生成的主流范式：

Sora从根本上改变了以往的技术路线，采用了一种全新的整体化思路：

- 首先，它并不是单帧逐个预测，而是一次性同时看到整段视频的所有帧；
- 其次，通过时空Patch和潜空间的辅助，Sora把视频从一帧一帧单独预测变成整体视频预测，彻底消灭了逐帧预测的“不稳定性”和“不一致性”；
- 更重要的是，Sora融合了Transformer的强大能力，具有更深层的语义理解与前瞻规划能力，能够生成达到20秒。

Sora的出现标志着AI视频生成告别了传统逐帧预测或者简单片段拼接阶段，正式进入了真正的长时稳定、全局视频内容把控与高质量内容生成阶段。



### Sora 训练：“从散沙到视频的学习之旅”

建立高质量的视频生成模型，最开始关键的是训练阶段，它决定了后续模型能生成出什么水平的视频内容：

1. 视频数据收集与高质量清洗（获得干净的“原材料”）

- 首先，研究团队购买和收集了大量版权允许的原始视频素材。
- 接着，对视频进行筛选、去除严重抖动、模糊、压缩质量差、涉及敏感违规场景的片段。这一步相当于对原料“挑拣+清洗”。

2. GPT-4V自动生成详细字幕（文字“膨胀”）

- 视频本身标题往往非常笼统（如“海边的狗”），而模型训练需要更多细节。
- 利用强大的GPT-4V，自动生成一句或一段高质量详细描述，比如：“夕阳照耀下，一只浅金色金毛猎犬嘴里叼着紫色飞盘自在奔跑，海浪轻拍浅色沙滩，摄影镜头跟随狗微微抖动”。
- 这个过程确保文本和视频画面能严格对应起来。

3. 文本数字编码（文本Embedding）

- 将长长的详细描述，通过Tokenizer编码成为数值化的一维文本特征token序列，模型才能读懂。

4. 视频帧抽取并压缩成潜空间（Visual Encoder—Video-VAE）

- 把原始视频文件抽取成垂直帧序列。
- 利用视觉编码器（Video-VAE）技术，把大量高清视频数据压缩进一个低维、高抽象、体积更小的多维数字特征空间（潜空间）。
- 降低计算负担的同时仍然保留画面的大部分重要信息，就象把高清影片“浓缩成胶片”放进了盒子，大大节约了存储空间。

3. 潜空间切成小积木块（Spacetime Patches）

- 模型再把得到的大潜空间『切割成3维小方块』（如8×8像素、2～4帧每块），也叫Spacetime Patch。
- 这些小Patch再展开转换成一维向量（token）序列，以便模型处理。

3.1 各种辅助编码（位置+扩散步长）

- 给每个小方块token分配三维空间位置与时间位置的“坐标编码”，这样在转为一串序列后模型能区分。
- 再添加一个额外的扩散步长编码（告诉模型处于扩散的哪一步），以及其他必要的元数据信号。

4. Transformer + 扩散模型逐步去噪（真正训练的核心）

- 在训练阶段，每个数据都会被「人为增加随机噪声」。
- Transformer网络的学习目标：根据视频与文字，在模糊有噪时候逐个Patch去掉噪声，还原原本干净有序的数据。
- 模型在海量数据、反复迭代的训练中学习到如何从噪声逐渐过渡、一步步恢复真实视频的技巧。

最终，模型学会了从一团随机噪声的立方体逐步生成出连贯清晰的视频。



### Sora 推理：“如何从零开始，创造高清视频”

那么，你真的调用Sora来生成视频时，模型如何一步一步实现的呢？

① 用户输入文字提示（Prompt）

你向Sora提供明确的文字Prompt，告知你想创造怎样的视频（如：“傍晚海滩，一条金毛狗叼着飞盘奔跑。”）。

②文本处理成向量（与训练同款模式）

- 文本Prompt被转化成模型能理解的Token向量序列。

3. 随机初始化视频潜空间（非直接配对prompt、而是从乱中生）

- 起点并非是“画面已清晰的潜空间”，而是一个随机生成的小潜空间立方体。
- 开始时你看不到任何实质图像信息，只是一块随机杂乱数字、毫无逻辑、毫无意义的潜空间，就像一大堆杂乱灰色颗粒的“沙粒团”。

> 小知识: 为何从随机噪声开始？
> 扩散模型设计的本质就是学习从噪声恢复清晰画面的技巧，所以推理端也必须从噪声出发，模型一路去除噪声、逐渐显现视频内容。

4. 切分、转化Patch-Token序列

- 潜空间被切割为同训练方式的小立方体（Patch）。
- Patch同样被展平转为一维的向量序列 (Tokens)，并加入位置信息。

5. 逐步去噪、预测下一个Patch（最核心关键一步）

- 同训练时配置一致的Transformer扩散模型开始发挥作用：
  - Transformer 从头到尾遍历这段token序列；
  - 根据序列第一部分已经生成的内容，逐步预测下一个仍充满噪声的token应当如何修正。
- 每一次迭代预测，都帮助整个视频显现更多细节：运动方向、形状颜色逐渐明确；
- 模型并非一次到位，而是逐次迭代，不断地微调"下一个patch"，逐渐推进成为清晰画面。

实际推理大约需要重复20～30次去噪步骤后，视频潜空间才能从纯随机噪声逐渐演化成清晰的视频表现。

5. 潜空间整体解码

- 最终的清晰潜空间整体送入训练好的解码器（与训练编码过程相对应的Video-VAE的逆过程）。
- 最终解码输出清晰的RGB视频帧序列（真正的视频帧）。

6. 封装为可播放视频

- 把得到的帧序列再封装压缩为标准的视频格式（例如MP4）。
- 同时模型在视频数据上加入不可见的数字水印信息，便于平台维护版权或溯源。



- **训练** = 原视频→潜空间→切patch→token序列 → Transformer学会预测下一个patch去噪生成；
- **推理过程**正是这套过程的反向版：从随机潜空间噪声出发，Transformer一次次预测patch内容，由噪声逐步重构出清晰视频。

这就是Sora的“前世今生”和实践路径: 看似复杂，其实沿着“编码→切分→预测→解码” 一条主线进行，非常清晰且明确。
希望通过这篇详细Blog，你彻底掌握Sora背后的技术方法，真正理解了视频生成目前前沿工程技术背后的逻辑。

### 为什么需要“Patch（积木块）

- 首先是为了减小计算量，将高清视频压缩成小巧的、易处理的数字积木；
- 其次是方便Sora将图像数据转化为类似于语言系统中的tokens（词条），做到逻辑和框架的统一；
- 同时，一次性处理整段视频（全部积木）可以更好地保证画面的连贯与一致性，避免画面跳跃或者出现不合理的运动和变化。

总结：

- Sora的训练过程，就是从视频片段到压缩再到让系统学会去噪还原的过程；
- Sora的生成过程，就是系统基于你给的文字提示，从随机杂乱的状态，一步步预测并重建精美视频的过程。



Patch（积木块）可以理解为视频内容的一个小型表示单元。在 Sora 里，每个 Patch有这些特点：

1. 空间信息（Spatial Information）： 一个Patch覆盖视频画面里的一小部分区域，例如8x8或16x16的小方格区域。这意味着每个 Patch 只包含画面中某个具体小区域的信息，而不是整个图像。
2. 时间信息（Temporal Information）： 每个Patch不仅在空间上专注于画面的一个小区域，同时也跨越了若干帧（例如2到4帧）。也就是说，每一个小积木块囊括了连续的几帧图像。这帮助模型不仅理解每一小局部在空间上的细节，也捕捉视频在时间上的动感变化。
3. 数字信息（数值化表示）： 每个Patch本质上就是一系列数字组成的向量（Token），这个向量里保存着画面、动作、细节等多种特征信息。
4. 位置坐标（Positional Encoding）： 每一个 Patch 还额外附带一个位置信号（位置编码），它告诉模型当前Patch位于整个视频画面和时间轴的哪个具体位置。

简单来说，每个 Patch 就像是一个“立体积木块”，包含：

- 局部的画面细节（空间）
- 局部的动作变化（时间）
- 一组代表画面特征的数据数字（数字化特征向量）
- 一个清晰的位置编号，代表它在整段视频序列里的具体位置 (Positional Encoding)

而所有这些小块（Patch）合在一起，才能组成一整个清晰连贯的动态视频。



在 Sora 模型的训练过程中，Patch里面并不是直接存储普通“图片”或“帧”本身的像素画面。

准确来说，「Patch （小积木块）」里存储的东西是：

- 视频经过视觉编码器（visual encoder）压缩后的特征表示，也叫作「潜空间（Latent）」信息。
- 这些潜空间信息已经从原始的像素图像转化为一系列的数字特征。所以，这些Patch里是一些高度概括过的视频特征数据，而不是原始的图片像素帧。

换句话说：

- 一个Patch不是直接的“图像帧”（不是普通像素组成的图片）
- 它是被压缩和编码后的视频“特征”或“简化版”信息，相当于浓缩为更易处理的“数字特征表示”；
- 这些数字特征在训练时被进一步划分为小Patch（积木块），并标注清晰的位置编码，以便Transformer能够进行学习并预测；
- 在训练或生成结束之后，系统才会再使用解码器，把这些高度浓缩特征Patch恢复为清晰可视、真实视频序列。

所以，Patch里面的确没有“原始的图片帧”，而是压缩后的视频潜空间特征表现形式（以数字而非图像像素存在）。



Patch本身并不是直接包含一张张传统意义上的图片，它包含的是压缩和编码后的视频潜空间特征（想象成一堆高度抽象的数字）。要把这些patch变成一段持续的视频画面（也就是多个连续的图片），整个过程大白话来说就是：

第一步：将分散的patch拼回潜空间

你可以把每一个patch想象成拼图游戏里的“一小块拼图”，单独看它只是很局部的小信息，没法看清画面。

我们把许多patch按照它们原有的位置编码（也就是每个拼图块的位置信息）拼回它们原本应有的位置，就组合成一张完整的压缩后的“小拼图”图案（潜空间）。到这一步，我们仍然是在一个抽象又压缩的空间中，而非真实视频帧。

第二步：从潜空间整体解码为连续帧序列

我们再使用解码器（decoder），将刚刚拼好的潜空间的“小拼图”整体做一个“超分辨率放大”和“解码”过程。这一步就是：

- 将压缩的小尺寸潜空间逐步放大，还原细节。
- 将通道数转变并还原成标准的RGB三色通道，逐步形成可理解、可视的普通像素图片。
- 将每个时间点上的帧单独拿出来，就还原出了连续的视频画面序列，也就是连续的图片帧。

总结一下：

- 每个Patch是压缩后的视频局部“特征块”，本身不是完整图像，也没有单个完整的图片帧。
- 多个Patch按位置拼起来，组合成完整的小型“潜空间”。
- 最后将潜空间整体解码，恢复成高清晰可视的多个连续图片帧（视频）。

一句话讲清楚：

Patch本身就是数字表示，它只能间接通过「拼成潜空间 → 整体解码 → 恢复到实际帧图像」的方式，最终转化为持续的视频画面（多个图片）。



## Sora的长时一致和可变分辨率

一、什么叫“长时一致”？
想像你在看一部动画短片。
• 如果第一秒里狗是黄色的，第二秒忽然变成黑色，第三秒又变回黄色——这就“不一致”。
• 如果镜头往前移动，前一帧桌子在画面左边，下一帧桌子却瞬间跳到右边——这也“不一致”。
Sora 追求的“长时一致”，就是让同一个角色、同一束灯光、同一条运动轨迹，在 10 秒、15 秒甚至更长的片段里都连贯、稳定，像真人拍摄那样自然。

为什么要把所有操作都放在“潜空间 + patch”里完成？
因为模型一次性“看见”整段视频的所有积木块（patch），可以同时考虑开头、中段、结尾的关联。这样它在去噪时会自动维护：
• 角色外形、衣服颜色前后保持一致；
• 光影、天气、镜头运动平滑过渡；
• 物体不乱闪、不瞬移。

二、什么叫“可变分辨率、可变长宽比”？
做视频的同学经常遇到：
• 横屏 1920×1080 要发到电脑端；
• 竖屏 1080×1920 要发到短视频 App；
• 宽银幕 2048×858 要在影院放映；
传统模型往往只能固定输出一种分辨率，要换尺寸就得额外裁剪、上采样，甚至重新训练。

Sora 的做法是：

1. 训练时就把各种原生分辨率、各种横竖屏素材一起喂给模型；
2. 模型内部统一在低维潜空间里运算，维度很小，对显存友好；
3. 最后一步解码时，再按你指定的尺寸一次性放大到 720p、1080p、4K……都行。

结果就是：
• 你同一个 prompt，可以要一条横屏广告，也可以要一条竖屏短视频；
• 不必剪裁，不会因为强行变形而丢构图；
• 训练好的大模型不需要为每种分辨率各训一份，节省大量算力和维护成本。

一句话总结
• “长时一致”＝让角色、动作、光影在整个片段里前后不打架；
• “可变分辨率”＝同一套模型随时给你横屏/竖屏/宽银幕、720p/1080p/4K，都能直接生成，不用重新训练。



Sora把“数据、算力、表示方式、模型骨架、训练策略、安全机制”这几件看似分散的事做到了极致，并且让它们彼此配合得天衣无缝。下面拆成五条“独门秘籍”——每一条业内都有人做，但 Sora 把它们做到了极端：

1. 超大规模、高质量的视频-文本对齐数据
   • 不只是“量大”（数千万～上亿段真实视频），更是“质好”——先用 GPT-4V 把原本简短或噪声很大的标题重写成细节丰富、物理细节完备的 long caption。
   • 这种“先精修文本→再喂模型”的做法让视频和文字对齐度极高，模型在生成时更少跑偏，更能严格执行提示。
   • 额外的多模态过滤（NSFW、剧烈抖动、压缩伪影）保证训练分布干净，减少低质样本对长时连贯性的破坏。
2. 时空 Patch + 大型 Transformer 的“统一表示”
   • 先用 Video-VAE 把整段视频压到低维 latent，再切成 3D patch（空间 × 时间）。
   • 所有 patch 在序列里与文本 token 并列，「视频生成=预测下一个 patch」→ 逻辑完全复用大语言模型的成功范式。
   • Transformer 一次就能“看到”十几秒全部 patch，天然保持人物、镜头、光照的全局一致，避开逐帧方法的漂移问题。
3. 可变时长、可变分辨率、原生长宽比混训
   • 训练时把横屏、竖屏、宽银幕、720p～4K 统统保持原尺寸喂给模型，不裁方块。
   • 再加随机截断/拼接，让同一模型习惯 2 秒、4 秒、16 秒甚至半分钟 clip。
   • 推理端输入一句 prompt + 想要的时长/分辨率，模型一次采样直接给出目标尺寸——不用再套超分或二次拼接，画面边缘、取景都自然得多。
4. “堆到极致”的算力与工程
   • 华尔街日报披露 OpenAI 为 Sora 级项目采购了 >2 万块 H100；配合 Flash-Attention、DeepSpeed-ZeRO、8-bit 权重、KV-Cache 重排序等尖端并行技巧，真正把 Video-Transformer 训练规模推到百亿 token。
   • 同时做了重采样蒸馏和推理管线量化，生产环境依然能在几十、甚至十几步就出片，兼顾质量与速度。
5. 完整的安全与后期工具链
   • 训练末期插入 RLAIF / RLHF，降低暴力、政治、违法场景出现概率。
   • 推理侧自动植入数字水印、指纹，平台可溯源；还有实时检测 API 限制高危 prompt。
   • 同一套时空 patch 表示天生支持续帧、插帧、风格替换、局部编辑等高阶功能——这让 Sora 不只是“能生”，还是一个可编辑的视频工作站内核。



### 显影阶段的隐藏

早期的视频生成模型（例如 Progressive GAN、StyleGAN 系列）在生成过程中，会给观众呈现一种由模糊逐渐清晰的画面效果。这种现象的原因是：它们采用分层的生成方法，即开始生成时分辨率很低（比如 8×8），随着生成步骤不断前进，网络逐级上采样，图像精度提高，每一级都转换为可直接观看的RGB像素图像，因此用户肉眼能够看到由低分辨率（模糊）到高分辨率（清晰）的完整生成过程。

而Sora这种新一代的视频生成模型虽然内在也有类似的从“噪声”逐渐变到“清晰画面”的过程，但用户看到的现象却完全不同：Sora 模型并不直接在像素上逐层绘制，而是始终在一个被称为『潜空间（Latent）』的抽象数字空间里操作。这种潜空间可以看作是数据被压缩和抽象后的精华表示，不直接对应肉眼可见的像素。

Sora模型从一开始便在这种机器可读的数字空间里操作：潜空间被自动分割成一个个『时空小积木（Patch）』，模型通过 Transformer 技术逐步预测这些积木从随机噪声到逐渐清晰的具体内容。尽管这个过程同样经历了模糊的噪声逐步转化为清晰视觉的“潜在显影”阶段，但用户一般情况下并不会在屏幕上看到这个过程。因为只有当模型彻底完成了所有预测、达到最终一步后，这个潜空间才会一次性被整体“解码”到高清的像素空间，呈现最终完成的视频画面。

做个简单的生活化比喻：

- Progressive GAN、StyleGAN 就像在公开能看到的画板上绘画，艺术家从大致轮廓、简单色块逐渐细化成高清细致作品，因此观众能看清由模糊草稿到高清画面的整个变化过程。
- 而 Sora 模型则更像是一位“秘密雕塑师”，他在一个被封闭起来的雕塑盒子内部进行创作，外界根本看不到创作中的任何细节。只有在雕塑彻底完成、打磨光滑后，艺术家才会突然地、一次性地掀开盒子展现高清成品。因此观众始终只能看到最后成品，看不到模糊到细致变化的过程。

那么，为什么 Sora 要选择这种隐藏了显影阶段的方式呢？
主要是为了实现更好的长视频质量和高效性：

- **更稳定的长视频画面效果**：Sora 能一次性看到整个视频的所有关键时刻和整体潜空间表示，因此能同时处理每个画面的全局细节，使视频画面在很长一段时间内保持稳定连续。
- **更高的生成效率**：Sora无需在中途频繁转换为像素画面再返回潜空间，因此节省巨大的计算成本和显存开销，只需要在生成最后才将潜空间统一解码一次到高清画面即可。

总之，用户在Sora界面上是看不到由模糊到清晰这一“显影”过程的，但实际上模型内部依然具备从噪声逐渐清晰化的完整过程，只不过它都隐蔽地“藏在”了潜空间之中，实际部署时并不会展现给用户直接观看，用户所看到的始终是最后一次性生成的高清成片。