# SoRA的架构介绍

过去十年里，AIGC 领域经历了三波重大跃迁：

| 时代      | 主力技术               | 代表事件                                        |
| --------- | ---------------------- | ----------------------------------------------- |
| 2014-2021 | GAN（对抗生成）        | Deepfake 换脸登上头条                           |
| 2021-2023 | 图像扩散               | Stable Diffusion 把“AI 画图”带进民用            |
| 2023-至今 | 视频扩散 + Transformer | OpenAI 发布 **Sora**，演示 1080p、15 s 的长镜头 |

Sora 之所以引发轰动，关键是一次性解决了困扰视频生成多年的三件事：

1. **长时一致**——人物、镜头运动、光照在十几秒内都稳定；
2. **原生多分辨率**——同一模型可直接输出竖屏、宽银幕或 4 K；
3. **一步到位**——用户只见最终高清帧，看不到“模糊→清晰”的显影过程，体验像实时渲染。



## 文生视频的实现流派

将文本描述转化为视频内容，一直都是生成式 AI 研究的重要课题。到目前为止，经过业界的的诸多探索与实践，逐渐形成了两个主流的技术路线。

#### **流派一：逐帧扩展生成方法（代表：AnimateDiff）**

这一方法的基本思路非常直观：先根据用户描述的文本，一帧一帧地逐步生成视频。模型首先把文字 prompt 生成为第一帧画面，随后基于这一帧继续预测生成下一帧，依此类推，不断连续扩展，从而获得完整视频序列。比如 AnimateDiff（Stable Diffusion的衍生产品）是这一类方法的典型代表之一。

**这种逐帧扩展方法在实际应用中，存在明显并且很难克服的缺陷：**

- **帧与帧缺乏语义联系与整体结构理解**

  每个新生成的画面仅仅基于紧邻的前一帧进行预测，完全受限于局部的视觉信息。这种局部“看一眼前面”的机制，使得模型无法真正地从整体层面理解视频内容。长时间生成则必然会出现内容飘忽、动画漂移、人物与场景跳跃甚至风格骤变的问题。

- **整体连贯性与一致性不足，难以应对长时视频**

  方法本身并没有建模全局信息，也就很难生成更长的稳定视频。因此，对于超过几秒的视频片段，这种方法常常力不从心，表现力大幅减弱。

- **应用场景有限，通常作为辅助工具**

  由于模型在整体结构理解的不充分，这类方法更适合完成简单的任务，比如短视频生成、视频风格迁移、滤镜特效等应用场合。

整体而言，逐帧生成方法由于长时稳定性不足以及缺乏全局理解能力，目前一般作为较短视频或辅助创作的工具使用，不适合作为高质量的视频主要生成机制。

#### 流派二：整段短视频整体训练与生成方法（代表：Runway, Pika）

为了弥补逐帧扩展的不足，研究人员随后提出了一种全新思路：整段视频片段作为基本单元进行整体的训练与生成。

具体过程：

- 在训练时，每次输入模型的是一个完整的视频片段(一般在1到4秒范围内)，以及这个视频内容与之对应的文字描述，模型学习的任务则是由文本整体生成对应的一段画面；
- Runway 和 Pika 就属于此类架构的典型代表，他们提供了更好的局部连贯性与短时一致性表现，明显优于逐帧扩展方法。

**然而，整段视频方法同样存在明显的不足：**

- **视频长度受限，长期生成依旧困难**

  这类模型常见是以较短片段（一到四秒）训练，超出训练长度后就需要多个短视频片段相连拼接，因此始终难以真正解决长视频的生成问题。

- **长视频片段拼接痕迹严重**

  一旦超出单个训练片段长度时，前后拼接容易引发画风突变、动作衔接不连贯等问题，影响生成视频的专业感和观看体验。

- **对真实世界缺乏深入理解，仅停留在数据表面**

  模型只是被动地模仿训练数据所展示的表面特征，而未真正学会视频内容背后的逻辑与内在语义，因此泛化能力和真实世界场景下的应用能力仍然不够好。

#### 流派革命性突破者：OpenAI Sora 模型的出现

上述两种方法均难以真正有效应对高质量、长视频连贯生成的难题。在这个背景下，2023年出现的OpenAI Sora 模型则真正意义上为该领域带来了新的突破。

Sora是一种基于扩散模型（Diffusion Model）框架、融合Transformer架构的视频生成模型。与以往单纯逐帧扩展或短段拼接方法不同，Sora模型在架构和训练上实现了一次革命性的重新设计：

- **全新的整体建模方式**

  Sora 摒弃了逐帧预测或短段拼接的简单路线，采用一种崭新的“潜空间 + 整段视频 patch”整体生成机制。具体而言，Sora 将整段视频压缩到低维潜空间 latent 中，再进一步切分为三维时空 patch 单元，然后由 Transformer 模型在一次前向计算中处理所有这些时空 patch，实现真正的「整段视频全局生成」。

- **强大的 Transformer 模型架构（DiT：Diffusion Transformer）**

  传统 Stable Diffusion 是以卷积神经网络（U-Net）作为去噪核心，其结构对长时序全局信息捕捉有限。而 Sora 采用的是融合了 Transformer 的 DiT 架构，将 Transformer 强大的长距离信息捕捉能力应用于扩散去噪过程，更好地保证了视频十几秒到数十秒范围的生成连贯性与稳定性。

- **高效数据处理与跨越式计算效率提升**

  Sora 不再直接操作高维像素数据（原始RGB值），而是先经视觉编码器压缩到更加高效的潜空间 latent 中，这种方式大大减少了计算负担并显著提高生成效率和质量。

- **文本与视频内容的精细对齐与深度理解**

  模型训练还利用了 GPT-4V 进行高质量自动化文本描述（视频 recaption），每段视频都配以更准确、细致的文本标注，从而实现更深的跨模态语义理解，让生成的视频更加符合用户输入的真实意图。

- **强大的工程投入与技术集成**

  OpenAI Sora 背后的算力投入和技术基础都进行了大幅跃升，引入了新的并行策略、高效Transformer优化方法，数据量和计算投入远超以往扩散模型。这种规模的变化和架构优化，已经远远不止“简单地把U-Net换成Transformer”这么单一的修改，而是一次全面升级的重要里程碑。

因此，Sora 的到来，标志着视频生成技术真正迈向了高质量、可控、长时一致的视频整体生成阶段，真正开启了 AI 视频内容生产的新纪元。

综上所述，AI 视频生成领域经历了从逐帧扩展到整段视频生成的发展与沉淀。而 Sora 以其全新架构设计、有力的技术突破、跨层次的模型能力，为未来 AI 数字内容创作开启了全新的可能，也标志着视频生成领域步入了一次革命性的技术飞跃！

## Sora 训练：“从散沙到视频的学习之旅”

Sora 模型的本质架构是扩散模型（Diffusion Model），扩散模型是一种最近广泛流行的生成式AI训练方式。整个扩散模型的基本思想是：首先在干净的数据上人为地添加随机噪声，然后模型需要学习如何逐步地预测并移除这些噪声。经过大量这样的噪声和去噪训练，模型最后可以从纯噪声数据逐步恢复生成出非常逼真的图像或视频。

![images](https://github.com/xinyuwei-david/david-share/blob/master/Multimodal-Models/SoRA/images/2.png)

**Sora 的独特升级点：融合 Transformer 与扩散模型**

传统的扩散模型结构，如 Stable Diffusion，主要采用了卷积神经网络结构（UNet）来完成图像或视频数据的加噪和去噪过程。UNet 网络结构在图像处理领域广泛应用，优点在于高效稳定，结构和使用方法相对简单，但缺点也十分明显——卷积网络结构本质上无法有效捕捉长距离的依赖关系，它难以准确理解视频序列中跨越较长帧距之间的上下文关系和全局信息。

而 Sora 模型真正的独特之处，就在于用 “Diffusion Transformer” 取代了传统扩散模型里用于去噪的 UNet 网络结构，其本质是将扩散模型 (diffusion) 与 Transformer 模型的强大序列建模能力相结合，形成了一种崭新的、更适用于视频生成任务的扩散模型架构。

具体来说：

- **Sora 首先对视频内容进行一种特殊的处理**：将视频数据压缩并切割成一个个包含空间和时间信息的时空小单元 (spacetime patches)。
- **接着，这些时空 patch 被转化为 Transformer 能够方便处理的 token 序列**。随后，在训练过程中模型人为地给这些 token 序列添加了随机噪声，从而得到模糊或“污染”后的数据。
- **关键的一步：Sora 使用 Diffusion Transformer（DiT）来实施去噪**。具体而言，模型通过 Transformer 强大的长距信息捕捉能力，精准地对 token 序列中每个 patch 单元之间的关系建模，并一步步预测它们如何从模糊的状态变为清晰，逐步去除其中的噪声，直到恢复到理想的视频潜空间表示中。

**为什么用 DiT 代替传统 UNet？**

传统的 Stable Diffusion 模型（例如用于 AI 作画），一般采用 U-Net 网络实现扩散算法（即噪声空间中的去噪过程），U-Net 的优点是简单高效而且计算量较少。然而这种卷积网络架构在建模视频时遇到了明显的局限性——卷积自身缺乏获取序列性质的长距离、整体的上下文信息的能力，难以很好捕捉视频序列中的连续帧之间复杂的依赖关系。

与此相对，**DiT（Diffusion Transformer）是一种基于 Transformer 架构设计扩散模型的创新结构**，Transformer 架构最早应用于大语言模型（LLM），最重要的特征在于善于捕捉长序列内的全局、远距离依赖关系。这让 DiT 相比于 UNet，在处理视频内容时有非常明显的性能优势：

- 首先，Transformer 更擅长捕捉全局及时空持续的信息依赖关系，提升了视频时间轴的连贯性；
- 此外，Transformer 提升了模型对视频整体细节与上下文理解能力，例如视频中的人物动作、运动轨迹以及整体场景光照的稳定性。
- 这一创新使得 Sora 可以轻易处理超过十几秒甚至更长的视频内容生成任务，提升了视频长时一致性和整体质量。

**总结：**

"Stable Diffusion + 替换 UNet 为 DiT" 这句术语的核心含义简单清晰地说，就是 Sora 抛弃了传统采用卷积架构 (UNet) 实现扩散模型训练的方式，创新性地引入 Transformer 架构 (DiT)，大幅提升了模型捕捉视频长期依赖关系以及生成长时、全局连贯视频内容的能力。这也是 Sora 模型技术升级的核心要点之一。



#### 详细步骤

想要构建一个高质量的视频生成模型，更深层次的关键在于模型训练阶段，因为它严格决定了后续生成视频内容的效果如何。Sora模型在训练时，从数据处理到网络结构、从视频特征编码到Transformer扩散预测，每个环节都经过精细设定，完整的步骤如下所述：

------

1. 视频数据收集与高质量清洗（获得干净的“原材料”）

- 首先，通过专业渠道和版权许可，研究团队采集到了海量的原始视频素材，包含大范围的场景与细节。
- 紧接着，团队利用自动和人工方式逐个检查每段视频，对片段质量进行极高要求的筛选：去除画面严重抖动、模糊失焦、压缩严重损坏的视频，以及涉及敏感违规内容的片段。
- 通过严谨的数据筛选和清理，确保了模型训练使用的数据质量极高，就如同为后续建造工程准备最纯净的优质建筑材料。

------

2. GPT-4V自动生成详细字幕（文本标注的“精细化”与“膨胀”）

- 从原始素材自带的标题或简单说明，通常只是非常笼统的概念（比如只有“海边的狗”，无法表达任何细节）。
- 因此，引入了业界领先的GPT-4V视觉语言模型，对每段视频进行精确的分析与识别，以自动生成更加详细、完整的文本描述。比如会生成：“在夕阳映照的海边，一只浅金色金毛猎犬嘴里叼着紫色飞盘自由地奔跑，海浪轻柔地拍打着浅色的沙滩，镜头跟随狗的奔跑而微微晃动。”
- 这一过程，可以建立更精准的视频-文本内容匹配关系，有效提高后续模型训练的数据对齐度与精度。这好比为每个视频片段都制作了精确、详细的字幕注释，帮助模型实现更加细致和丰富的语义理解。

------

3. 文本Token化和数字编码（Embedding）

- 接下来，对GPT-4V生成的文本描述进行数字化处理，使用专业的Tokenizer技术，将详细文本长句切割成一连串数字形式的Token序列。
- 通过这些数字Token，模型才可以真正“理解”文本含义。这相当于为模型的每个视频提供了一份清晰的数字化文字“说明书”，便于模型的深度学习与语义映射。

------

4. 视频帧抽取与视觉编码压缩（Visual Encoder– Video-VAE）

- 所有原始高清视频文件首先被拆分成独立的连续垂直帧序列，以便后续技术的进一步处理。
- 然后，引入先进的视觉自编码器（Video-VAE）这一技术，能够高效地把视频帧从高维像素空间直接压缩为潜空间的低维度特征表示（latent representation）。
- 这个步骤有效地保留了视频的核心结构与信息，又极大降低了后续计算要求与复杂度。这类似于把复杂的视频内容“压制”成一种更高效、更紧凑的格式，帮助模型进行更高效的学习和训练。

------

5. 潜空间切分：从空间视觉压缩到时空Patch编码

- 得到压缩后的“视频胶片”（Latent特征）后，模型随后将这些Latent空间数据进一步细分成三维时空小方块（Spacetime Patches），每一块内包含空间信息（画面的局部区域）和时间信息（若干连续帧）。
- 模型将这些3维Patch的小方块每一个都转化成一种更易处理的一维向量（Transformer所需的Token序列），以便后续扩散处理。

![images](https://github.com/xinyuwei-david/david-share/blob/master/Multimodal-Models/SoRA/images/1.png)

- 这里的Patch切分技术思想，继承和参考了ViT(Vision Transformer)中的经典做法：
  - 首先，原始ViT提出将图片切分成有序的小Patch；
  - 接着，ViT将图像Patch展平为一维向量形式，再加入位置编码；
  - Sora在训练视频时，也采用了类似的概念：首先将3维时空数据以Patch方式组织，然后将其展平成一维序列模式，作为Transformer模型的输入。

------

6. 加入位置编码与扩散步长信息（时空位置嵌入）

- 每个Patch的向量数据需要明确有序的位置信息。因此，Sora模型会为每个时空patch分别添加明确的三维空间位置信息（空间中的x,y座标与时间轴t是必须的）。
- 同时，在标准位置编码之外，再额外加入一个扩散步长（timestep embedding）的信息，以明确告诉模型当前正处于扩散模型（Diffusion Model）处理的哪一步。

------

7. Transformer扩散去噪训练（Diffusion Transformer Training：训练核心）

- 在每个视频数据的训练过程中，模型都会人为加入一定强度的随机噪声，以模拟现实中非理想情况下的视频画面。
- 模型的训练目标：Transformer (这里采用的称为 DiT–Diffusion Transformer) 网络根据语义信息（文本）和视觉信息（视频Patch）逐步预测并去除Packet中的噪声，再逐步恢复还原出本来干净的视频数据。
- 模型在海量视频数据和丰富文本数据的深度融合训练中，逐步掌握并学会了如何从噪声step by step的获取清晰的视频内容。

在这个关键步骤中，Sora模型将经典的ViT Transformer（视觉Transformer）结构进行了扩展和优化，变成更适合于扩散模型的DiT结构（Diffusion Transformer）：

- 经典ViT侧重图像理解和判断；
- DiT则扩展了Transformer结构，专门优化用于视频生成与去噪任务，它更擅长捕捉视频内容中的长距离全局信息，从而更有效率地实现从噪声中逐步还原清晰、连续、稳定的视频画面。

因此，DiT的设计，既继承了ViT核心架构的精髓，又根据扩散模型的生成任务特性进行了关键性的升级与调整。

------

#### 总体总结

经过上述严格的训练步骤与设计，Sora模型不仅具备了强大的视频全局理解与高效扩散去噪能力，更实现了视频、文本跨模式的精确深度融合，由此获得了生成高质量、长时稳定视频内容的能力。ViT与DiT在其中的应用，可谓是模型训练和整体架构成功的关键所在。



## Sora 推理：“如何从零开始，创造高清视频”

那么，你真的调用Sora来生成视频时，模型如何一步一步实现的呢？

![images](https://github.com/xinyuwei-david/david-share/blob/master/Multimodal-Models/SoRA/images/3.png)

① 用户输入文字提示（Prompt）

你向Sora提供明确的文字Prompt，告知你想创造怎样的视频（如：“傍晚海滩，一条金毛狗叼着飞盘奔跑。”）。

②文本处理成向量（与训练同款模式）

- 文本Prompt被转化成模型能理解的Token向量序列。

3. 随机初始化视频潜空间（非直接配对prompt、而是从乱中生）

- 起点并非是“画面已清晰的潜空间”，而是一个随机生成的小潜空间立方体。
- 开始时你看不到任何实质图像信息，只是一块随机杂乱数字、毫无逻辑、毫无意义的潜空间，就像一大堆杂乱灰色颗粒的“沙粒团”。

> 为何从随机噪声开始？
> 扩散模型设计的本质就是学习从噪声恢复清晰画面的技巧，所以推理端也必须从噪声出发，模型一路去除噪声、逐渐显现视频内容。

4. 切分、转化Patch-Token序列

- 潜空间被切割为同训练方式的小立方体（Patch）。
- Patch同样被展平转为一维的向量序列 (Tokens)，并加入位置信息。

5. 逐步去噪、预测下一个Patch（最核心关键一步）

- 同训练时配置一致的Transformer扩散模型开始发挥作用：
  - Transformer 从头到尾遍历这段token序列；
  - 根据序列第一部分已经生成的内容，逐步预测下一个仍充满噪声的token应当如何修正。
- 每一次迭代预测，都帮助整个视频显现更多细节：运动方向、形状颜色逐渐明确；
- 模型并非一次到位，而是逐次迭代，不断地微调"下一个patch"，逐渐推进成为清晰画面。

实际推理大约需要重复20～30次去噪步骤后，视频潜空间才能从纯随机噪声逐渐演化成清晰的视频表现。

5. 潜空间整体解码

- 最终的清晰潜空间整体送入训练好的解码器（与训练编码过程相对应的Video-VAE的逆过程）。
- 最终解码输出清晰的RGB视频帧序列（真正的视频帧）。

6. 封装为可播放视频

- 把得到的帧序列再封装压缩为标准的视频格式（例如MP4）。
- 同时模型在视频数据上加入不可见的数字水印信息，便于平台维护版权或溯源。



- **训练** = 原视频→潜空间→切patch→token序列 → Transformer学会预测下一个patch去噪生成；
- **推理过程**正是这套过程的反向版：从随机潜空间噪声出发，Transformer一次次预测patch内容，由噪声逐步重构出清晰视频。

这就是Sora的“前世今生”和实践路径: 看似复杂，其实沿着“编码→切分→预测→解码” 一条主线进行，非常清晰且明确。
希望通过这篇详细Blog，你彻底掌握Sora背后的技术方法，真正理解了视频生成目前前沿工程技术背后的逻辑。

## 为什么需要Patch

- 首先是为了减小计算量，将高清视频压缩成小巧的、易处理的数字积木；
- 其次是方便Sora将图像数据转化为类似于语言系统中的tokens（词条），做到逻辑和框架的统一；
- 同时，一次性处理整段视频（全部积木）可以更好地保证画面的连贯与一致性，避免画面跳跃或者出现不合理的运动和变化。

总结：

- Sora的训练过程，就是从视频片段到压缩再到让系统学会去噪还原的过程；
- Sora的生成过程，就是系统基于你给的文字提示，从随机杂乱的状态，一步步预测并重建精美视频的过程。



Patch（积木块）可以理解为视频内容的一个小型表示单元。在 Sora 里，每个 Patch有这些特点：

1. 空间信息（Spatial Information）： 一个Patch覆盖视频画面里的一小部分区域，例如8x8或16x16的小方格区域。这意味着每个 Patch 只包含画面中某个具体小区域的信息，而不是整个图像。
2. 时间信息（Temporal Information）： 每个Patch不仅在空间上专注于画面的一个小区域，同时也跨越了若干帧（例如2到4帧）。也就是说，每一个小积木块囊括了连续的几帧图像。这帮助模型不仅理解每一小局部在空间上的细节，也捕捉视频在时间上的动感变化。
3. 数字信息（数值化表示）： 每个Patch本质上就是一系列数字组成的向量（Token），这个向量里保存着画面、动作、细节等多种特征信息。
4. 位置坐标（Positional Encoding）： 每一个 Patch 还额外附带一个位置信号（位置编码），它告诉模型当前Patch位于整个视频画面和时间轴的哪个具体位置。

简单来说，每个 Patch 就像是一个“立体积木块”，包含：

- 局部的画面细节（空间）
- 局部的动作变化（时间）
- 一组代表画面特征的数据数字（数字化特征向量）
- 一个清晰的位置编号，代表它在整段视频序列里的具体位置 (Positional Encoding)

而所有这些小块（Patch）合在一起，才能组成一整个清晰连贯的动态视频。



在 Sora 模型的训练过程中，Patch里面并不是直接存储普通“图片”或“帧”本身的像素画面。

准确来说，「Patch （小积木块）」里存储的东西是：

- 视频经过视觉编码器（visual encoder）压缩后的特征表示，也叫作「潜空间（Latent）」信息。
- 这些潜空间信息已经从原始的像素图像转化为一系列的数字特征。所以，这些Patch里是一些高度概括过的视频特征数据，而不是原始的图片像素帧。

换句话说：

- 一个Patch不是直接的“图像帧”（不是普通像素组成的图片）
- 它是被压缩和编码后的视频“特征”或“简化版”信息，相当于浓缩为更易处理的“数字特征表示”；
- 这些数字特征在训练时被进一步划分为小Patch（积木块），并标注清晰的位置编码，以便Transformer能够进行学习并预测；
- 在训练或生成结束之后，系统才会再使用解码器，把这些高度浓缩特征Patch恢复为清晰可视、真实视频序列。

所以，Patch里面的确没有“原始的图片帧”，而是压缩后的视频潜空间特征表现形式（以数字而非图像像素存在）。



Patch本身并不是直接包含一张张传统意义上的图片，它包含的是压缩和编码后的视频潜空间特征（想象成一堆高度抽象的数字）。要把这些patch变成一段持续的视频画面（也就是多个连续的图片），整个过程大白话来说就是：

第一步：将分散的patch拼回潜空间

你可以把每一个patch想象成拼图游戏里的“一小块拼图”，单独看它只是很局部的小信息，没法看清画面。

我们把许多patch按照它们原有的位置编码（也就是每个拼图块的位置信息）拼回它们原本应有的位置，就组合成一张完整的压缩后的“小拼图”图案（潜空间）。到这一步，我们仍然是在一个抽象又压缩的空间中，而非真实视频帧。

第二步：从潜空间整体解码为连续帧序列

我们再使用解码器（decoder），将刚刚拼好的潜空间的“小拼图”整体做一个“超分辨率放大”和“解码”过程。这一步就是：

- 将压缩的小尺寸潜空间逐步放大，还原细节。
- 将通道数转变并还原成标准的RGB三色通道，逐步形成可理解、可视的普通像素图片。
- 将每个时间点上的帧单独拿出来，就还原出了连续的视频画面序列，也就是连续的图片帧。

总结一下：

- 每个Patch是压缩后的视频局部“特征块”，本身不是完整图像，也没有单个完整的图片帧。
- 多个Patch按位置拼起来，组合成完整的小型“潜空间”。
- 最后将潜空间整体解码，恢复成高清晰可视的多个连续图片帧（视频）。

一句话讲清楚：

Patch本身就是数字表示，它只能间接通过「拼成潜空间 → 整体解码 → 恢复到实际帧图像」的方式，最终转化为持续的视频画面（多个图片）。



## Sora的长时一致和可变分辨率

#### 一、什么叫“长时一致”？

想像你在看一部动画短片。
• 如果第一秒里狗是黄色的，第二秒忽然变成黑色，第三秒又变回黄色——这就“不一致”。
• 如果镜头往前移动，前一帧桌子在画面左边，下一帧桌子却瞬间跳到右边——这也“不一致”。
Sora 追求的“长时一致”，就是让同一个角色、同一束灯光、同一条运动轨迹，在 10 秒、15 秒甚至更长的片段里都连贯、稳定，像真人拍摄那样自然。

为什么要把所有操作都放在“潜空间 + patch”里完成？
因为模型一次性“看见”整段视频的所有积木块（patch），可以同时考虑开头、中段、结尾的关联。这样它在去噪时会自动维护：
• 角色外形、衣服颜色前后保持一致；
• 光影、天气、镜头运动平滑过渡；
• 物体不乱闪、不瞬移。

##### 二、什么叫“可变分辨率、可变长宽比”？

做视频的同学经常遇到：
• 横屏 1920×1080 要发到电脑端；
• 竖屏 1080×1920 要发到短视频 App；
• 宽银幕 2048×858 要在影院放映；
传统模型往往只能固定输出一种分辨率，要换尺寸就得额外裁剪、上采样，甚至重新训练。

Sora 的做法是：

1. 训练时就把各种原生分辨率、各种横竖屏素材一起喂给模型；
2. 模型内部统一在低维潜空间里运算，维度很小，对显存友好；
3. 最后一步解码时，再按你指定的尺寸一次性放大到 720p、1080p、4K……都行。

结果就是：
• 你同一个 prompt，可以要一条横屏广告，也可以要一条竖屏短视频；
• 不必剪裁，不会因为强行变形而丢构图；
• 训练好的大模型不需要为每种分辨率各训一份，节省大量算力和维护成本。

一句话总结
• “长时一致”＝让角色、动作、光影在整个片段里前后不打架；
• “可变分辨率”＝同一套模型随时给你横屏/竖屏/宽银幕、720p/1080p/4K，都能直接生成，不用重新训练。



Sora把“数据、算力、表示方式、模型骨架、训练策略、安全机制”这几件看似分散的事做到了极致，并且让它们彼此配合得天衣无缝。下面拆成五条“独门秘籍”——每一条业内都有人做，但 Sora 把它们做到了极端：

1. 超大规模、高质量的视频-文本对齐数据
   • 不只是“量大”（数千万～上亿段真实视频），更是“质好”——先用 GPT-4V 把原本简短或噪声很大的标题重写成细节丰富、物理细节完备的 long caption。
   • 这种“先精修文本→再喂模型”的做法让视频和文字对齐度极高，模型在生成时更少跑偏，更能严格执行提示。
   • 额外的多模态过滤（NSFW、剧烈抖动、压缩伪影）保证训练分布干净，减少低质样本对长时连贯性的破坏。
2. 时空 Patch + 大型 Transformer 的“统一表示”
   • 先用 Video-VAE 把整段视频压到低维 latent，再切成 3D patch（空间 × 时间）。
   • 所有 patch 在序列里与文本 token 并列，「视频生成=预测下一个 patch」→ 逻辑完全复用大语言模型的成功范式。
   • Transformer 一次就能“看到”十几秒全部 patch，天然保持人物、镜头、光照的全局一致，避开逐帧方法的漂移问题。
3. 可变时长、可变分辨率、原生长宽比混训
   • 训练时把横屏、竖屏、宽银幕、720p～4K 统统保持原尺寸喂给模型，不裁方块。
   • 再加随机截断/拼接，让同一模型习惯 2 秒、4 秒、16 秒甚至半分钟 clip。
   • 推理端输入一句 prompt + 想要的时长/分辨率，模型一次采样直接给出目标尺寸——不用再套超分或二次拼接，画面边缘、取景都自然得多。
4. “堆到极致”的算力与工程
   • 华尔街日报披露 OpenAI 为 Sora 级项目采购了 >2 万块 H100；配合 Flash-Attention、DeepSpeed-ZeRO、8-bit 权重、KV-Cache 重排序等尖端并行技巧，真正把 Video-Transformer 训练规模推到百亿 token。
   • 同时做了重采样蒸馏和推理管线量化，生产环境依然能在几十、甚至十几步就出片，兼顾质量与速度。
5. 完整的安全与后期工具链
   • 训练末期插入 RLAIF / RLHF，降低暴力、政治、违法场景出现概率。
   • 推理侧自动植入数字水印、指纹，平台可溯源；还有实时检测 API 限制高危 prompt。
   • 同一套时空 patch 表示天生支持续帧、插帧、风格替换、局部编辑等高阶功能——这让 Sora 不只是“能生”，还是一个可编辑的视频工作站内核。



## 显影阶段的隐藏

早期的视频生成模型（例如 Progressive GAN、StyleGAN 系列）在生成过程中，会给观众呈现一种由模糊逐渐清晰的画面效果。这种现象的原因是：它们采用分层的生成方法，即开始生成时分辨率很低（比如 8×8），随着生成步骤不断前进，网络逐级上采样，图像精度提高，每一级都转换为可直接观看的RGB像素图像，因此用户肉眼能够看到由低分辨率（模糊）到高分辨率（清晰）的完整生成过程。

而Sora这种新一代的视频生成模型虽然内在也有类似的从“噪声”逐渐变到“清晰画面”的过程，但用户看到的现象却完全不同：Sora 模型并不直接在像素上逐层绘制，而是始终在一个被称为『潜空间（Latent）』的抽象数字空间里操作。这种潜空间可以看作是数据被压缩和抽象后的精华表示，不直接对应肉眼可见的像素。

Sora模型从一开始便在这种机器可读的数字空间里操作：潜空间被自动分割成一个个『时空小积木（Patch）』，模型通过 Transformer 技术逐步预测这些积木从随机噪声到逐渐清晰的具体内容。尽管这个过程同样经历了模糊的噪声逐步转化为清晰视觉的“潜在显影”阶段，但用户一般情况下并不会在屏幕上看到这个过程。因为只有当模型彻底完成了所有预测、达到最终一步后，这个潜空间才会一次性被整体“解码”到高清的像素空间，呈现最终完成的视频画面。

做个简单的生活化比喻：

- Progressive GAN、StyleGAN 就像在公开能看到的画板上绘画，艺术家从大致轮廓、简单色块逐渐细化成高清细致作品，因此观众能看清由模糊草稿到高清画面的整个变化过程。

  ***Please click below pictures to see my demo video on Youtube***:
  [![BitNet-demo1](https://raw.githubusercontent.com/xinyuwei-david/david-share/refs/heads/master/IMAGES/6.webp)](https://youtu.be/QssbwkFrfVc)

- 而 Sora 模型则更像是一位“秘密雕塑师”，他在一个被封闭起来的雕塑盒子内部进行创作，外界根本看不到创作中的任何细节。只有在雕塑彻底完成、打磨光滑后，艺术家才会突然地、一次性地掀开盒子展现高清成品。因此观众始终只能看到最后成品，看不到模糊到细致变化的过程。

  ***Please click below pictures to see my demo video on Youtube***:
  [![BitNet-demo1](https://raw.githubusercontent.com/xinyuwei-david/david-share/refs/heads/master/IMAGES/6.webp)](https://youtu.be/tkWVFHTq_48)

那么，为什么 Sora 要选择这种隐藏了显影阶段的方式呢？
主要是为了实现更好的长视频质量和高效性：

- **更稳定的长视频画面效果**：Sora 能一次性看到整个视频的所有关键时刻和整体潜空间表示，因此能同时处理每个画面的全局细节，使视频画面在很长一段时间内保持稳定连续。
- **更高的生成效率**：Sora无需在中途频繁转换为像素画面再返回潜空间，因此节省巨大的计算成本和显存开销，只需要在生成最后才将潜空间统一解码一次到高清画面即可。

总之，用户在Sora界面上是看不到由模糊到清晰这一“显影”过程的，但实际上模型内部依然具备从噪声逐渐清晰化的完整过程，只不过它都隐蔽地“藏在”了潜空间之中，实际部署时并不会展现给用户直接观看，用户所看到的始终是最后一次性生成的高清成片。



### VIT与DIT的对比

##### **主要任务和核心目标的不同**

Vision Transformer (ViT): 专注于「理解与识别」

ViT （Vision Transformer）由 Google 在 2020年提出。它将传统的Transformer从语言领域延伸到了图像分类领域。ViT 的设计思路，就是把图片切割成若干更小的图像patch（通常是16×16的小块），将这些图像小块转换成Transformer能处理的序列token，然后进行“理解和判断”：

- 典型任务：图像分类、物体识别、目标检测、语义分割…
- 输入数据：静态图片的原始像素块
- 训练目标：学习如何根据全局上下文判断输入的类别标记或含义（例如动物的品种、物体类型等）
- 常用模型：ViT, DeiT, BEiT, DINO, MAE 等

Diffusion Transformer (DiT)：致力于「从噪声到生成、去噪」

相比之下，DiT（Diffusion Transformer）的出现是由于扩散模型(diffusion model)的流行与发展。最初扩散模型，如Stable Diffusion，使用U-Net网络实现图像从随机噪声逐步变清晰的“去噪”过程。

但U-Net卷积结构存在一个天然缺陷，即无法很好捕捉图像（尤其是视频）中的长距离、跨帧依赖关系。因此，研究人员提出DiT架构，以Transformer取代了原来的U-Net作为扩散模型中的去噪网络：

- 典型任务：图像或视频生成、扩散过程的去噪处理
- 输入数据：被随机高斯噪声污染后的“潜空间(latent)”或者“时空patch”
- 训练目标：逐步预测噪声残差 (noise residual)，恢复干净图像或视频
- 常用模型：Stable-Diffusion-DiT、Imagen-DiT、OpenAI Sora

一句话提炼：

- **ViT**：“看清楚”, 专注于图像理解和判断——将图像转化为类和标签；
- **DiT**：“创造清晰画像”, 专注于生成与去噪——从纯随机噪声数据一步步生成逼真的图像与视频画面。



##### **模型细节设计上的区别**

###### **ViT 架构细节**

Vision Transformer 本质架构更适合判别任务，有几个典型设计特征：

1. **Patch 切分方式**：通常选择16×16或较大的像素patch。
2. **位置编码(Positional Embedding)**：加入图片空间长宽两维度的位置信息，使用Transformer捕捉空间内的区域关系。
3. **任务头(task head)**：序列前加入 CLS (分类) token，再通过MLP层输出类别预测。
4. **训练损失函数**：常见交叉熵损失，用于分类与判别任务。

###### DiT 架构细节（以Sora为代表）

Diffusion Transformer则设计思路显著不同，更侧重于生成任务，有如下关键特点：

1. **Patch切分与数据准备**：输入的是扩散后的latent空间数据（低维潜空间压缩后的patch），而非原始像素。
2. **扩展位置编码**：移除标准空间位置信息外，还加入时间步信息(timestep embedding)，精确掌控扩散过程的位置。
3. **输出目标**：Transformer直接输出噪声残差或latent更新方向，而非类别预测。
4. **训练损失函数**：均方误差(MSE Loss)，用以学习预测去除噪声的目标。

可见，虽然ViT与DiT骨架类似，但两者在数据输入、位置编码、输出任务头和训练方式上均存在明显差异。

###### 实际应用场景的对比

- ViT广泛应用于图像分类、物体识别、自监督学习等任务，如DeiT、BEiT、MAE等，在实际工业中用于照片自动描述、物体识别与医疗图像诊断。
- DiT则应用在视觉生成任务，例如图像艺术创造 (Stable-Diffusion-DiT)、高清视频生成 (Sora)、视频状态预测（视频生成扩散模型）或图生图任务上。由于Transformer的长序列能力，DiT特别适合对视频这样的复杂内容生成与处理。

**总结**

ViT (视觉分类Transformer) 和 DiT (视觉生成Transformer) 表面上虽然名字接近、都属于视觉Transformer家族，但二者本质任务、数据处理方式、网络设计细节其实差别极大。

- ViT从静态图片出发，倾向于理解与判别；
- DiT则聚焦扩散生成，从噪声连续地创造或复原出高质量图像与视频。

它们同为 Transformer 家族的优秀成员，但分别服务于视觉领域中的两个截然不同方向，同时也共同推动着计算机视觉领域的未来发展。