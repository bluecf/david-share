{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-969c4652-eb7c-4d78-afcc-6a2b01aff76b', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2000 Olympics actually refer to the Summer Olympics of the year 2000. They were held in Sydney, Australia.\\n', role='assistant', function_call=None, tool_calls=None))], created=1712061118, model='mistral', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=38, prompt_tokens=65, total_tokens=103))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"sk-xxx\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistral\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"In what city were the 2000 olympics taken place?\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline  \n",
    "import torch  \n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-large-v2\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "#transcription = pipe(audio_file_path)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/miniconda/envs/assistant/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://77c505f3a238ad775b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://77c505f3a238ad775b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/gradio/queueing.py\", line 522, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/gradio/blocks.py\", line 1255, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/gradio/utils.py\", line 750, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_55357/1700450795.py\", line 47, in transcribe_and_query_llm_voice\n",
      "    transcription = pipe(audio_file_path)['text']\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 285, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1198, in __call__\n",
      "    return next(\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 266, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 675, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 183, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "  File \"/opt/miniconda/envs/assistant/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 410, in preprocess\n",
      "    raise ValueError(f\"We expect a numpy ndarray as input, got `{type(inputs)}`\")\n",
      "ValueError: We expect a numpy ndarray as input, got `<class 'NoneType'>`\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n",
      "/opt/miniconda/envs/assistant/lib/python3.10/site-packages/gradio/processing_utils.py:405: UserWarning: Trying to convert audio automatically from float64 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n",
      "/opt/miniconda/envs/assistant/lib/python3.10/site-packages/gradio/processing_utils.py:405: UserWarning: Trying to convert audio automatically from float64 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n",
      "/opt/miniconda/envs/assistant/lib/python3.10/site-packages/gradio/processing_utils.py:405: UserWarning: Trying to convert audio automatically from float64 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://77c505f3a238ad775b.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "from transformers import AutoProcessor, BarkModel\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "WORDS_PER_CHUNK = 25\n",
    "\n",
    "def split_sentence_into_chunks(sentence, n):\n",
    "    words = sentence.split()\n",
    "    if len(words) <= n:\n",
    "        return [sentence]\n",
    "    else:\n",
    "        chunks = [' '.join(words[i:i+n]) for i in range(0, len(words), n)]\n",
    "        return chunks\n",
    "\n",
    "\n",
    "# Setup Whisper client\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-large-v2\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "voice_processor = AutoProcessor.from_pretrained(\"suno/bark\")\n",
    "voice_model = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16).to(\"cuda:0\")\n",
    "\n",
    "voice_model =  voice_model.to_bettertransformer()\n",
    "voice_preset = \"v2/en_speaker_9\"\n",
    "\n",
    "\n",
    "system_prompt = \"You are a helpful AI, you need reply user's question with users' name. You should speek in language according to the user's question. \"\n",
    "\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"sk-xxx\")  # Placeholder, replace \n",
    "sample_rate = 48000\n",
    "\n",
    "def transcribe_and_query_llm_voice(audio_file_path):\n",
    "\n",
    "    transcription = pipe(audio_file_path)['text']\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistral\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # Update this as per your needs\n",
    "            {\"role\": \"user\", \"content\": transcription + \"\\n Answer briefly.\"}\n",
    "        ],\n",
    "    )\n",
    "    llm_response = response.choices[0].message.content\n",
    "\n",
    "    sampling_rate = voice_model.generation_config.sample_rate\n",
    "    silence = np.zeros(int(0.25 * sampling_rate))\n",
    "\n",
    "    BATCH_SIZE = 12\n",
    "    model_input = sent_tokenize(llm_response)\n",
    "\n",
    "    pieces = []\n",
    "    for i in range(0, len(model_input), BATCH_SIZE):\n",
    "        inputs = model_input[BATCH_SIZE*i:min(BATCH_SIZE*(i+1), len(model_input))]\n",
    "        \n",
    "        if len(inputs) != 0:\n",
    "            inputs = voice_processor(inputs, voice_preset=voice_preset)\n",
    "            \n",
    "            speech_output, output_lengths = voice_model.generate(**inputs.to(\"cuda:0\"), return_output_lengths=True, min_eos_p=0.2)\n",
    "            \n",
    "            speech_output = [output[:length].cpu().numpy() for (output,length) in zip(speech_output, output_lengths)]\n",
    "            \n",
    "            pieces += [*speech_output, silence.copy()]\n",
    "        \n",
    "        \n",
    "    whole_ouput = np.concatenate(pieces)\n",
    "\n",
    "    audio_output = (sampling_rate, whole_ouput) \n",
    "\n",
    "    return llm_response, audio_output\n",
    "\n",
    "\n",
    "def transcribe_and_query_llm_text(text_input):\n",
    "\n",
    "    transcription = text_input\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistral\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # Update this as per your needs\n",
    "            {\"role\": \"user\", \"content\": transcription + \"\\n Answer briefly.\"}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    llm_response = response.choices[0].message.content\n",
    "\n",
    "    sampling_rate = voice_model.generation_config.sample_rate\n",
    "    silence = np.zeros(int(0.25 * sampling_rate))\n",
    "\n",
    "    BATCH_SIZE = 12\n",
    "    model_input = sent_tokenize(llm_response)\n",
    "\n",
    "    pieces = []\n",
    "    for i in range(0, len(model_input), BATCH_SIZE):\n",
    "        inputs = model_input[BATCH_SIZE*i:min(BATCH_SIZE*(i+1), len(model_input))]\n",
    "        \n",
    "        if len(inputs) != 0:\n",
    "            inputs = voice_processor(inputs, voice_preset=voice_preset)\n",
    "            \n",
    "            speech_output, output_lengths = voice_model.generate(**inputs.to(\"cuda:0\"), return_output_lengths=True, min_eos_p=0.2)\n",
    "            \n",
    "            speech_output = [output[:length].cpu().numpy() for (output,length) in zip(speech_output, output_lengths)]\n",
    "            \n",
    "            \n",
    "            pieces += [*speech_output, silence.copy()]\n",
    "        \n",
    "        \n",
    "    whole_ouput = np.concatenate(pieces)\n",
    "\n",
    "    audio_output = (sampling_rate, whole_ouput)  \n",
    "\n",
    "    return llm_response, audio_output\n",
    "\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            text_input = gr.Textbox(label=\"Type your request\", placeholder=\"Type here or use the microphone...\")\n",
    "            audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Or record your speech\")\n",
    "        with gr.Column():\n",
    "            output_text = gr.Textbox(label=\"LLM Response\")\n",
    "            output_audio = gr.Audio(label=\"LLM Response as Speech\", type=\"numpy\")\n",
    "    \n",
    "    submit_btn_text = gr.Button(\"Submit Text\")\n",
    "    submit_btn_voice = gr.Button(\"Submit Voice\")\n",
    "\n",
    "    submit_btn_voice.click(fn=transcribe_and_query_llm_voice, inputs=[audio_input], outputs=[output_text, output_audio])\n",
    "    submit_btn_text.click(fn=transcribe_and_query_llm_text, inputs=[text_input], outputs=[output_text, output_audio])\n",
    "\n",
    "demo.launch(ssl_verify=False,\n",
    "            share=True,\n",
    "            debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
