{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import logging\n",
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import FastEmbedEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPDF(source_path):\n",
    "    try:\n",
    "        document_pages_lc = None\n",
    "        document_pages_lc = PyPDFLoader(source_path).load()\n",
    "\n",
    "        return document_pages_lc\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error readPDF(): {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMSWord(source_path):\n",
    "    try:\n",
    "        #整个文档每分页的单词数量.\n",
    "        one_page_size = 300 \n",
    "        document_pages_lc = None\n",
    "        #此方法并未返回与PDF加载器相同的对象，例如，无法识别文档页面。因此，下面构建了自定义逻辑.\n",
    "        document_pages_lc = UnstructuredWordDocumentLoader(source_path).load() \n",
    "        document_pages_lc_list = []        \n",
    "        \n",
    "        # UnstructuredWordDocumentLoader将整个文档作为一个单独的页面返回，所以需要实现自定义的分割\n",
    "        for page in document_pages_lc:                       \n",
    "            \n",
    "            #分割文档为单词\n",
    "            page_words = page.page_content.split(' ') \n",
    "\n",
    "            #分割文档为每页包含one_page_size个单词的页面\n",
    "            for i in range((len(page_words) // one_page_size)+1): \n",
    "\n",
    "                doc = Document(page_content=' '.join(page_words[i*one_page_size:(i+1)*one_page_size]),\n",
    "                               metadata={\"source\":page.metadata[\"source\"], \"page\":i})\n",
    "                document_pages_lc_list.append(doc)                   \n",
    "        \n",
    "        return document_pages_lc_list\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error readMSWord_old(): {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocumentExtension(documentPath):\n",
    "    try:\n",
    "        return os.path.basename(documentPath).split('.')[len(os.path.basename(documentPath).split('.'))-1]\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error getDocumentExtension(): {e}')    \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingEntireDoc(documentPath):\n",
    "\n",
    "    try:\n",
    "        docType = None\n",
    "        document_pages_lc = None\n",
    "    \n",
    "        #Get document type\n",
    "        docType = getDocumentExtension(documentPath).lower()\n",
    "        \n",
    "        if docType == 'pdf':\n",
    "            document_pages_lc = readPDF(documentPath)\n",
    "        # Custom word doc 自定义Word文档处理，因为它没有像PDF加载器那样的页面元数据, \n",
    "        # 此外，文档并未像PDF那样默认被分割成多个页面。请查看 readMSWord() 方法以获取更多详细信息\n",
    "        elif docType == 'docx' or docType == 'doc':\n",
    "            document_pages_lc = readMSWord(documentPath)\n",
    "        return document_pages_lc\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error getEmbeddingEntireDoc(): {e}')\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docPath = './doc/故宫简介.pdf'\n",
    "documents = getEmbeddingEntireDoc(docPath)\n",
    "\n",
    "# 按照设置分割文档\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f'Number of chunks: {len(chunks)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 从.env 文件中导入OpenAI API 的秘钥\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "azure_embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.environ.get(\"AZURE_EMBEDDING_DEPLOYMENT\"),\n",
    "    model=os.environ.get(\"AZURE_EMBEDDING_MODEL\"),\n",
    ")\n",
    "\n",
    "\n",
    "# vector_store = Chroma.from_documents(documents=chunks, embedding=azure_embeddings, persist_directory=\"./chroma_db\")\n",
    "vector_store = Chroma.from_documents(documents=chunks, embedding=FastEmbedEmbeddings())\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "        \"score_threshold\": 0.5,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM\n",
    "llm = AzureChatOpenAI(\n",
    "                openai_api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "                azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "        )\n",
    "\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use two sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 设置运行的流水线\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions = [\"故宫的长宽和面积是多少?\", \n",
    "             \"故宫的三大殿是什么?\",\n",
    "             \"御花园原名叫什么?\",\n",
    "            ]\n",
    "ground_truths = [[\"故宫东西宽７５０米，南北长９６０米，面积达到７２万平方米\"],\n",
    "                [\"太和殿，中和殿和保和殿\"],\n",
    "                [\"宫后苑\"]]\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# Inference\n",
    "for query in questions:\n",
    "  answers.append(rag_chain.invoke(query))\n",
    "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truths\": ground_truths\n",
    "}\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "print(f\"result: {data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    "    embeddings=azure_embeddings,\n",
    "    llm=llm,\n",
    "\n",
    ")\n",
    "df = result.to_pandas()\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
