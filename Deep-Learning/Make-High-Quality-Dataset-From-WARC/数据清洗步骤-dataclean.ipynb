{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 14:30:12.719464: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 14:30:16.396905: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 12.3665\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.5180\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 10.6987\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 9.9333\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.1499\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.3664\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.5305\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 6.6356\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 5.6925\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 4.6734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f438ff53eb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Generate Sample Data\n",
    "# For demonstration, we create synthetic data for a regression task\n",
    "X = np.random.rand(100, 3)  # 100 samples, 3 features\n",
    "y = X @ np.array([2.0, -1.0, 3.0]) + 1.5  # Linear relationship with some coefficients and bias\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(3,)),  # Input layer with 3 inputs\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # Another dense layer\n",
    "    tf.keras.layers.Dense(1)  # Output layer with 1 unit (for regression)\n",
    "])\n",
    "# Compile the Model with the Adam Optimizer\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='mean_squared_error')  # MSE is typical for regression\n",
    "model.fit(X, y, epochs=10)  # Train for 10 epochs as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk  \n",
    "nltk.download('punkt')  \n",
    "nltk.download('stopwords')  \n",
    "nltk.download('wordnet')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'coding', 'pythonprogramming', 'fun', 'let', 'clean', 'text']\n"
     ]
    }
   ],
   "source": [
    "import re  \n",
    "import nltk  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "  \n",
    "# 带有表情符号、哈希标签和其他字符的示例文本  \n",
    "text = \"I love coding! 😊 #PythonProgramming is fun! 🐍✨ Let’s clean some text 🧹\"  \n",
    "  \n",
    "# 先使用正则表达式去除表情符号和特殊字符  \n",
    "text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "  \n",
    "# 分词  \n",
    "tokens = word_tokenize(text)  \n",
    "  \n",
    "# 标准化（转换为小写）  \n",
    "cleaned_tokens = [token.lower() for token in tokens]  \n",
    "  \n",
    "# 去除停用词  \n",
    "stop_words = set(stopwords.words('english'))  \n",
    "cleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]  \n",
    "  \n",
    "# 词形还原  \n",
    "lemmatizer = WordNetLemmatizer()  \n",
    "cleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]  \n",
    "  \n",
    "print(cleaned_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'coding', 'pythonprogramming', 'fun', 'let', 'clean', 'text']\n"
     ]
    }
   ],
   "source": [
    "import re  \n",
    "import nltk  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "  \n",
    "# 带有表情符号、哈希标签和其他字符的示例文本  \n",
    "text = \"I love coding! 😊 #PythonProgramming is fun! 🐍✨ Let’s clean some text 🧹\"  \n",
    "  \n",
    "# 先使用正则表达式去除表情符号和特殊字符  \n",
    "text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "  \n",
    "# 分词  \n",
    "tokens = word_tokenize(text)  \n",
    "  \n",
    "# 标准化（转换为小写）  \n",
    "cleaned_tokens = [token.lower() for token in tokens]  \n",
    "\n",
    "  \n",
    "# 去除停用词  \n",
    "stop_words = set(stopwords.words('english'))  \n",
    "stop_words.remove('i')  # 从停用词列表中移除“I”，注意停用词列表中的“I”是小写  \n",
    "cleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]  \n",
    "  \n",
    "# 词形还原  \n",
    "lemmatizer = WordNetLemmatizer()  \n",
    "cleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]  \n",
    "  \n",
    "print(cleaned_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But it's not ccenhancese about more language ccenhancese. Other important aspect is ensuring accurate retrieval by ccenhancese product name spellings. Additionally, refining descriptions enhances the ccenhancese of the content.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re  \n",
    "  \n",
    "# 带有拼写错误的示例文本  \n",
    "text_with_errors = \"\"\"But it's not coherence about more language coherence. Other important aspect is ensuring accurate retrieval by coherence product name spellings. Additionally, refining descriptions enhances the coherence of the content.\"\"\"  \n",
    "  \n",
    "# 纠正拼写错误的函数  \n",
    "def correct_spelling_errors(text):  \n",
    "    # 定义常见拼写错误及其纠正的字典  \n",
    "    spelling_corrections = {  \n",
    "        \"oherence\": \"coherence\",  # 注意这里修正了多个 \"oherence\" 的映射，只保留了一个正确的映射  \n",
    "        \"accurte\": \"accurate\",  \n",
    "        \"retrievel\": \"retrieval\",  \n",
    "        \"refning\": \"refining\",  \n",
    "        \"oherenc\": \"enhances\",  \n",
    "        \"contnt\": \"content\",  \n",
    "    }  \n",
    "  \n",
    "    # 遍历字典中的每一对键值对，用正确的版本替换拼写错误的单词  \n",
    "    for mistake, correction in spelling_corrections.items():  \n",
    "        text = re.sub(mistake, correction, text)  \n",
    "  \n",
    "    return text  \n",
    "  \n",
    "# 在示例文本中纠正拼写错误  \n",
    "cleaned_text = correct_spelling_errors(text_with_errors)  \n",
    "  \n",
    "print(cleaned_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"text\": \"'The Top 10 Tech Trends\", \"label\": \"WORK_OF_ART\"}, {\"text\": \"2024\", \"label\": \"DATE\"}, {\"text\": \"John\", \"label\": \"PERSON\"}, {\"text\": \"Google\", \"label\": \"ORG\"}, {\"text\": \"Microsoft\", \"label\": \"ORG\"}, {\"text\": \"AI\", \"label\": \"ORG\"}]\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_sm  \n",
    "import spacy  \n",
    "import json  \n",
    "  \n",
    "# 加载英语语言模型  \n",
    "nlp = spacy.load(\"en_core_web_sm\")  \n",
    "  \n",
    "# 带有元数据候选者的示例文本  \n",
    "text = \"\"\"In a blog post titled 'The Top 10 Tech Trends of 2024,'  \n",
    "John Doe discusses the rise of artificial intelligence and machine learning  \n",
    "in various industries. The article mentions companies like Google and Microsoft  \n",
    "as pioneers in AI research. Additionally, it highlights emerging technologies  \n",
    "such as natural language processing and computer vision.\"\"\"  \n",
    "  \n",
    "# 使用spaCy处理文本  \n",
    "doc = nlp(text)  \n",
    "  \n",
    "# 提取命名实体及其标签  \n",
    "meta_data = [{\"text\": ent.text, \"label\": ent.label_} for ent in doc.ents]  \n",
    "  \n",
    "# 将元数据转换为JSON格式  \n",
    "meta_data_json = json.dumps(meta_data, ensure_ascii=False)  \n",
    "  \n",
    "print(meta_data_json)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"text\": \"'The Top 10 Tech Trends\",\n",
      "        \"label\": \"WORK_OF_ART\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"2024\",\n",
      "        \"label\": \"DATE\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"John\",\n",
      "        \"label\": \"PERSON\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"Google\",\n",
      "        \"label\": \"ORG\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"Microsoft\",\n",
      "        \"label\": \"ORG\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"AI\",\n",
      "        \"label\": \"ORG\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json  \n",
    "  \n",
    "# 假设meta_data_json是您从spaCy处理后得到的JSON字符串  \n",
    "meta_data_json = '[{\"text\": \"\\'The Top 10 Tech Trends\", \"label\": \"WORK_OF_ART\"}, {\"text\": \"2024\", \"label\": \"DATE\"}, {\"text\": \"John\", \"label\": \"PERSON\"}, {\"text\": \"Google\", \"label\": \"ORG\"}, {\"text\": \"Microsoft\", \"label\": \"ORG\"}, {\"text\": \"AI\", \"label\": \"ORG\"}]'  \n",
    "  \n",
    "# 使用json.loads将字符串转换成Python对象  \n",
    "meta_data = json.loads(meta_data_json)  \n",
    "  \n",
    "# 使用json.dumps将Python对象转换回格式化的JSON字符串  \n",
    "formatted_json = json.dumps(meta_data, indent=4, ensure_ascii=False)  \n",
    "  \n",
    "print(formatted_json)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello, how are you?\n",
      "Translated Text: Hola, ¿cómo estás?\n"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "\n",
    "# 原始文本  \n",
    "text = \"Hello, how are you?\"  \n",
    "\n",
    "# 翻译文本  \n",
    "translator = Translator(to_lang=\"es\")\n",
    "translated_text = translator.translate(text)\n",
    "\n",
    "print(\"Original Text:\", text)  \n",
    "print(\"Translated Text:\", translated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello, how are you?\n",
      "Translated Text: 你好你怎么样？\n"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "\n",
    "# 原始文本  \n",
    "text = \"Hello, how are you?\"  \n",
    "\n",
    "# 翻译文本  \n",
    "translator = Translator(to_lang=\"zh\")\n",
    "translated_text = translator.translate(text)\n",
    "\n",
    "print(\"Original Text:\", text)  \n",
    "print(\"Translated Text:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "aims tone text analysis sentiment\n",
      "Topic 2:\n",
      "human learning function algorithms structure\n"
     ]
    }
   ],
   "source": [
    "#！conda install scikit-learn  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.decomposition import LatentDirichletAllocation  \n",
    "  \n",
    "# 示例文档  \n",
    "documents = [  \n",
    "    \"Machine learning is a subset of artificial intelligence.\",  \n",
    "    \"Natural language processing involves analyzing and understanding human languages.\",  \n",
    "    \"Deep learning algorithms mimic the structure and function of the human brain.\",  \n",
    "    \"Sentiment analysis aims to determine the emotional tone of a text.\"  \n",
    "]  \n",
    "  \n",
    "# 将文本转换为数值特征向量  \n",
    "vectorizer = CountVectorizer(stop_words='english')  \n",
    "X = vectorizer.fit_transform(documents)  \n",
    "  \n",
    "# 应用Latent Dirichlet Allocation (LDA)进行主题建模  \n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)  \n",
    "lda.fit(X)  \n",
    "  \n",
    "# 显示主题  \n",
    "for topic_idx, topic in enumerate(lda.components_):  \n",
    "    print(\"Topic %d:\" % (topic_idx + 1))  \n",
    "    # 使用.get_feature_names_out()代替.get_feature_names()  \n",
    "    print(\" \".join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-5 - 1:-1]]))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_text = \"\"\"\n",
    "Sarah (S): Technology Enthusiast\n",
    "Mark (M): AI Expert\n",
    "S: Hey Mark! How's it going? Heard about the latest advancements in Generative AI (GA)?\n",
    "M: Hey Sarah! Yes, I've been diving deep into the realm of GA lately. It's fascinating how it's shaping the future of technology!\n",
    "S: Absolutely! I mean, GA has been making waves across various industries. What do you think is driving its significance?\n",
    "M: Well, GA, especially Retrieval Augmented Generative (RAG), is revolutionizing content generation. It's not just about regurgitating information anymore; it's about creating contextually relevant and engaging content.\n",
    "S: Right! And with Machine Learning (ML) becoming more sophisticated, the possibilities seem endless.\n",
    "M: Exactly! With advancements in ML algorithms like GPT (Generative Pre-trained Transformer), we're seeing unprecedented levels of creativity in AI-generated content.\n",
    "S: But what about concerns regarding bias and ethics in GA?\n",
    "M: Ah, the age-old question! While it'\n",
    "s \n",
    "true\n",
    " that GA can inadvertently perpetuate biases \n",
    "present\n",
    "in\n",
    " the training \n",
    "data\n",
    ", there \n",
    "are\n",
    " techniques \n",
    "like\n",
    " Adversarial Training (\n",
    "AT\n",
    ") that aim \n",
    "to\n",
    " mitigate such issues.\n",
    "S: Interesting! So, where do you see GA headed in the next few years?\n",
    "M: Well, I believe we'll witness a surge in applications leveraging GA for personalized experiences. From virtual assistants to content creation tools, GA will become ubiquitous in our daily lives.\n",
    "S: That'\n",
    "s exciting! Imagine AI-powered \n",
    "virtual\n",
    " companions tailored \n",
    "to\n",
    " our preferences.\n",
    "M: Indeed! And with advancements in Natural Language Processing (NLP) and computer vision, these virtual companions will be more intuitive and lifelike than ever before.\n",
    "S: I can't wait to see what the future holds!\n",
    "M: Agreed! It'\n",
    "s an exciting \n",
    "time\n",
    "to\n",
    " be \n",
    "in\n",
    " the \n",
    "field\n",
    "of\n",
    " AI.\n",
    "S: Absolutely! Thanks for sharing your insights, Mark.\n",
    "M: Anytime, Sarah. Let's keep pushing the boundaries of Generative AI together!\n",
    "S: Definitely! Catch you later, Mark!\n",
    "M: Take care, Sarah!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "import nltk  \n",
    "  \n",
    "# Download necessary NLTK data  \n",
    "nltk.download('punkt')  \n",
    "nltk.download('stopwords')  \n",
    "nltk.download('wordnet')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sarah', 'technology', 'enthusiast', 'mark', 'ai', 'expert', 'hey', 'mark', 'going', 'heard', 'latest', 'advancement', 'generative', 'ai', 'ga', 'hey', 'sarah', 'yes', 'diving', 'deep', 'realm', 'ga', 'lately', 'fascinating', 'shaping', 'future', 'technology', 'absolutely', 'mean', 'ga', 'making', 'wave', 'across', 'various', 'industry', 'think', 'driving', 'significance', 'well', 'ga', 'especially', 'retrieval', 'augmented', 'generative', 'rag', 'revolutionizing', 'content', 'generation', 'regurgitating', 'information', 'anymore', 'creating', 'contextually', 'relevant', 'engaging', 'content', 'right', 'machine', 'learning', 'ml', 'becoming', 'sophisticated', 'possibility', 'seem', 'endless', 'exactly', 'advancement', 'ml', 'algorithm', 'like', 'gpt', 'generative', 'pretrained', 'transformer', 'seeing', 'unprecedented', 'level', 'creativity', 'aigenerated', 'content', 'concern', 'regarding', 'bias', 'ethic', 'ga', 'ah', 'ageold', 'question', 'true', 'ga', 'inadvertently', 'perpetuate', 'bias', 'present', 'training', 'data', 'technique', 'like', 'adversarial', 'training', 'aim', 'mitigate', 'issue', 'interesting', 'see', 'ga', 'headed', 'next', 'year', 'well', 'believe', 'witness', 'surge', 'application', 'leveraging', 'ga', 'personalized', 'experience', 'virtual', 'assistant', 'content', 'creation', 'tool', 'ga', 'become', 'ubiquitous', 'daily', 'life', 'exciting', 'imagine', 'aipowered', 'virtual', 'companion', 'tailored', 'preference', 'indeed', 'advancement', 'natural', 'language', 'processing', 'nlp', 'computer', 'vision', 'virtual', 'companion', 'intuitive', 'lifelike', 'ever', 'ca', 'nt', 'wait', 'see', 'future', 'hold', 'agreed', 'exciting', 'time', 'field', 'ai', 'absolutely', 'thanks', 'sharing', 'insight', 'mark', 'anytime', 'sarah', 'let', 'keep', 'pushing', 'boundary', 'generative', 'ai', 'together', 'definitely', 'catch', 'later', 'mark', 'take', 'care', 'sarah']\n",
      "sarah technology enthusiast mark ai expert hey mark going heard latest advancement generative ai ga hey sarah yes diving deep realm ga lately fascinating shaping future technology absolutely mean ga making wave across various industry think driving significance well ga especially retrieval augmented generative rag revolutionizing content generation regurgitating information anymore creating contextually relevant engaging content right machine learning ml becoming sophisticated possibility seem endless exactly advancement ml algorithm like gpt generative pretrained transformer seeing unprecedented level creativity aigenerated content concern regarding bias ethic ga ah ageold question true ga inadvertently perpetuate bias present training data technique like adversarial training aim mitigate issue interesting see ga headed next year well believe witness surge application leveraging ga personalized experience virtual assistant content creation tool ga become ubiquitous daily life exciting imagine aipowered virtual companion tailored preference indeed advancement natural language processing nlp computer vision virtual companion intuitive lifelike ever ca nt wait see future hold agreed exciting time field ai absolutely thanks sharing insight mark anytime sarah let keep pushing boundary generative ai together definitely catch later mark take care sarah\n"
     ]
    }
   ],
   "source": [
    "# Sample text with emojis, hashtags, and unicode characters\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(synthetic_text)\n",
    "\n",
    "# Remove Noise and filter out empty tokens  \n",
    "cleaned_tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens if re.sub(r'[^\\w\\s]', '', token)]  \n",
    "  \n",
    "# Normalization (convert to lowercase)  \n",
    "cleaned_tokens = [token.lower() for token in cleaned_tokens]  \n",
    "  \n",
    "# Remove Stopwords  \n",
    "stop_words = set(stopwords.words('english'))  \n",
    "cleaned_tokens = [token for token in cleaned_tokens if token not in stop_words and token.strip()]  \n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\n",
    "\n",
    "print(cleaned_tokens)\n",
    "new_content_string = ' '.join([lemmatizer.lemmatize(token) for token in cleaned_tokens])\n",
    "print(new_content_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "MESSAGE_SYSTEM_CONTENT = \"\"\"You are a customer service agent that helps   \n",
    "a customer with answering questions. Please answer the question based on the  \n",
    "provided context below.   \n",
    "Make sure not to make any changes to the context if possible,  \n",
    "when prepare answers so as to provide accurate responses. If the answer   \n",
    "cannot be found in context, just politely say that you do not know,   \n",
    "do not try to make up an answer.\"\"\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "from openai import AzureOpenAI  \n",
    "  \n",
    "# 定义 Azure OpenAI 客户端  \n",
    "client = AzureOpenAI(  \n",
    "  azure_endpoint=\"https://gpt4-0125.openai.azure.com/\",  \n",
    "  api_key=\"7b1d32be7008483086cc936a046ab848\",  \n",
    "  api_version=\"2024-02-15-preview\"  \n",
    ")  \n",
    "  \n",
    "# 更新后的 response_test 函数  \n",
    "def response_test(question: str, context: str, model: str = \"gpt4-0125\"):    \n",
    "    message_text = [    \n",
    "        {\"role\": \"system\", \"content\": MESSAGE_SYSTEM_CONTENT},    \n",
    "        {\"role\": \"user\", \"content\": question},    \n",
    "        {\"role\": \"assistant\", \"content\": context},    \n",
    "    ]    \n",
    "        \n",
    "    completion = client.chat.completions.create(    \n",
    "        model=model,    \n",
    "        messages=message_text,    \n",
    "        temperature=0.7,    \n",
    "        max_tokens=4040,    \n",
    "        top_p=0.95,    \n",
    "        frequency_penalty=0,    \n",
    "        presence_penalty=0,    \n",
    "        stop=None    \n",
    "    )    \n",
    "        \n",
    "    return completion.choices[0].message.content  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but the provided conversation between Sarah and Mark does not include specific techniques in Adversarial Training (AT) that can help mitigate biases in Generative AI models. They mention the importance and the potential of Adversarial Training in addressing bias, but they do not delve into specific techniques or methodologies. If you have any other questions or need further clarification on a different topic, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# 示例调用  \n",
    "question = \"What are some specific techniques in Adversarial Training (AT) that can help mitigate biases in Generative AI models?\"  \n",
    "response = response_test(question, synthetic_text)    \n",
    "print(response)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but the provided context does not specifically mention techniques in Adversarial Training (AT) that can help mitigate biases in Generative AI models. Adversarial Training is indeed a method used to improve the robustness of AI models, including generative ones, by exposing them to adversarial examples during training. However, the conversation primarily focuses on the general advancements and potentials of Generative AI, without delving into specific techniques for bias mitigation. If you have any other questions or need further information on a different topic, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "question = \"What are some specific techniques in Adversarial Training (AT) that can help mitigate biases in Generative AI models?\" \n",
    "response = response_test(question, new_content_string)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
