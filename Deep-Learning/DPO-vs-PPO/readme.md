## DPO VS PPO

***Refer to ：Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback***

### 摘要（Abstract）

从偏好反馈（preference feedback）中进行学习，已经成为当代大语言模型（LMs）提升生成质量与各种任务性能的关键途径。简单来说，“偏好反馈”通常让人类（或模拟系统）对两段不同的模型输出做出偏好判断，进而指导模型学习哪种回复更优。

然而，目前在应用中，对于偏好数据的来源、学习算法的选择，以及最终如何评估，做法各不相同，这使得人们不易分清究竟哪些部分对于最终性能影响最大。本研究将“偏好学习”过程拆分为四个核心要素：

1. 偏好数据（数据本身的来源与质量），

2. 学习算法（PPO、DPO 等），

3. 奖励模型（reward model），

4. 策略训练时所用的提示（policy training prompts）。

   我们对它们逐一进行系统性分析，发现所有要素都会影响最终性能，但影响程度并不一样：
   • 偏好数据质量最关键，好的数据能带来显著提升。
   • 之后是学习算法的差异，尤其是 PPO 与 DPO。
   • 再往后是一款更健全的奖励模型。
   • 最后，如果只关注单一领域，还可以在策略训练时加入更多有针对性的提示，但对全面多任务性能的增益相对有限。

   在我们的实验中：
   • 当聚焦数学任务时，**PPO 的性能最高可比 DPO 高 2.5 个百分点；在通用任务上也能领先 1.2 个百分点。**
   • 高质量的偏好数据在指令**遵从性（instruction following）和真值性（truthfulness）上带来高达 8% 的性能提升**。
   • 即便把奖励模型从中小规模升级到更大规模，在数学任务里可以获得显著**（最多 5%）提升，**但对其他通用任务帮助不大。

本研究开源了所有训练与评测代码，以及对应的模型与数据集。总体而言，高质量的偏好数据、合适的算法与奖励模型、再配合恰当的提示，是“偏好学习”能够取得更好下游性能的配方。

**名词解释**
• “偏好反馈”：假设我们给模型同一个问题，让它生成两个不同的回答，A 和 B。人类标注者对这两个答案做出比较，认为 A 更好。于是模型就会“知道”A 的样式或思路是更优的，从而在后续训练中朝这个方向微调。
• “指令遵从性”与“真值性”: 如果系统设定了“请先列出解题思路，再给出最终答案”的指令，一个具备较高指令遵从性与真值性的模型，会提供正确且完整的思路，并避免无根据的胡编乱造。


### 引言（Introduction）

在现代大语言模型（LMs）的开发流程中，往往会额外添加一个阶段——**从偏好反馈中学习（有时也称 RLHF，reinforcement learning from human feedback）——再将模型投入实际使用。**先前有大量研究表明，该阶段能够显著增强模型，包括使模型在指令执行、代码生成、数学解题、文本摘要等方面都获得大幅度改善。

不过，由于各研究在具体实现中对数据、算法、评估方式的做法千差万别，我们难以弄清“究竟哪一环节是提质增效的主要来源”。尤其是在对比最常见的两种偏好学习算法——PPO（Proximal Policy Optimization）与 DPO（Direct Preference Optimization）——时，人们常困惑：两者有什么优劣？该如何选择？

PPO 与 DPO 都基于偏好数据进行模型训练，但过程并不相同：
• DPO：直接在偏好数据（prompt, chosen response, rejected response）上进行离线优化。
• PPO：先训练一个“奖励模型（reward model）”，再用该奖励模型在线给策略模型的输出打分，并通过强化学习更新策略。



因此，我们将这类偏好学习过程拆解为四部分：
1）偏好数据，
2）学习算法，
3）奖励模型，
4）策略训练提示（训练时用于在线生成与打分的提示集合）。

本工作便是要系统探究：若我们在同一个已训练好的强大 SFT（Supervised Fine-Tuned）模型基础上，分别改变上述任意一个要素，下游性能会发生什么变化？我们在实验中发现，各要素确实都会产生影响，但重要性和效果各异。

【简单示例】
• 当我们用人类真实标注的对比数据（chosen / rejected）进行训练，往往能让模型更好地贴合某些真实场景。
• 如果采用大规模爬虫数据或合成数据，可能在覆盖范围上更广，但其质量不一定稳定，需要仔细挑选或标注。

### 设置（Setup）

本节先简要介绍 PPO 与 DPO 的概念与原理，然后描述我们在实验与评测中的具体做法。图 2（原文示意）呈现了 PPO 与 DPO 间的结构化对比。  

**PPO 与 DPO（PPO and DPO）**


(1) PPO
在偏好学习中，PPO 常被视作“在线”强化学习方式：
• 首先训练一个“奖励模型（reward model）”Rψ(x, y)，该模型输入 (x, y) 后输出代表好坏的分数。它通常用类似 Transformer 的架构实现，在原有语言模型头部换成了一个回归头，用来回归分数。
• 在策略训练阶段，我们针对每个提示 x，用当前策略模型 πθ 生成回答 y，再用奖励模型打分。模型根据分数进行策略更新，并在损失函数中加入 KL 惩罚项，以防止策略过度偏离初始参考策略 πref。
• 整个过程持续迭代，模型边生成新的回答边被打分，属于“在线”训练（即数据并非固定，而是会不断由当前策略模型生成）。

若将之类比到现实场景，可想象一个学生（策略模型）和老师（奖励模型），学生答题以后，老师根据答题质量给分，分数高就意味着好的回答；学生会不断根据分数来修正答题策略。

(2) DPO
相比之下，DPO 则是一种“离线”方法：
• 它不在训练过程中动态抽样新的回答；
• 也不需要在策略训练时额外维护一个价值网络或奖励模型。
• DPO 的核心思路是直接在现成的 (prompt, chosen, rejected) 数据上做优化，目标函数与前面奖励模型训练类似，只是将对比信息直接用来更新策略。

DPO 的优点是实现简单、算力开销小，但类似地，它也缺乏在线探索。对于某些“策略分布随时间不断变化”的场景，DPO 可能覆盖不足，而 PPO 能看到更多“最新生成回答”，进一步进行针对性调整。


**实验与评估设置（Experimental and Evaluation Setup）**

我们基于公开发布的 TÜLU 2 13B[22] 模型系列来做进一步研究。该模型是基于 Llama 2[52] 原始权重，在大量开源指令数据上进行过有监督微调（SFT）形成的，已有较强的综合能力。我们将其视为初始策略模型 πref，然后再分别用 PPO 或 DPO 进行训练。

评测方面，我们扩展了 TÜLU[55] 的测试体系，涵盖以下能力：
• MMLU[20]：测试知识准确度（factuality）。
• GSM8k[9]、Big Bench Hard[5, 47]：测试推理（reasoning）能力。
• TruthfulQA[29]：测试回答内容的真实与正确程度（truthfulness）。
• HumanEval+[6, 32]、MBPP+[2, 32]：测试编程（coding）能力。
• ToxiGen[19]、XSTest[42]：测试对不当行为或有害信息的安全性（safety）。
• AlpacaEval 1 & 2[27, 13]、IFEval[65]：测试模型对指令的跟随度（instruction following）。

我们将上述评测按类型分组，并汇总得到平均得分；再对所有大类求算术平均，得到一个综合指标，用于衡量模型的多方面表现（具体评测细节见附录 D）。  

【示例说明】 • MMLU 问题类似各种学科知识问答（历史、地理、医学等），要求回答准确无误。 • GSM8k 通常是一系列小学到中学级别的数学题，着重检验模型的推理正确性。 • ToxiGen 会给出一些可能涉及偏见、歧视或具有攻击性的话题，测试模型对敏感内容的处理是否安全得当。



### 基于偏好反馈的探索 

在此我们从以下四个方面展开： (1) 偏好数据；(2) 学习算法；(3) 奖励模型；(4) 策略训练提示。  

**偏好数据（Preference Data）**


我们收集了 14 个有代表性的偏好数据集，用 DPO 分别进行训练，然后评估下游性能（见表 1，原文中）。这些数据集来自多种来源：
• 人工标注（human-annotation），如 HH-RLHF[4]、HelpSteer[56]；
• 网络爬取（web-scraping），如 SHP-2[15]、StackExchange[25]；
• 自动合成（synthetic），如 Ultra-Feedback[11]、Nectar[66]、Orca[34]、Capybara[12] 等。

部分数据集还带有不同粒度的注释，如 UltraFeedback 既提供整体评分，也提供多维度细分评分。经过对比，我们的发现包括：
• 对“指令遵从”与“真值”提升巨大，但对“知识准确”帮助不大：最优模型在指令遵从、真值性上相对基线 SFT 可提升 8 个百分点以上，但对 MMLU（考察 factuality）几乎无改善。
• “合成 + 多维度标注”往往效果最好：与单纯人工标注或纯整体打分的合成数据相比，带细粒度评分的合成数据在真值性上尤其出色，也常在多维度表现更稳定。例如，即使 HelpSteer 数据量更小，也能胜过大规模人工数据 HH-RLHF。综合来看，UltraFeedback（多维）在此部分表现最佳。
• 部分 Arena 数据在安全性上较差：如 Chatbot Arena 数据训练的模型对违规内容可能更宽松，原因或在于标注者在对话平台上倾向于接受更“攻击性”或“不当”的回复，导致模型学到了不良偏好。

【额外示例】
想象如果一段合成数据同时包含“语言是否礼貌”、“回答是否正确”、“是否遵守格式要求”三种维度打分，那么模型就能更好地同时兼顾不同的目标。


**学习算法：DPO 与 PPO（Preference Learning Algorithm）**

在偏好数据相同、模型规模相同（13B）、数据量相同（裁剪至 60,908 条）的条件下，我们分别用 DPO 与 PPO 进行训练并对比。表 2（原文）给出了详细结果，主要结论有：
• PPO 整体略优于 DPO：在所有数据集上，PPO 的综合评分均高于或持平于 DPO，优势约在 0.7 个点左右。
• 推理、编程和安全性方面，PPO 相对提升更明显：平均来看，PPO 相比 DPO 在推理、编程和安全性上分别提升约 1.3、2.9、2.3 个点；但在真值性方面，却平均下降了 2.5 个点。指令遵从、知识准确则相差不大。
• 对数学、编程等需要探索空间较大的任务，PPO 的在线生成与打分机制更有优势。不过，当模型过于“追求高奖励”时，可能减少对事实核对的谨慎度，故真值性可能出现下滑。


**奖励模型（Reward Models）**

接下来我们聚焦 PPO 训练所用的奖励模型本身，主要考察两个问题：

- 用更多的数据来训练奖励模型，会不会更好？

- 奖励模型规模从 13B 提升到 70B，会不会进一步提高最终策略的性能？

  具体做法是：
  • 首先，我们从前文比较优秀的若干偏好数据中，挑选并混合成一个大号“Mix RM”数据集，用它来训练奖励模型。另一个对照是只用 UltraFeedback（UFRM）数据。
  • 同时，比对 13B 与 70B 两种规模的奖励模型训练结果。

  (1) 直接评估奖励模型：
  • 我们固定一个 SFT 模型（TÜLU 2 13B），在同一批提示上采样 16 个回答，然后用不同的奖励模型打分，从中选出得分最高的回答（Best-of-N，BoN），再进行测试。
  • 此外，还用 RewardBench[26] 这套基准来检验奖励模型对一系列偏好对的判别能力。

  结果表明：在 RewardBench 与 BoN 场景下，更大规模（70B），或者采用更多训练数据（Mix RM），确实能让奖励函数更精准，BoN 选择的回答质量提升明显。但另一方面，不同的大号奖励模型也各擅胜场，比如 13B Mix RM 在 RewardBench 上最好，70B UltraF. RM 在 BoN 场景下最好。

  (2) 在 PPO 下游任务中的真实表现：
  • 当将这些优化过的奖励模型用于 PPO 训练整套流程时，我们发现只有在 GSM（数学）指标上出现最显著的改善，而在其他任务上收效非常有限。
  • 这说明，即便奖励模型本身变得更“聪明”，在综合多任务场景中也不一定能带来大幅度性能跃升[37, 56]。
  • 一方面，数学题最需要严谨推理，因此对奖励模型的鉴别能力要求更高。更强的奖励模型就能指导策略更好地生成正确答案。
  • 另一方面，通用任务可能有更多开放式回答，不仅限于单一对错，也让高精度的奖励模型在打分环节并不显得特别具有决定性优势。

  【额外示例】
  对数学题 A：
  “已知 2x + 3 = 7，求 x？”
  若奖励模型能正确识别 x=2 的回答优于 x=100 的回答，那么 PPO 训练出的模型便能更快锁定正确解法。而对某些开放式问答，奖励模型只能笼统打分，差异就没那么明显。


  **策略训练提示（Policy Training Prompts）**

  1 针对性提示能否提升特定能力？
  以上我们提到了策略训练提示集 Dπ：PPO 训练时，会让模型在这些提示上不断生成回答并被奖励模型打分。是否可以通过“加大某领域提示”来加强模型在该领域的能力？以数学为例：
  • 分别使用 ① GSM8k 训练集原始提示、② 一批从多数据源挖掘的数学相关提示、③ 随机抽取的一批 UltraFeedback 提示（与数学无关）进行 PPO 训练。
  • 搭配前面提到的不同规模奖励模型，最终测试数学正确率。

  结果如图 3（原文）：
  • 当奖励模型足够强（如 70B 规模）且提示分布与数学足够贴近时，测试成绩可以从 46% 提升到 62%。
  • 如果奖励模型较弱，那么使用与其不匹配的提示集（如不相关的随机提示）时，增益受限。
  • 这进一步说明，PPO 有机会利用“在线”采样能力，加上与偏好反馈相辅相成的提示，来针对性地优化某些场景。

  

  2 扩展提示是否能提升多任务整体？
  既然对单一领域有帮助，那么对多任务领域能否也通过“扩大不同类型提示”来提升整体性能？我们尝试将 2 万条数学提示、2 万条编程提示，以及 2 万条 UltraFeedback 提示混合起来（共约 60K），用以训练 PPO。这与“只使用 UltraFeedback 提示 60K”的做法对比。
  • 实验发现，混合提示并没有明显比单纯使用 UltraFeedback 提示更好，有时甚至更差。这可能是因为原本 UltraFeedback 提示就足够多样化，已经覆盖了数学和编程场景；此时再“刻意堆砌”数学或编程提示，反而削弱了对其他方面的关注度。因此并未带来预期的综合提升。

  

  ### 基于偏好反馈的一份“食谱”（A Recipe）

  

  综合上述分析，我们推荐的“最佳实践”如下（见表 5，原文）：

  偏好数据：使用高质量的合成偏好数据（如包含多维度打分的 UltraFeedback）。

  学习算法：在大多数场景下，PPO 的性能普遍优于 DPO。

  奖励模型：若有算力和资源，可选用更大规模的奖励模型；在预算有限的情况下，13B 规模也能获得不错的效果。

  策略训练提示：若只需在单一域（如数学）中极致提升，则可再度强化相应领域的提示分布；但如果想要多任务兼顾，就让提示相对均衡。

  我们实测用 70B 奖励模型 + UltraFeedback 数据训练 PPO，能比最优的 DPO 模型，或是其他同级开源 Llama 2 13B 衍生模型（例如官方 Llama 2 Chat），在综合性能上更进一步。

  

  ### 相关工作（Related Work）

  

  (1) 从偏好反馈中学习（Learning from Preferences for LMs）
  早期研究多将其视作强化学习（RL）的思路，尤其着重人类反馈[8, 67]；其中 PPO[44, 37] 是最常用的实践方法之一。也有一些工作使用 REINFORCE[46]、策略梯度[1, 50] 等变体，但本质流程类似：先训练奖励模型，再用无标签提示不断在线采样并优化策略。

  本研究与上述工作不同之处在于，我们系统性考察了数据、算法、奖励模型、策略提示等多维度对最终性能的综合影响。而与之相对的 DPO[39] 及其变形[3, 21, 59, 16]方法，则省去了一系列在线采样和价值网络环节，因而实现上简化不少。

  (2) 近期并行研究
  Xu 等[60] 关注在当偏好数据分布与初始策略差距较大时，DPO 性能表现不稳、PPO 相对更鲁棒；Tajwar 等[48] 也对比了 DPO、PPO 在小规模合成任务下的表现，强调在线采样和对负样本进行显式梯度更新的重要性。本工作则在更大规模、多种真实数据、不同奖励模型规模等层面做了更全面的实验与量化。

  

  ### 结论（Conclusion）

  

  本研究对偏好学习的四大核心环节——偏好数据、学习算法（PPO / DPO）、奖励模型、策略训练提示——作了系统探究。结果显示，它们都重要，但影响次序可概括为：

- 偏好数据质量最先决定模型能达到的上限，

- 学习算法（PPO 较 DPO）带来更高的潜在上限，

- 奖励模型规模越大，对特定任务（如数学）能有明显促进作用，

- 策略训练提示只在专域（如数学）时能大幅度加成，对于多任务综合性能改善有限。

  总体而言，有了更强大的奖励模型与有效的在线采样，PPO 训练就能够更好地利用高质量偏好数据，进一步提升模型表现。我们开源了所有训练与评测代码，以及相应模型与数据，期望为更多后续研究奠定基础。偏好反馈研究的长远目标，是让语言模型不仅输出流畅，更能符合人类的复杂需求与价值观。


  【背景补充与小结】
  • 偏好反馈在对话式大模型中日益受到重视，可以帮助模型更好地贴近人类表达习惯、避免胡编和不当内容。
  • 当面对特定任务（如数学或编程）时，在线采样可以充分探索不同回答，能够逼近真正可行的解题方案；若仅静态对比现成数据，则容易因覆盖不足而效果不佳。
  • 有了更高质量的偏好数据与更精细的奖励模型，人机交互中的“好坏判断”也能更客观，更能教会模型遵从指令与输出真值性更高的结果。