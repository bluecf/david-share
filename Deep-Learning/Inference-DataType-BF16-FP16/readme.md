## Inference Data Type: BF16 vs FP16

在大语言模型（LLMs）的训练和推理阶段，浮点数据类型（Floating-Point Data Type）的选择尤为重要。本文将深入介绍BF16与FP16这两种主流16位浮点数据类型的差异，详细分析其算法原理与数值特性，并以Google发布的Gemma 3模型为案例，探讨数据类型选择对模型推理性能和数值稳定性的实际影响。

### 一、为何从FP32迁移到低精度浮点数？

 
最早期的大语言模型（如GPT-1、GPT-2）普遍使用FP32（单精度浮点数）进行训练与推理。FP32的数据结构如下：

- 1位符号位（sign bit）

- 8位指数位（exponent bits），指数位决定了数值表示的范围。

- 23位尾数位（mantissa bits），尾数位决定了数值表示的精度。

  虽然FP32的数值范围大（±1.18×10⁻³⁸ 到 ±3.4×10³⁸），精度也高，但每参数消耗4字节内存，极大地增加了模型存储和推理成本。例如，一个70B参数模型在FP32下需要约280GB内存。为降低内存占用，业界开始普遍采用16位浮点数，即BF16和FP16。

### 二、详细算法对比：BF16与FP16的数据结构

 
FP16与BF16都是16位浮点数，内存占用均为FP32的一半，但二者位结构设计有显著差别：

| 浮点类型 | 符号位 | 指数位 | 尾数位 | 指数范围 | 数值表示范围            | 精度 |
| -------- | ------ | ------ | ------ | -------- | ----------------------- | ---- |
| FP16     | 1      | 5      | 10     | ±15      | ±6.1×10⁻⁵ ~ ±6.5×10⁴    | 高   |
| BF16     | 1      | 8      | 7      | ±127     | ±1.18×10⁻³⁸ ~ ±3.4×10³⁸ | 较低 |

- **指数位（Exponent）**：决定了浮点数能表示的数值范围大小，指数位越多，范围越大。

- **尾数位（Mantissa）**：决定浮点数的有效数字精度，尾数位越多，精度越高。

  由上表可见：

- FP16尾数位（10位）更多，理论上精度更高。

- BF16指数位（8位）更多，动态范围更大，与FP32完全相同。

### 三、为何业界倾向于BF16而非FP16？

 
虽然FP16理论精度更高，但实践表明，对于大语言模型而言：

- **动态范围（指数位）远比精度（尾数位）更重要**。大模型训练与推理经常出现极大或极小的中间计算值，FP16范围不够，易导致数值溢出（NaN、Inf）或下溢（变为0）的问题。

- FP16需要复杂的loss scaling技术来防止数值问题，增加了工程复杂度与计算开销。

- BF16因其更大的指数范围，无需loss scaling，训练和推理过程更稳定、更简单、更高效。

  因此，尽管BF16的精度略低，但业界主流（Google TPU、NVIDIA A100/H100、AMD MI300）都逐步转向了BF16。

### 四、案例分析：Gemma 3推理数据类型转换的实际影响

 
Google发布的Gemma 3模型原本使用TPU并且采用BF16进行深度优化训练。我们尝试直接将Gemma 3模型从BF16转换到FP16进行推理，发现了严重的数值问题：

- 模型在FP16推理下完全失效，无法产生合理输出，只输出EOS token。

- 在BF16和FP32推理模式下，模型表现正常，输出准确。

  Gemma 3模型推理失败的根本原因在于：

- Gemma 3在TPU优化下充分利用了BF16的大动态范围，模型参数中存在大量FP16无法表示的数值。

- 直接转换到FP16时，导致模型激活值、权重值溢出或下溢，推理阶段彻底失效。

### 五、缓解策略与具体算法探讨


面对上述问题，工程化的缓解方案：

#### 方案1：使用FP32进行推理

 

- 模型权重存储和计算全用FP32
- 优势：完全避免数值问题
- 缺陷：内存消耗翻倍，推理速度降低

#### 方案2：使用PyTorch AMP混合精度推理（推荐）

 
具体方法：

- 模型权重依旧存储为FP16以节省内存

- 在推理计算时，临时将关键计算提升到FP32进行，以避免溢出/下溢：

  ```
  with torch.cuda.amp.autocast(dtype=torch.float32):  
      output = model(input_ids)  
  ```

 
这种AMP方式的效果（实测Gemma 3模型）：

| 数据类型                                                     | 内存占用（Gemma 3实测） | 数值稳定性 | 推理速度 |
| ------------------------------------------------------------ | ----------------------- | ---------- | -------- |
| FP16                                                         | 8.322 GB                | ❌ 失效     | 快       |
| FP32                                                         | 16.426 GB               | ✅ 稳定     | 慢       |
| AMP混合精度                                                  | 11.029 GB               | ✅ 稳定     | 适中     |
| BF16                                                         | 8.254 GB                | ✅ 稳定     | 快       |
| AMP混合精度推理在数值稳定性、内存占用和速度之间取得了很好的权衡，适合硬件不支持BF16的场景。 |                         |            |          |

- 应尽可能保持推理与训练使用相同的数据类型：

  - BF16训练模型用BF16推理
  - FP16训练模型用FP16推理

- 如果因硬件限制不得不更改数据类型，应避免直接使用FP16以防止严重数值问题，建议使用FP32或AMP混合精度推理方法。

  这种BF16到FP16严重问题的案例并不普遍，但必须引起工程化部署的足够重视。

> 当使用新的数据类型进行推理时，务必提前进行充分的性能与数值稳定性测试，以确保模型的正常运行。


