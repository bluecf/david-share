# Comparison of Various Fine-Tuning Methods

本文将会对如下微调技术进行对比：SFT、ReFT、RHLF、RLAIF、DPO、PPO、TPO。



## 几种技术之间的关系

如果把复杂的问题简单理解，这些技术之间的关系大概是：

**ReFT（Reinforced Fine-Tuning，强化微调）：**

- **组成**：ReFT = SFT + PPO
- **过程**：在有监督微调（SFT）的基础上，使用 PPO（近端策略优化）进行强化学习。
- **评估方式**：通常通过**自动化程序**对模型输出进行评估，奖励信号来自程序的评价。

**RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）：**

![图片](https://mmbiz.qpic.cn/mmbiz_png/akGXyic486nUZtF6XYK9EpTpg40XvUeRCHQFd39MdyIIIbGaFjQKZ8PDxic6faSnOGnITqdpvbznWY1Sp2aqIIcw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- **组成**：RLHF = SFT + PPO + 人类反馈
- **过程**：在 SFT 的基础上，使用 PPO 进行强化学习，奖励信号来自**人类反馈**。
- **评估方式**：人类对模型输出进行评价，或者使用基于人类反馈训练的**奖励模型**来评估。

##### DPO 方法（Direct Preference Optimization，直接偏好优化）：

- **组成**：DPO 方法 = SFT + **参考模型** + DPO
- **过程**：在 SFT 的基础上，**引入参考模型**（通常是经过 SFT 的初始模型，参数固定不更新），使用 DPO（直接偏好优化）方法，利用参考模型和人类偏好数据，直接优化模型参数。
- **评估方式**：利用**人类偏好数据和参考模型**，构建损失函数，直接优化模型参数，使模型更倾向于生成被人类偏好的输出。

**RLAIF（Reinforcement Learning from AI Feedback，基于 AI 反馈的强化学习）：**

![图片](https://mmbiz.qpic.cn/mmbiz_png/akGXyic486nUZtF6XYK9EpTpg40XvUeRCxfjelLwiaed6DNmzrv9LKwPYwaPAqFJ0qc9ddesiaDzsU9wgaEmettJg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- **组成**：RLAIF = SFT + PPO + AI 反馈
- **过程**：在 SFT 的基础上，使用 PPO 进行强化学习，奖励信号来自**AI 模型的反馈**。
- **评估方式**：辅助的 AI 模型（可能是奖励模型）对模型输出进行评价，提供奖励信号。


**TPO（Thought Preference Optimization，思维偏好优化）：**

- **组成**：TPO = SFT + 思维生成（Thought Generation）+ DPO
- **过程**：在 SFT 的基础上，引入**思维生成**，即模型在输出回答前生成内部的思维过程。然后，使用 DPO 方法直接优化模型，偏好数据来自**AI 判别模型的反馈**。
- **评估方式**：利用**AI 判别模型**对模型输出的**回答部分**进行评价，形成偏好对（优选和劣选）。根据这些偏好对，使用 DPO 方法优化模型参数，提升模型性能。



**解释：**

- ReFT（强化微调）通过在监督微调后的模型上，使用PPO算法进行强化学习，奖励信号来自于自动化程序对模型输出与标准答案的比较。

- RLHF（基于人类反馈的强化学习）在SFT基础上，使用PPO算法进行强化学习，奖励信号来自人类对模型输出的评价。

- DPO方法（直接偏好优化）在SFT基础上，使用DPO算法直接优化模型参数以符合人类偏好，不使用PPO等传统强化学习算法。

- RLAIF（基于AI反馈的强化学习）类似于RLHF，但人类反馈替换为AI模型的反馈，使用PPO算法进行强化学习。




ReFT、RLHF、DPO和RLAIF。这些方法都是在监督微调（SFT）的基础上，进一步优化模型以提高性能，但它们在优化策略和反馈来源上有所不同。

1. **ReFT（Reinforced Fine-Tuning，强化微调）**：这是SFT和PPO（近端策略优化）的结合。在第一阶段，模型通过SFT在有标注的数据上进行训练，建立基本的语言理解和生成能力。第二阶段，引入PPO算法，对模型进行强化学习优化。此时，模型的输出由自动化程序进行评估，程序根据预设的规则或标准对模型的输出进行评价，并生成奖励信号。模型根据这些奖励信号，使用PPO算法调整自身参数，以产生更优的输出。ReFT的特点是评估过程自动化，无需人类参与，适用于有明确客观标准的任务，例如数学问题求解。

2. **RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）**：在SFT的基础上，结合PPO算法，但奖励信号来自人类反馈。具体而言，人类对模型的输出进行评价，指出更优的回答，或通过偏好对比的方式提供反馈。这些人类反馈可以直接用于指导模型优化，或者用于训练一个奖励模型，后续由奖励模型对模型输出进行评估。RLHF的优势在于引入了人类的主观判断，使模型的输出更符合人类偏好，适用于需要复杂评价和主观判断的任务。

3. **DPO（Direct Preference Optimization）方法：** 与前两种方法不同，DPO 不使用强化学习算法（如 PPO），而是采用监督学习的方法，直接优化模型。在 SFT（有监督微调）之后，**引入一个参考模型**（通常为经过 SFT 的初始模型，参数固定不更新），利用人类偏好数据和参考模型，构建损失函数，对模型进行微调。具体来说：

   - **收集人类偏好数据**：收集人类对模型输出的偏好数据，如在给定的多个回答中标注出人类更喜欢的那个，形成偏好对（首选和非首选响应）。

   - **参考模型的引入**：参考模型提供了一个稳定的概率基准，用于与当前模型的输出概率进行比较，防止模型在优化过程中过度偏离预训练的语言分布。

   - **构建损失函数**：设计一个损失函数，利用当前模型和参考模型对首选和非首选响应的对数概率，鼓励模型倾向于生成被人类偏好的输出。损失函数通常包含对数概率差和正则项，以确保训练的稳定性。

   - **直接优化模型参数**：通过最小化该损失函数，直接调整模型参数，使其更倾向于生成被人类偏好的输出。

     DPO 避免了强化学习中的试错过程，训练更稳定，效率更高，适用于有大量人类偏好数据的场景。**同时，参考模型的引入有助于保持模型生成质量的稳定性，防止模型偏离预训练分布过远。**

   ## Azure OpenAI的DPO

   目前AOAI支持DPO：

   *https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning?tabs=azure-openai%2Cturbo%2Cpython-new&pivots=programming-language-studio#direct-preference-optimization-dpo-preview*

   ```
   {  
     "input": {  
       "messages": {"role": "system", "content": ...},  
       "tools": [...],  
       "parallel_tool_calls": true  
     },  
     "preferred_output": [{"role": "assistant", "content": ...}],  
     "non_preferred_output": [{"role": "assistant", "content": ...}]  
   }  
   ```

   Jsonal format:

   ```
   {{"input": {"messages": [{"role": "system", "content": "You are a chatbot assistant. Given a user question with multiple choice answers, provide the correct answer."}, {"role": "user", "content": "Question: Janette conducts an investigation to see which foods make her feel more fatigued. She eats one of four different foods each day at the same time for four days and then records how she feels. She asks her friend Carmen to do the same investigation to see if she gets similar results. Which would make the investigation most difficult to replicate? Answer choices: A: measuring the amount of fatigue, B: making sure the same foods are eaten, C: recording observations in the same chart, D: making sure the foods are at the same temperature"}]}, "preferred_output": [{"role": "assistant", "content": "A: Measuring The Amount Of Fatigue"}], "non_preferred_output": [{"role": "assistant", "content": "D: making sure the foods are at the same temperature"}]}
   }
   ```

   

4. **RLAIF（Reinforcement Learning from AI Feedback，基于AI反馈的强化学习）**：这是SFT、PPO和AI反馈的结合。在SFT后，使用PPO进行强化学习，然而奖励信号不是来自人类，而是来自辅助的AI模型（如奖励模型）的反馈。AI模型对主模型的输出进行评估，提供奖励信号。这样的方法节省了人类评价的成本，但依赖于辅助AI模型的质量。

**总结：**

四种方法中，ReFT、RLHF和RLAIF都使用了PPO作为强化学习算法，区别在于奖励信号的来源不同：ReFT来自自动化程序的评估，RLHF来自人类反馈，RLAIF来自AI模型的反馈。只有DPO方法使用了监督学习的方式，不采用PPO等强化学习算法，而是直接利用人类偏好数据优化模型。



**那么，DPO的意义是什么？**

强化学习方法（如PPO）需要模型在环境中自行探索，通过试错学习获得奖励信号，这个过程复杂，训练不稳定，且调参困难。相比之下，监督学习的方法更直接高效：通过人类提供的偏好数据，直接告诉模型什么是好的输出，构建损失函数，调整模型参数。这样避免了强化学习的复杂性，训练过程更稳定，效率更高，特别适用于有大量人类偏好数据的情况。

举个例子，使用强化学习的模型就像是在黑暗中摸索前进，需要不断试错；而使用监督学习的DPO方法，就像是有人直接给了一张地图，告诉你正确的前进路线。采用监督学习，可以更快地达到目标。

选择方法的依据：如果任务有明确的客观评价标准，适合使用ReFT，通过自动化程序评估模型输出。如果希望模型的输出更符合人类主观偏好，并有大量人类反馈数据，可以选择RLHF或DPO方法。RLHF使用强化学习算法，需要模型与环境交互，训练复杂；DPO则采用监督学习，训练更简单高效。若人类反馈成本高，可以考虑RLAIF，用辅助AI模型提供反馈信号。

## 强化学习类与有监督微调类的区别

**一、强化学习**

强化学习的基本框架

强化学习（Reinforcement Learning，RL）是一个智能体（Agent）与环境（Environment）互动的过程。智能体的目标是通过学习策略（Policy），在与环境的交互中最大化累积的奖励（Reward）。

 

![图片](https://mmbiz.qpic.cn/mmbiz_png/akGXyic486nUmuCC75DMa6J6ZShqhcHx0wRmzErG3eIKhpxmNeHU4GyAm491eAhwXhibweP4qAHWqH4kuPLOIQSA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### **1. 关键组成部分**

##### **（1）状态空间（State Space，S）**

- **定义**：环境可能处于的所有状态的集合。
- **表示**：在时间步 t，环境的状态记为 **s_t**。

##### **（2）动作空间（Action Space，A）**

- **定义**：智能体能够执行的所有可能动作的集合。
- **表示**：在时间步 t，智能体采取的动作记为 **a_t**。

##### **（3）策略（Policy，π）**

- **定义**：策略是从状态到动作的映射，决定智能体在每个状态下选择哪个动作。
- **表示**：
  - **确定性策略**：a_t = π(s_t)，即在状态 s_t 下，总是选择相同的动作。
  - **随机性策略**：根据概率分布选择动作，a_t ~ π(a | s_t)。

##### **（4）奖励函数（Reward Function，R）**

- **定义**：定义了智能体在特定状态下采取某个动作后获得的即时奖励。
- **表示**：R(s_t, a_t)，表示在状态 s_t 采取动作 a_t 后获得的奖励。

##### **（5）状态转移概率（State Transition Probability，P）**

- **定义**：描述了智能体在当前状态采取某个动作后，环境转移到下一个状态的概率。
- **表示**：P(s_{t+1} | s_t, a_t)，表示在状态 s_t 采取动作 a_t 后，转移到状态 s_{t+1} 的概率。

##### **（6）折扣因子（Discount Factor，γ）**

- **定义**：用于平衡即时奖励和未来奖励的重要性，取值范围 0 ≤ γ ≤ 1。
- **特点**：
  - 当 γ 接近 0 时，更关注即时奖励。
  - 当 γ 接近 1 时，更关注长期累积奖励。

##### **（7）价值函数（Value Function）**

- **状态价值函数（V^π(s)）**：

  - **定义**：在策略 π 下，从状态 s 开始所能获得的期望累积奖励。

  - **表示**：

    ```
    V^π(s) = E_π [ R(s_t, a_t) + γ * V^π(s_{t+1}) | s_t = s ]
    ```

- **行动价值函数（Q^π(s, a)）**：

  - **定义**：在策略 π 下，从状态 s 采取动作 a 后所能获得的期望累积奖励。

  - **表示**：

    ```
    Q^π(s, a) = E_π [ R(s, a) + γ * Q^π(s_{t+1}, a_{t+1}) ]
    ```

#### **2. 举例说明**


**例子：自动驾驶小车**

- **环境**：一个简单的二维道路网格，小车需要从起点到达终点。
- **状态（s_t）**：小车当前的位置坐标，例如 (x, y)。
- **动作（a_t）**：小车可以选择的移动方向，例如：向上、向下、向左、向右。
- **策略（π）**：决定小车在每个位置选择哪个方向移动的规则。
- **奖励函数（R）**：
  - **到达终点**：奖励 +100。
  - **每移动一步**：惩罚 -1（鼓励尽快到达终点）。
  - **撞到障碍物**：惩罚 -50。
- **状态转移概率（P）**：确定性环境中，采取动作后，会确定地移动到相应的新位置。但如果引入随机性，例如，有一定概率因路面湿滑而滑动到其他位置，则需要用 P(s_{t+1} | s_t, a_t) 描述这种概率。
- **折扣因子（γ）**：设为 0.9，使得小车既关注即时奖励（避免无谓的移动），也重视最终到达终点的高额奖励。



#### **3. 强化学习的目标**

- **目标**：学习一个最优策略 π*，使得智能体在环境中行动时，累积的期望奖励最大化。

- **数学表示**：

  ```
  π* = arg max_π E_π [ ∑_{t=0}^∞ γ^t * R(s_t, a_t) ]
  ```

  其中，E_π 表示在策略 π 下，对所有可能的状态和动作序列的期望。



#### **4. 强化学习的关键要素总结**

- **状态（s_t）**：对环境的感知，智能体当前所处的位置。
- **动作（a_t）**：智能体可以执行的动作集合。
- **策略（π）**：指导智能体选择动作的规则。
- **奖励信号（R）**：即时反馈，评估智能体动作的好坏。
- **折扣因子（γ）**：衡量即时奖励和未来奖励的重要性。
- **价值函数（V^π(s), Q^π(s, a)）**：用于评估策略的优劣，指导策略改进。
- **学习算法**：用于更新策略和价值函数的方法，例如 Q-learning、SARSA、策略梯度方法、Actor-Critic 等。



#### **5. 总结**


在强化学习中，智能体通过不断与环境交互，根据奖励信号调整策略，目的是找到能使累积奖励最大化的最优策略。关键组件包括：

- **状态空间（S）\**和\**动作空间（A）**

- **策略（π）**

- **奖励函数（R）**

- **折扣因子（γ）**

- **价值函数（V^π(s), Q^π(s, a)）**

- **学习算法**

  

**二、奖励函数与奖励模型**

强化学习确实需要奖励信号来指导智能体（Agent）学习最优策略，以最大化累积奖励。但是，需要明确的是，强化学习需要奖励信号，但并不一定必须通过奖励模型来获得。奖励信号可以由预先定义的奖励函数直接提供，也可以通过奖励模型来获得，这取决于具体的任务和环境。

**奖励函数与奖励模型的区别**

**1. 奖励函数（Reward Function）**

- **定义**：由人类**手工设计的明确规则或公式**，根据智能体的状态和动作，直接计算即时的奖励值。

- **特点**：

  - **规则清晰**：明确规定了在特定情况下的奖励或惩罚。
  - **直接计算**：无需训练或学习过程，直接根据状态和动作计算奖励。
  - **可解释性强**：由于规则是手工制定的，容易理解和解释。

- **适用场景**：**评价标准明确、客观、易于量化**的任务。

- **示例**：

  在之前的**自动驾驶小车**的例子中，我们定义了如下奖励函数：

  这个奖励函数是由人类明确设计的，智能体可以根据这个函数直接计算每个动作的奖励。

  - **到达终点**：奖励 **+100**。
  - **每移动一步**：惩罚 **-1**，鼓励最短路径。
  - **撞到障碍物**：惩罚 **-50**，避免危险行为。

**2. 奖励模型（Reward Model）**

- **定义**：通过**机器学习方法训练得到的模型**，用于**预测或评估**智能体行为或输出的奖励值。

- **特点**：

  - **数据驱动**：依赖大量人类反馈数据进行训练。
  - **适应复杂评价**：能够捕捉**复杂、主观**的评价标准，适用于难以手工设计奖励函数的任务。
  - **需要训练**：模型需要通过学习过程来调整参数，以准确预测奖励。

- **适用场景**：**评价标准复杂、主观、难以量化**的任务。

- **示例**：

  在**对话生成模型**的训练中，例如ChatGPT，难以手工设计一个明确的奖励函数来评估回复的质量。因此，我们：

  - **收集人类反馈数据**：人类对模型生成的回复进行评价，如打分或偏好比较。
  - **训练奖励模型**：使用这些数据，训练一个模型，使其能够预测回复的质量评分。
  - **应用于强化学习**：在训练过程中，模型生成回复后，奖励模型对其进行评估，给予奖励信号，指导模型优化。

**总结**

- **强化学习必须有奖励信号**，它是智能体学习的关键驱动力。
- **奖励信号的来源**可以是：
  - **奖励函数**：手工设计，适用于评价标准明确的任务。
  - **奖励模型**：通过训练得到，适用于评价标准复杂、主观的任务。
- **是否需要奖励模型取决于具体的任务需求**：
  - **简单、规则明确的任务**：奖励函数足以满足需求。
  - **复杂、主观的任务**：需要奖励模型来捕捉人类的评价标准。



因此，强化学习一定需要奖励信号，但不一定必须使用奖励模型。**在很多传统的强化学习应用中，手工设计的奖励函数已经足够有效。然而，在某些复杂领域，特别是涉及人类主观评价的任务中，奖励模型变得必要。



**三、直接偏好优化（Direct Preference Optimization，DPO）**

直接偏好优化（DPO）是一种利用人类偏好数据直接优化策略的方法，旨在使模型的行为更符合人类的期望。与传统的强化学习相比，DPO不需要训练单独的奖励模型，而是直接使用人类的偏好来指导模型的优化。 

 

**1. 关键组成部分**

**（1）策略模型（Policy Model）**

- **定义**：需要优化的模型，决定智能体在每个状态下采取的动作。
- **表示**：策略模型记为 πₜθ，其中 θ 是模型的参数。

**（2）人类偏好数据**

- **定义**：由人类对模型行为的偏好反馈，通常以成对比较的形式。
- **表示**：给定同一输入，模型生成两个不同的输出，称为 A 和 B。人类评估者选择他们更喜欢的输出。

**（3）参考策略（Reference Policy）**

- **定义**：初始的未优化策略模型，用于在训练过程中稳定新策略的行为。
- **作用**：防止策略模型在优化过程中偏离初始行为过远，保持模型的稳定性。

**（4）损失函数（Loss Function）**

- **定义**：基于人类偏好数据构建的损失函数，用于优化策略模型的参数。
- **目标**：最大化策略模型生成被人类偏好选择的输出的概率。

**2. 举例说明**

**例子：通过DPO优化小车的导航策略**

**背景**：

- **环境**：一个二维的网格世界，智能小车需要从起点到达终点，途中可能有障碍物。

- **状态（s_t）**：小车当前的位置坐标 (x, y)。

- **动作（a_t）**：小车可以选择的移动方向：上、下、左、右。

- **目标**：不仅希望小车能够最短路径到达终点，还希望其行驶路径符合人类的偏好，例如避免特定区域（如危险地带）或经过风景优美的路线。

  **步骤**：

**（1）初始策略模型**

- **设定**：小车有一个初始的策略模型 π₀，它可能是基于最短路径算法生成的。
- **问题**：这个策略可能会让小车经过危险区域，或错过风景优美的路线，不符合人类的偏好。

**（2）收集人类偏好数据**

- **生成候选路径**：
  - 给定起点和终点，策略模型 π₀ 生成不同的行驶路径。
- **人类评估者进行比较**：
  - **安全性**：避免危险区域。
  - **美观性**：经过风景优美的地方。
  - **效率**：路径长度适中。
  - 对于每一对候选路径 A 和 B，人类评估者根据自己的偏好选择他们更喜欢的路径。
  - **偏好因素**可能包括：

**（3）构建数据集**

- **数据形式**：[(起点, 终点), 路径A, 路径B, 人类偏好]
- **示例**：
  - **起点**：坐标 (0, 0)
  - **终点**：坐标 (5, 5)
  - **路径A**：经过危险区域但路径最短。
  - **路径B**：稍长一些，但避开危险区域并经过美景。
  - **人类偏好**：选择路径B。

**（4）定义损失函数**

- **目标**：优化策略模型 πₜθ，使其更倾向于生成被人类偏好选择的路径。

- **损失函数设计**：

  - 如果人类偏好路径 B，但策略模型更倾向于路径 A，那么 Δ > 0，损失函数会较大，促使模型调整参数。

  - 通过最小化损失函数，使策略模型更倾向于生成被人类偏好的路径。

  - **Δ = s_θ(A) - s_θ(B)**，表示策略模型对路径 A 和 B 的偏好得分差异。

  - **s_θ(X)** 是策略模型给路径 X 计算的得分（例如，路径的对数概率）。

  - **σ(Δ)** 是 Sigmoid 函数，将差值映射到 (0,1) 区间。

  - 对于每个偏好数据：

    ```
    L(θ) = -log(σ(Δ))
    ```

    其中：

  - **直观理解**：

#### **（5）加入参考策略**

- **参考策略 π₀**：初始的策略模型。

- **正则化项**：防止策略模型偏离初始策略过远，保持路径的合理性。

  ```
  R(θ) = KL(πₜθ || π₀)
  ```

  其中：

  - **KL** 表示 Kullback-Leibler 散度，衡量策略模型与参考策略之间的差异。

- **总损失函数**：

  ```
  L_total(θ) = L(θ) + λ * R(θ)
  ```

  - **λ** 是权衡损失和正则化项的超参数，控制模型偏离初始策略的程度。

#### **（6）优化策略模型**

 

- **最小化总损失函数 L_total(θ)**，更新策略模型的参数 θ。
- **迭代训练**：重复上述过程，直到模型在验证数据上的表现达到预期。

#### **（7）结果**

- **优化后的策略模型 πₜθ**：
  - 更倾向于为小车规划符合人类偏好的路径。
  - 在遇到类似的导航问题时，能够自动选择既安全又美观的路线。

**3. DPO的优势**

- **直接利用人类偏好数据**：不需要额外训练奖励模型，减少了复杂性。
- **训练稳定**：通过引入参考策略，防止模型过度拟合，保持行为合理性。
- **效率更高**：相比于传统的强化学习方法，DPO的训练过程更简单，资源消耗更少。
- **适用性强**：适用于需要根据人类偏好调整策略的任务。

**4. 总结**

- **DPO**是一种有效的策略优化方法，通过直接利用人类的偏好数据，优化智能体的策略，使其行为更符合人类的期望。
- **在小车导航的例子中**，DPO方法帮助智能体学习如何规划既安全又美观的路径，提升了用户体验。
- **关键思想**：
  - **利用人类偏好数据构建损失函数**，直接优化策略模型。
  - **引入参考策略作为正则化**，确保模型的稳定性。

**四、各种训练/微调技术采用的技术**

1. **SFT（Supervised Fine-Tuning，有监督微调）**

   - 是否属于强化学习：不属于强化学习。

   - 归属：有监督学习。

   - **解释：**

     SFT是对预训练模型（如大型语言模型）进行有监督的微调。它使用已标注的数据（输入-输出对）对模型进行训练，使其在特定任务上表现更好。模型直接学习输入与期望输出之间的映射关系，不涉及强化学习的概念。



2. **ReFT（Reinforced Fine-Tuning，强化微调）**

- 是否属于强化学习：属于强化学习。

- **使用奖励函数还是奖励模型：\**使用\**奖励函数**。

- **解释：**

  ReFT在SFT的基础上，使用强化学习算法（如PPO）进一步优化模型。在ReFT中，奖励信号通常是通过**明确的奖励函数**计算的，例如根据模型输出与标准答案的匹配程度。模型根据奖励函数提供的即时奖励信号，使用强化学习算法调整参数，以提升性能。这种方法适用于有明确评价标准的任务，例如数学问题求解。



3. **RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）**

- 是否属于强化学习：属于强化学习。

- **使用奖励函数还是奖励模型：\**使用\**奖励模型**。

- **解释：**

  RLHF结合了SFT和强化学习。在SFT之后，模型通过收集**人类反馈**来了解其输出的质量。人类对模型的输出进行评分或排序，这些反馈用于训练一个**奖励模型**。该奖励模型可以预测人类对不同输出的偏好或满意度。在强化学习阶段，模型使用奖励模型提供的奖励信号，利用算法（如PPO）优化其策略，使输出更符合人类期望。



4. **RLAIF（Reinforcement Learning from AI Feedback，基于AI反馈的强化学习）**

- 是否属于强化学习：属于强化学习。

- **使用奖励函数还是奖励模型：\**使用\**奖励模型**。

- **解释：**

  RLAIF与RLHF类似，但区别在于奖励信号的来源。RLAIF使用预先训练的**AI模型**（而非人类）对主模型的输出进行评价，提供奖励信号。这个AI模型充当**奖励模型**的角色，通过预测输出的质量或符合性，引导主模型进行优化。这种方法减少了对人类反馈的依赖，但效果取决于AI反馈模型的质量。



5. **DPO（Direct Preference Optimization，直接偏好优化）**

- 是否属于强化学习：不属于强化学习。

- 归属：有监督学习，直接优化模型参数。

- **解释：**

  DPO方法不使用强化学习算法，而是直接利用人类偏好数据进行**监督学习**。在收集到人类对模型输出的偏好后，构建一个损失函数，使模型倾向于生成被人类偏好的输出。通过最小化这个损失函数，直接优化模型参数。这种方法避免了强化学习的复杂性，训练过程更稳定，适用于有大量人类偏好数据的场景。



6. **PPO（Proximal Policy Optimization，近端策略优化）**

- 是否属于强化学习：属于强化学习。

- **使用奖励函数还是奖励模型：\**取决于应用场景，可以使用\**奖励函数**或**奖励模型**。

- **解释：**

  PPO是一种强化学习算法，用于更新策略网络。它通过限制策略更新的幅度，确保训练的稳定性和效率。PPO本身是一种算法工具，常用于需要强化学习的模型，如ReFT和RLHF。它可以根据不同的奖励信号进行优化：

  - **使用奖励函数：**当奖励信号来自明确的计算（如ReFT中的正确答案比较）时，PPO使用奖励函数。
  - 使用奖励模型：当奖励信号来自训练的奖励模型（如RLHF中的人类反馈模型）时，PPO使用奖励模型。



## **ReFT简介**

### OpenAI的 ReFT

监督式微调 (SFT) 涉及采用预先训练的模型，并使用监督式学习技术利用额外数据对其进行调整。在实践中，当目标是使模型的输出或格式与特定数据集保持一致，或确保模型遵循某些指令时，SFT 效果最佳。

虽然监督微调和强化微调都依赖于标记数据，但它们的使用方式不同。在 SFT 中，标记数据直接驱动模型的更新。模型将其视为目标输出，并调整其参数以缩小其预测输出与已知正确答案之间的差异。

在 RFT 中，模型对标签的接触是间接的，因为它主要用于创建奖励信号，而不是直接目标。这就是为什么模型在 RFT 中需要的标记数据更少的原因——模型旨在寻找模式来产生我们想要的输出，而不是直接产生我们的输出，这保证了更强的*泛化倾向*。

我们用这张表来总结一下差异：

| 特征                   | 监督微调（SFT）                                        | 强化微调（RFT）                                              |
| ---------------------- | ------------------------------------------------------ | ------------------------------------------------------------ |
| **核心理念**           | 直接在标记数据上训练模型以匹配所需的输出。             | 使用“ Grader ”为模型提供奖励以生成所需的输出。               |
| **标签使用**           | 直接作为模型模仿的目标。                               | 间接用于为模型创建奖励信号。                                 |
| **数据效率**           | 需要更多标记数据。                                     | 由于泛化，可能需要较少的标记数据。                           |
| **人类参与**           | 仅在初始数据标记中。                                   | 仅在设计“评分器”功能。                                       |
| **概括**               | 可能过度拟合训练数据，限制泛化。                       | 由于关注模式和奖励，因此具有更高的概括潜力。                 |
| **与人类偏好保持一致** | 有限，因为它完全依赖于模仿标记数据。                   | 如果“评分器”准确反映人类偏好，则可以更好地进行调整。         |
| **示例**               | 微调语言模型以生成特定类型的文本格式（如诗歌或代码）。 | 训练语言模型来生成创意内容，并由“评分员”根据原创性和连贯性进行评判。 |

训练数据范例如下，训练中，并不把答案直接放入到训练集。

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/Comparison-of-Various-Fine-Tuning-Methods/images/3.png)

训练过程中，模型可能包含或者不包含正确答案：

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/Comparison-of-Various-Fine-Tuning-Methods/images/4.png)

创建训练集和校验集的jsonal文件：

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/Comparison-of-Various-Fine-Tuning-Methods/images/5.png)

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/Comparison-of-Various-Fine-Tuning-Methods/images/6.png)

构建奖励函数：

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/Comparison-of-Various-Fine-Tuning-Methods/images/7.png)

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/Comparison-of-Various-Fine-Tuning-Methods/images/8.png)

这个JSON文件（grader.json）的内容定义了一个评分系统的配置。具体来说，这个配置文件定义了如何对某个对象进行评分。让我们逐行解析这个文件的内容：

1. `"type": "object-grader"`：

   - 这行定义了评分器的类型为“object-grader”，表示这是一个用于评分对象的评分器。

2. `"property_graders": { ... }`：

   - 这个字段定义了对对象属性进行评分的具体规则。在这个例子中，针对对象的属性“genes”有特定的评分规则。

3. `"genes": { "type": "inverse-rank-grader" }`：

   - 这个定义了对属性“genes”的评分器类型为“inverse-rank-grader”。“inverse-rank-grader”通常意味着评分是基于逆序排名的。
   - 逆序排名评分器（inverse-rank-grader）通常的工作原理是根据元素在列表中的位置来计算得分。位置越靠前，得分越高。例如，排名第一的元素可能得分为1，第二名得分为0.5，第三名得分为0.33，等等。

4. `"calculate_output": "genes"`：

   - 这行定义了评分器最终输出的计算方式，是根据“genes”属性来计算输出的分数。

     

     总结：
     这个JSON配置文件定义了一个评分系统，使用“inverse-rank-grader”对对象的“genes”属性进行评分，并根据这个评分来计算最终的输出。具体来说，“inverse-rank-grader”评分器根据“genes”属性中的元素在列表中的位置来计算得分，位置越靠前得分越高。

     结合之前提到的得分情况（0.7分），可以推测评分机制可能是基于“inverse-rank-grader”的逆序排名得分。例如，如果“FOXE3”在列表中的排名较高（比如第一或第二位），那么它可能会得到一个较高的分数。具体的得分计算规则需要查看“inverse-rank-grader”的实现细节来确定。

设置训练超参：

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/Comparison-of-Various-Fine-Tuning-Methods/images/9.png)

训练结果：

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/Comparison-of-Various-Fine-Tuning-Methods/images/10.png)

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/Comparison-of-Various-Fine-Tuning-Methods/images/11.png)

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/Comparison-of-Various-Fine-Tuning-Methods/images/12.png)

### 字节的ReFT

先看ReFT论文中的流程图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/akGXyic486nUpgIIJic3v3UHiaRriaapjX2omaoJWqB4dr3dQyGEkDMjjR5JeI8LibRVX9icuCAiarOA0kMgPfWhoqiamg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如上图所示，ReFT，该框架结合了监督微调（Supervised Fine-Tuning, SFT）和强化微调（Reinforced Fine-Tuning, ReFT）的方法。以下是对图中各部分的详细解释：

1. **监督微调（Supervised Fine-Tuning）**：

   - **模型（Model）**：初始模型通过多个SFT周期（epochs）在训练数据上进行训练。训练数据包含问题（x），推理链（CoT，e）和答案（y）。
   - **SFT Epochs**：模型在训练数据上进行多个周期的训练，以学习如何从问题（x）和推理链（e）生成正确的答案（y）。
   - **不同阶段的模型**：图中展示了经过不同训练阶段后的模型表情变化，表示模型逐渐变得更好。

2. **强化微调（Reinforced Fine-Tuning）**：

   - **预热阶段（Warm-up）**：在进入强化学习之前，模型通过SFT进行预热。
   - **问题（question）**：模型接受一个输入问题（x）。
   - **On-Policy Sampling**：在策略内采样，模型生成一个推理链和答案（e', y'）。
   - **Golden Reward**：对生成的答案（y'）与正确答案（y）进行比较，给予奖励信号。如果答案正确，给予正奖励（√），否则给予负奖励（×）。
   - **强化学习（Reinforcement Learning）**：利用奖励信号来调整模型参数，以提高模型在相同数据上的表现。

3. **最终策略（Final Policy）**：

   - 经过SFT和ReFT训练后，模型形成最终策略，可以更准确地回答问题。

     图例说明了在GSM8K数据集上，一个问题（x）、推理链（e）和答案（y）的示例。通过多个SFT周期对训练数据进行迭代，并使用ReFT方法从SFT进行预热，然后在相同数据上进行强化学习训练。

## **TPO的流程**

先看Thought Preference Optimization（TPO）的流程：

![图片](https://mmbiz.qpic.cn/mmbiz_png/akGXyic486nUpgIIJic3v3UHiaRriaapjX2obrFTylPby9mlhw3NUiaJicSIAianv6WrYjTbaCe24w2nic8xrHLOUo4VLg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- TPO 方法由三个主要部分组成：
  1. SFT（有监督微调）：提供了模型的基础。
  2. 思维生成（Thought Generation）：让模型在回答前进行内部思考。
  3. DPO（直接偏好优化）：利用 AI 判别模型生成的偏好对，直接优化模型参数。
- TPO 的创新之处在于：
  - 引入了思维生成，增强模型的推理和规划能力。
  - 利用 AI 判别模型的反馈，降低了对人类偏好数据的依赖。
  - 使用 DPO 方法，简化了训练流程，提高了训练效率。



**TPO = SFT + 思维生成（Thought Generation）+ DPO（偏好优化）**

需要指出的是，**DPO 方法（Direct Preference Optimization，直接偏好优化）** 通常直接使用人类的偏好数据，并且 **引入了参考模型**。在传统的应用中，DPO 方法依赖于人类对模型输出的偏好反馈，并通过 **参考模型** 来构建损失函数，直接优化模型参数。然而，**TPO 方法（Thought Preference Optimization，思维偏好优化）** 将 DPO 的应用扩展到了使用 **AI 模型生成的偏好数据**，这一创新使得模型能够在缺乏人类偏好数据的情况下，仍然利用偏好优化方法进行训练。需要强调的是，对于 DPO 方法而言，**参考模型和偏好数据的结合是关键**。无论偏好数据的来源是人类还是 AI 模型，DPO 方法都通过 **参考模型** 和偏好数据来构建损失函数，直接优化模型，而 **不涉及强化学习算法**。 
**上面 TPO 方法的具体解释如下：**

**1. SFT（Supervised Fine-Tuning，有监督微调）：**

- **起点模型（Seed Model）**：TPO 方法以一个已经过有监督微调的预训练模型作为基础（即经过 SFT 的模型），例如 Llama-3-8B-Instruct。

  **2. 思维生成（Thought Generation）：**

- **引入思维过程**：在生成最终回答之前，模型通过特定的提示（Prompt），被引导生成内部的思维过程（Thought）。

- **思维与回答的分离**：生成的输出分为 **思维部分** 和 **回答部分**，思维部分在最终呈现给用户时被隐藏。

  **3. DPO（Direct Preference Optimization，直接偏好优化）：**

- **参考模型的引入**：使用经过 SFT 的模型作为 **参考模型**，参数固定不更新。

- **偏好数据来源**：偏好对（Preference Pairs）来自于 **AI 判别模型** 对模型输出的评估，而非传统的人类偏好数据。

- **AI 判别模型**：

  - **Self-Taught Evaluator（STE）**：基于大型语言模型（如 Llama-3-70B-Instruct），用于对两个回答进行比较，输出偏好结果。

  - **ArmoRM**：一个奖励模型，直接对单个回答进行评分。

    **优化过程：**

- **生成候选输出**：模型针对每个输入指令，生成多个包含 **思维和回答** 的候选输出。

- **评估回答部分**：将 **回答部分** 输入到 **AI 判别模型**，进行评分或偏好比较。

- **构建偏好对**：根据评估结果，选出 **最佳和最差回答**，形成偏好对（Chosen 和 Rejected）。

- **使用 DPO 优化**：利用这些偏好对，结合 **参考模型**，使用 DPO 方法构建损失函数，直接优化模型参数。

## **PPO和DPO的本质区别**

### 1. 强化学习的本质


强化学习就像训练一只小狗做动作。

**比方说：**

- **训练小狗**：当小狗做对了动作（比如坐下、握手），我们就给它一块小零食作为奖励；当它做错了，我们就不给奖励。

- **目的**：经过多次训练，小狗会明白，做对了就有奖励，于是它会更愿意做我们希望的动作。

  **在机器学习中：**

- **模型与环境互动**：模型（相当于小狗）在某个环境中，根据当前的情况，选择一个动作。

- **获得奖励或惩罚**：环境会根据模型的动作，给出一个奖励（奖励值可以是正的，也可以是负的）。

- **优化目标**：模型的目标是学习到一种策略，使得它在与环境的长期互动中，累积的奖励最大化。

  **简单理解：**

- **强化学习 = 试错学习 + 奖励机制**

- 模型通过不断尝试不同的动作，学习到哪些行为能够获得更多的奖励。

  **举个例子：**

- 游戏中的机器人：想象一个机器人在迷宫中寻找出口。

  - **到达出口**：奖励 +10 分。
  - **撞墙**：奖励 -1 分。
  - **其他情况**：奖励 0 分。

- 行动选择：

  - 每走一步，机器人会根据当前的位置（状态），选择向上、下、左、右移动（动作）。

- 模型目标：

  - 学会一条最优路径，快速找到出口，获得最高的累计奖励。

### 2. 偏好数据的直接优化（DPO）的本质

**直接偏好优化（DPO）** 是一种直接利用偏好数据和参考模型来优化模型的方法。

**比方说：**

- **模型生成两个回答**：对于同一个问题，当前模型（Policy Model）生成了回答 A 和回答 B。

- **获得偏好反馈**：我们请人类评估者或辅助的 AI 模型告诉我们，哪个回答更好。

  - 例如，人类评估者说：“回答 A 比回答 B 好。”

- **引入参考模型**：

  - **参考模型（Reference Model）**：通常是经过监督微调（SFT）的初始模型，其参数在 DPO 训练过程中保持固定不变。
  - **作用**：提供一个稳定的概率基准，防止当前模型在优化过程中偏离预训练的语言分布。

- **构建损失函数**：

  - **利用当前模型和参考模型**，对两个回答的对数概率进行计算。
  - **损失函数**：设计一个损失函数，使当前模型更倾向于生成被人类偏好的回答，同时限制模型的更新幅度，保持训练稳定性。

- **直接优化模型**：

  - 通过最小化损失函数，直接调整当前模型的参数，使其更倾向于生成被偏好的回答 A。

    **与强化学习的区别：**

- **不需要复杂的奖励函数**：DPO 不需要设计一个数值化的奖励函数或与环境交互的机制，只需利用偏好比较的信息和参考模型的概率基准。

- **不需要环境交互**：DPO 直接使用已有的偏好数据和参考模型进行优化，无需模型与环境持续交互，也不涉及即时奖励信号。

  **简单理解：**

- **DPO = 利用偏好比较和参考模型直接调整模型参数**

- 模型根据哪些输出被偏好，结合参考模型的概率信息，直接优化自身，倾向于生成更符合人类偏好的输出。

  **举个例子：**

- **训练聊天机器人**：

  1. **模型生成两个回答**：针对用户提出的问题，当前模型生成了两个候选回答。

     - **回答 1**：“好的，我会帮您查询相关信息。”
     - **回答 2**：“我不知道，你自己去查吧。”

  2. **获取偏好数据**：人类评估者比较两个回答，选择更好的一个。

     - 人类评估者表示更喜欢 **回答 1**，因为它更有礼貌、服务态度更好。

  3. **引入参考模型**：

     - 使用经过 SFT 的初始模型作为参考模型，其参数固定。

  4. **构建损失函数**：

     - 计算 **当前模型** 和 **参考模型** 对 **回答 1** 和 **回答 2** 的对数概率。
     - 设计损失函数，鼓励当前模型提高对被偏好回答（回答 1）的概率，降低对不被偏好回答（回答 2）的概率，同时参考模型的信息用于稳定训练。

  5. **直接优化模型**：

     - 通过最小化损失函数，更新当前模型的参数，使其更倾向于生成类似 **回答 1** 的内容。

       **总结：**

- **DPO 方法** 通过结合 **人类偏好数据** 和 **参考模型**，在监督学习的框架下直接优化模型参数。

- **参考模型** 的作用是提供一个稳定的基准，防止模型过度偏离预训练的语言分布，确保训练过程的稳定性和生成质量。

- **与强化学习的区别**：

  - DPO 不涉及与环境的交互和即时奖励信号，避免了强化学习中的复杂性和不稳定性。
  - DPO 的优化过程更直接、更稳定，适合在有大量偏好数据的场景下使用。

### **3. 关于 DPO 中的“交互”**


在 DPO 的描述中，模型需要生成多个回答，然后人类评估者或辅助的 AI 判别模型提供偏好反馈（**注：在 TPO 的 DPO 部分，偏好数据可能来自 AI 模型**）。这似乎也涉及到一些交互。那么，DPO 不是也有交互吗？

**的确，DPO 涉及到模型与人类评估者或 AI 判别模型之间的“交互”，因为需要收集偏好数据。然而，这种交互与强化学习中的交互有本质的不同。**

#### **1. DPO 中的“交互”**

- 数据收集阶段的交互：
  - 在 DPO 中，模型会为每个输入生成多个候选输出（如回答）。
  - **人类评估者或 AI 判别模型**对这些候选输出进行偏好标注，形成偏好对（首选和非首选响应）。
  - **特点**：这种交互是一次性的、离线的，用于构建训练数据集。数据收集完成后，进入模型训练阶段。
- 训练阶段的优化：
  - 一旦偏好数据收集完成，模型的训练过程就是基于这些偏好对，**结合参考模型**，使用监督学习的方法直接优化模型参数。
  - **训练过程中不再需要与人类评估者或环境进行交互**，参考模型的参数在训练中保持固定。

#### **2. 强化学习中的交互**



- 持续的环境交互：
  - 在强化学习（如 PPO）中，模型在训练过程中需要与环境进行持续、动态的交互。
  - 模型根据当前状态选择动作，环境反馈新的状态和奖励信号，模型根据这些信息更新策略。
- 训练过程的复杂性：
  - 强化学习的训练是在线的、迭代的，模型需要不断地试错来学习最优策略。
  - 涉及到 **状态、动作、奖励、价值函数、策略更新** 等复杂概念。

#### **3. 两者的区别**

- 交互的性质和阶段不同：

  - **DPO** 的交互发生在训练前的数据收集阶段，是 **静态的、离线的**。在训练过程中，模型只需利用已获得的偏好数据和参考模型进行优化。
  - **强化学习** 的交互发生在训练过程中，是 **动态的、在线的**。模型需要与环境持续交互，获取即时奖励信号。

- 训练方法不同：

  - **DPO** 使用监督学习方法，结合 **参考模型**，利用偏好对构建损失函数，直接优化模型参数。
  - **强化学习** 需要策略梯度或价值函数等方法，根据环境反馈的奖励信号更新策略，通常采用强化学习算法（如 PPO）。

- 复杂性和稳定性：

  - **DPO** 的训练过程相对简单、稳定，因为它是基于监督学习，参考模型的引入进一步稳定了训练过程。
- **强化学习** 的训练过程更复杂，可能存在不稳定性，需要精心调整超参数和训练策略。

#### **4. 举个例子**


**DPO 的例子：**

- **步骤 1：数据收集**

  - 模型生成两个回答：

    - **回答 1**：“好的，我会帮您查询相关信息。”
  - **回答 2**：“我不知道，你自己去查吧。”
    
  - **人类评估者或 AI 判别模型**对这两个回答进行比较，偏好 **回答 1**。

- **步骤 2：模型优化**

  - **引入参考模型**：使用经过 SFT 的模型作为 **参考模型**，参数固定不更新。
  - 构建损失函数：
    - 利用 **当前模型** 和 **参考模型** 对 **回答 1** 和 **回答 2** 的对数概率进行计算。
    - 根据偏好数据，设计损失函数，鼓励当前模型提高对被偏好回答（回答 1）的概率，降低对不被偏好回答（回答 2）的概率。
  - **优化模型**：通过最小化损失函数，直接调整当前模型的参数，使其更倾向于生成被偏好的输出。

- **特点**：

  - **数据收集和模型训练是分开的**。

  - **训练过程中不需要与人类评估者或环境交互**，参考模型在训练中提供稳定的基准。

    **强化学习的例子：**

- **模型与环境持续交互**：

  - 模型在训练过程中，不断与环境交互，选择动作，观察反馈，获取即时奖励。
  - 例如，**训练一个游戏 AI**，模型需要在游戏环境中不断尝试不同的行动，根据得分（奖励）更新策略。

- **特点**：

  - **训练过程需要持续的、在线的交互**。
  - **模型的行为会影响后续的状态和奖励**，训练过程更加复杂，需要处理环境的不确定性。

#### **5. 总结**

- **直接偏好优化（DPO）**：
  - **交互发生在数据收集阶段**，是离线的、静态的。偏好数据收集完成后，训练过程不再需要交互。
  - **训练过程中使用监督学习方法**，结合参考模型，直接利用偏好数据优化模型。
  - **训练过程简单、稳定**，不涉及环境交互，参考模型的引入确保了训练的稳定性。
- **强化学习**：
  - **交互发生在训练过程中**，是在线的、动态的。模型需要与环境持续交互，获取即时奖励信号。
  - **模型通过与环境的持续交互**，基于奖励信号更新策略，需要处理环境的不确定性。
  - **训练过程复杂**，可能存在不稳定性，需要精心调整训练策略和超参数。

## 几种技术对比

| **比较维度**   | **SFT（有监督微调）**                            | **ReFT（强化微调）**                                         | **RLHF（基于人类反馈的强化学习）**                        | **DPO（直接偏好优化）**                                      | **PPO（近端策略优化）**                            | **RLAIF（基于 AI 反馈的强化学习）**                          | **TPO（思维偏好优化）**                                      |
| -------------- | ------------------------------------------------ | ------------------------------------------------------------ | --------------------------------------------------------- | ------------------------------------------------------------ | -------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **概念**       | 使用标注数据对预训练模型进行有监督的微调         | 在 SFT 基础上，**结合参考模型**，使用自动化评估和 DPO 方法直接优化模型 | 在 SFT 基础上，结合人类反馈和 PPO 算法进行强化学习        | 使用人类偏好数据和**参考模型**，直接优化模型参数，避免强化学习复杂性 | 一种强化学习算法，限制策略更新幅度，保持训练稳定性 | 在 SFT 基础上，使用 AI 模型的反馈，结合 PPO 算法进行强化学习 | 模型在回答前进行内部思考，结合**参考模型**和偏好数据优化模型参数 |
| **实现方法**   | 收集标注数据，最小化模型输出与目标输出之间的损失 | SFT 后，采样模型输出，利用自动化评估或偏好数据，**结合参考模型**使用 DPO 方法直接优化 | SFT 后，收集人类反馈，训练奖励模型，使用 PPO 算法优化策略 | 收集人类偏好数据，**结合参考模型**，构建损失函数，直接优化模型参数 | 与环境交互，计算优势函数，使用剪辑目标函数优化策略 | SFT 后，使用辅助 AI 模型评估，训练奖励模型，使用 PPO 优化策略 | 使用提示引导模型生成思维和回答，利用**参考模型**和偏好数据，使用 DPO 方法优化 |
| **数据需求**   | 大量高质量标注数据                               | 标注数据 + 自动化评估程序或偏好数据 + **参考模型**           | 标注数据 + 大量人类反馈 + 奖励模型                        | 大量人类偏好数据 + **参考模型**                              | 与环境交互产生的数据                               | 标注数据 + 辅助 AI 模型 + 奖励模型                           | 输入数据 + 判别模型 + **参考模型**                           |
| **人类参与**   | 高，需要人类标注数据                             | 低，无需额外人类反馈（如偏好数据来自自动化评估）             | 高，需要大量人类反馈                                      | 高，需要人类偏好数据                                         | 取决于任务，一般无需人类参与                       | 低，无需人类反馈，依赖 AI 模型                               | 低，无需人类思维标注，偏好数据可来自 AI 模型                 |
| **参考模型**   | **否**                                           | **是**，利用参考模型稳定训练                                 | **否**                                                    | **是，关键组成部分**                                         | **否**                                             | **是**，用于稳定训练和对比                                   | **是，关键组成部分**                                         |
| **思维过程**   | 否                                               | 否                                                           | 否                                                        | 否                                                           | 否                                                 | 否                                                           | **是**，模型在回答前生成思维                                 |
| **奖励机制**   | 基于损失函数，最小化预测输出与目标输出的差异     | 利用自动化评估或偏好数据，**结合参考模型构建损失函数**，直接优化 | 人类反馈训练的奖励模型评估输出，给予奖励                  | 基于人类偏好数据和**参考模型**构建损失函数，直接优化模型参数 | 环境提供奖励，计算优势函数，指导策略更新           | 辅助 AI 模型评估输出，训练奖励模型，给予奖励                 | 通过评估回答质量，利用**参考模型构建损失函数**，优化模型参数 |
| **训练复杂度** | 低                                               | 中，需结合参考模型和偏好数据进行优化，训练稳定               | 高，多阶段训练，需要人类反馈和强化学习                    | 中，避免了强化学习的复杂性，训练稳定                         | 中，需要调整超参数，训练稳定                       | 中，需要辅助 AI 模型、奖励模型和强化学习                     | 高，多次迭代训练，优化思维和回答，需考虑参考模型和偏好数据   |
| **优势**       | 简单直接，易于实现                               | 训练稳定，直接优化目标，适用于有自动化评估的任务             | 输出更符合人类期望，提高安全性和自然度                    | 训练稳定，直接优化目标，效率高，避免强化学习复杂性           | 训练稳定性高，样本效率高                           | 降低人类反馈成本，可规模化训练                               | 提升复杂任务表现，无需人类思维数据，多任务适用               |
| **缺点**       | 数据需求高，泛化能力有限                         | 依赖自动化评估或偏好数据的质量，参考模型选择需谨慎           | 人力成本高，训练复杂，需要多阶段训练                      | 依赖偏好数据质量和参考模型的设计                             | 超参数调节复杂，样本效率较低                       | 依赖辅助模型质量，可能引入偏差                               | 训练复杂度高，判别模型质量影响大，思维过程可能不可控         |
| **适用场景**   | 有大量标注数据的任务                             | 有明确评估标准，可自动计算评估或有偏好数据的任务             | 需要高质量输出，强调人类价值观和主观评价的任务            | 有大量人类偏好数据，需简化训练流程的任务                     | 强化学习任务，需与环境交互                         | 无法获得人类反馈但有可靠的 AI 模型评估的任务                 | 复杂任务，多步骤推理，缺乏人类思维标注数据的任务             |

**ReFT（Reinforced Fine-Tuning，强化微调）：**

- **组成**：ReFT = SFT + **参考模型** + DPO
- **过程**：在有监督微调（SFT）的基础上，引入**参考模型**（通常为经过 SFT 的初始模型，参数固定不更新），使用 DPO（直接偏好优化）方法，结合自动化评估或偏好数据，直接优化模型参数。
- **评估方式**：通常通过**自动化程序或偏好数据**对模型输出进行评估，利用**参考模型**和评估结果构建损失函数，直接优化模型。


**RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）：**

- **组成**：RLHF = SFT + PPO + 人类反馈
- **过程**：在 SFT 的基础上，使用 PPO（近端策略优化）进行强化学习，**引入奖励模型**，其奖励信号来自**人类反馈**。模型通过与环境交互，利用奖励信号更新策略。
- **评估方式**：人类对模型输出进行评价，构建偏好数据，训练**奖励模型**。在强化学习过程中，奖励模型对模型输出进行评估，提供奖励信号，指导模型优化。


**DPO 方法（Direct Preference Optimization，直接偏好优化）：**

- **组成**：DPO 方法 = SFT + **参考模型** + DPO
- **过程**：在 SFT 的基础上，引入**参考模型**（参数固定不更新），使用 DPO 方法，利用**人类偏好数据**和参考模型，直接优化模型参数。
- **评估方式**：利用人类偏好数据，结合**参考模型**，构建损失函数，直接优化模型参数，使模型倾向于生成被人类偏好的输出。


**RLAIF（Reinforcement Learning from AI Feedback，基于 AI 反馈的强化学习）：**

- **组成**：RLAIF = SFT + PPO + **AI 反馈** + 奖励模型
- **过程**：在 SFT 的基础上，使用 PPO 进行强化学习，**引入奖励模型**，其奖励信号来自**AI 模型的反馈**。模型通过与环境交互，利用 AI 模型评估的奖励信号更新策略。
- **评估方式**：辅助的 AI 模型对模型输出进行评价，生成偏好数据，训练**奖励模型**。在强化学习过程中，奖励模型对模型输出进行评估，提供奖励信号，指导模型优化。


**TPO（Thought Preference Optimization，思维偏好优化）：**

- **组成**：TPO = SFT + 思维生成（Thought Generation）+ **参考模型** + DPO
- **过程**：在 SFT 的基础上，引入**思维生成**，即模型在输出回答前生成内部的思维过程。然后，使用 DPO 方法，结合**参考模型**，直接优化模型参数，偏好数据来自**AI 判别模型的反馈**。
- **评估方式**：利用**AI 判别模型**对模型输出的**回答部分**进行评价，形成偏好对（优选和劣选）。结合**参考模型**，使用 DPO 方法构建损失函数，直接优化模型参数，提升模型性能。

## DPO详解

### RLHF与DPO

在训练大型语言模型（如聊天模型）时，人类反馈强化学习（RLHF）通常使用近端策略优化（Proximal Policy Optimization, PPO）方法。这种方法可以有效地使模型的行为与人类偏好保持一致。然而，RLHF的过程可能既不稳定又复杂。因此，研究人员正在探索更简单、更稳定的方法来训练这些模型，使其更好地符合人类的偏好，而无需依赖复杂的强化学习过程。

RLHF（Reinforcement Learning from Human Feedback）实际上在整个过程中涉及到四个不同的模型：

1. 参考模型（reference model），通过监督式精调（SFT）在指令数据集上训练得到。

2. 奖励模型（reward model），训练用来预测人类偏好的排名。

3. 价值模型（value model），通常由奖励模型初始化。

4. 我们希望通过RLHF训练的模型（policy），通常由参考模型初始化。

   DPO 直接偏好优化（Direct Preference Optimization）则需要两个模型：

1.参考模型，同样是使用SFT在指令数据集上精调得到的。

2.我们希望通过DPO训练的基础模型（base model）。

[![图片](https://camo.githubusercontent.com/81cdd069eeba8e879e293d7c292e2fe85adcd08088891daff97b26bba991263d/68747470733a2f2f6d6d62697a2e717069632e636e2f6d6d62697a5f706e672f616b47587969633438366e576b6176795855466237596d56363333534e747750514139526f72727a446548354e6961426d3054514332715a756b69626364726a4c4642324d336141573569624c684f586a447769615456454776772f3634303f77785f666d743d706e672666726f6d3d6170706d73672674703d7765627026777866726f6d3d352677785f6c617a793d312677785f636f3d31)](https://camo.githubusercontent.com/81cdd069eeba8e879e293d7c292e2fe85adcd08088891daff97b26bba991263d/68747470733a2f2f6d6d62697a2e717069632e636e2f6d6d62697a5f706e672f616b47587969633438366e576b6176795855466237596d56363333534e747750514139526f72727a446548354e6961426d3054514332715a756b69626364726a4c4642324d336141573569624c684f586a447769615456454776772f3634303f77785f666d743d706e672666726f6d3d6170706d73672674703d7765627026777866726f6d3d352677785f6c617a793d312677785f636f3d31)

在DPO（Direct Preference Optimization）中，参考模型与基础模型的区别主要在于它们在优化过程中的角色和使用方法。

1. **参考模型（Reference Model）：**

   - 这是通过监督式学习（SFT）在一个指令数据集上训练得到的模型，它代表了对特定任务的基本理解和执行能力。
   - 参考模型在DPO中充当了一个基线，用来比较和评估其他模型生成的输出。
   - 在某些情况下，参考模型可以用作后续DPO训练过程的初始状态，尽管这不是必须的。

2. **基础模型（Base Model）：**

   - 这是我们想要通过DPO进行优化的模型，它可能是未经训练的，或者是已经在一些任务上有了初步训练的模型。

   - 基础模型在DPO过程中将直接根据人类的反馈进行训练，学习如何产生更符合人类偏好的输出。

   - 通过DPO的训练，基础模型将逐渐学会模仿那些被人类评价为高质量的输出。

     在DPO中，参考模型主要是用作比较的基准，而基础模型则是实际进行优化和学习的目标。通过这种方式，DPO旨在通过分类问题简化优化过程，使得基础模型能够直接从人类的偏好中学习，而无需依赖于复杂的奖励函数或强化学习算法。

### **RLHF、RLAIF、DPO偏见**

RLHF（Reinforcement Learning with Human Feedback）和RLAIF（Reinforcement Learning with AI Feedback）是两种用于微调大型语言模型（LLM）的方法。它们的主要区别在于反馈的来源：RLHF依赖于人类提供反馈，而RLAIF则使用另一个LLM生成反馈。

RLHF的优点在于，它可以训练AI系统处理诸如内容审查等用例，其中人类对于构成仇恨言论、欺凌和其他不良行为的语言有比AI更好的判断。

RLHF依赖于人类提供反馈，这可能会带来一些挑战：

1. 成本和可扩展性：对于需要领域专家特定知识和技能集的反馈的用例，这个过程可能会变得昂贵和耗时。因此，RLHF可能在大规模应用中面临困难。
2. 反馈的一致性：人类反馈可能会受到个人偏见和主观性的影响，这可能会影响训练的一致性和质量。

RLAIF试图通过使用另一个大型语言模型（LLM）生成反馈来解决这些问题。这种方法的优点是，它可以大大降低反馈获取的成本，并提高反馈的一致性。然而，它也有自己的挑战，比如可能会复制和放大原始模型的偏见和错误。这就是为什么RLAIF和RLHF通常会结合使用，以充分利用两者的优点。

- DPO的偏见主要来自于用于生成反馈的AI模型。如果这个模型在训练数据中存在偏见，那么这种偏见可能会被复制到微调的模型中。
- RLAIF的偏见也主要来自于用于生成反馈的AI模型。如果这个模型在训练数据中存在偏见，那么这种偏见可能会被复制到微调的模型中。
- RHLF的偏见主要来自于人类提供反馈。如果提供反馈的人有某种偏见，那么这种偏见可能会被反馈到模型中。

总的来说，这三种方法都需要谨慎地处理偏见问题。这通常需要在数据收集和模型训练的过程中采取一些措施，如使用多元化的数据源，进行公平性和偏见的审核，以及在可能的情况下，使用透明和可解释的模型。这也是一个活跃的研究领域，研究人员正在寻找更好的方法来理解和减少AI偏见。

### **在DPO的场景中参考模型的作用**

**https://huggingface.co/docs/trl/dpo_trainer**

DPO（Direct Preference Optimization）训练框架中，参考模型的意义如下：

1. **隐式奖励计算**：参考模型用于计算所谓的隐式奖励。在DPO训练中，我们不直接训练一个奖励模型来输出奖励值；相反，我们使用参考模型来估计偏好和被拒绝回答的概率，并基于这些概率来计算隐式奖励。这种隐式奖励是用来指导基础模型（被训练的模型）的学习，使其更倾向于生成被认为是“好”的输出。

2. **损失函数的基础**：隐式奖励差异（即参考模型和基础模型对选定回答和被拒绝回答的概率差异）用作损失函数的基础。这个损失函数在DPO训练中被最大化，目的是提高模型生成偏好回答的概率。

3. **提供稳定性**：参考模型作为训练过程中的固定点，提供了稳定性，帮助避免基础模型在学习过程中偏离过远。它作为一个常数存在，使得基础模型的训练更加稳定和可预测。

4. **模型架构的一致性**：DPO训练要求参考模型和基础模型具有相同的架构。这是因为在计算隐式奖励时，参考模型和基础模型需要在同样的输入上输出可比较的概率值。

5. **简化训练流程**：与传统的强化学习方法相比，DPO通过使用参考模型来简化训练流程。这种方法避免了设计复杂的奖励模型和价值模型，从而降低了训练的复杂性。

6. **参数beta的作用**：在DPO训练中，beta是一个温度参数，用于缩放隐式奖励差异。这个参数控制了优化过程中对参考模型行为的依赖程度。beta值越小，基础模型在优化过程中越自由，对参考模型的依赖越小。

   总的来说，参考模型在DPO训练中提供了一个稳定的比较基准，使得基础模型能够在优化过程中有一个清晰的方向，并且通过隐式奖励的方式简化了优化流程。这使DPO成为一种高效的语言模型优化方法，特别是在处理偏好数据时。

我们来看一段DPO的代码。DPO的参考模型是之前使用监督式精调（SFT）在特定数据集上训练过的Mistral 7B模型。在这个上下文中，参考模型被用来初始化一个适配器（adapter），这个适配器随后被用于DPO训练。

```
model = PeftModel.from_pretrained(model, "kaitchup/Mistral-7B-v0.1-SFT-ultrachat-v2", is_trainable=True, adapter_name="DPO")  
model.load_adapter("kaitchup/Mistral-7B-v0.1-SFT-ultrachat-v2", adapter_name="reference")
```

这里，`model` 是基础模型，即要通过DPO进行训练的模型。`load_adapter` 方法用来加载一个名为 "reference" 的适配器，这个适配器是在SFT过程中使用Mistral 7B训练得到的，并且其训练是基于 "ultrachat" 数据集的。在DPO训练中，这个 "reference" 适配器用作比较的基准，以帮助DPO评估生成的输出是否符合人类的偏好。

在DPO中，参考模型的作用是提供一个与被训练模型的输出相比较的标准。在训练期间，系统会计算参考模型输出和被训练模型输出的概率对数差异，并将其乘以一个系数（beta），这个差异用于指导DPO的训练过程，使得被训练模型能够生成更优选的响应。

实际上，在选择DPO（Direct Preference Optimization）的参考模型时，通常会选择与基础模型（在这种情况下是Mistral 7B）结构和参数设置相同或相似的模型。这样做的理由包括：

1. **相似性**：相同或相似的模型结构保证了在比较基础模型和参考模型的输出时，差异主要来自于模型权重的不同，而非模型架构的不同。
2. **一致性**：使用同一模型架构可以确保生成的输出有可比性，这对于训练过程中评估和提升模型性能至关重要。
3. **简化训练**：如果参考模型与基础模型在架构上一致，可以简化训练流程，因为可以共享部分模型组件，这有助于减少内存消耗和计算需求。
4. **适配器技术**：在某些实现中，如上文中的例子，使用了适配器技术来对模型进行微调。这种情况下，参考模型和基础模型之间的适配器可以共享，减少了资源的需求。

## DPO微调代码

定义要被微调的模型

```
model_name = "mistralai/Mistral-7B-v0.1"
#Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'left' #Left is necessary for FlashAttention

#Better to use bf16 if supported (Ampere GPUs or more recent)
#If bf16 is supported, the GPU is also recent enough to support FlashAttention
if torch.cuda.is_bf16_supported():
  compute_dtype = torch.bfloat16
  attn_implementation = 'flash_attention_2'
else:
  compute_dtype = torch.float16
  attn_implementation = 'sdpa'
```

加载数据集：

```
dataset = load_dataset("UltraFeedback-prompt-chosen-rejected")
```

查看数据集的第一条，在数据集中，`chosen` 和 `rejected` 标签可以用于训练模型理解什么是好的回复，什么是不好的回复，从而优化模型的输出质量。

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/Comparison-of-Various-Fine-Tuning-Methods/images/1.png)

```
bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=True,
)
model = AutoModelForCausalLM.from_pretrained(
          model_name, quantization_config=bnb_config, device_map={"": 0}, torch_dtype=compute_dtype, attn_implementation=attn_implementation
)
model = prepare_model_for_kbit_training(model, gradient_checkpointing_kwargs={'use_reentrant':True})
#Configure the pad token in the model
model.config.pad_token_id = tokenizer.pad_token_id
```

下面这段代码的主要作用是：

1. **加载一个预训练的模型，并为它添加两个“小插件”（我们称之为“适配器”）：**
   - **第一个适配器**叫做 **"DPO"**，这个适配器是**可以训练的**，即在训练过程中会被更新和改进。
   - **第二个适配器**叫做 **"reference"**，这个适配器是**固定的，不会被训练**，用于作为对照参考。

```
model = PeftModel.from_pretrained(model, "kaitchup/Mistral-7B-v0.1-SFT-ultrachat-v2", is_trainable=True, adapter_name="DPO")
model.load_adapter("kaitchup/Mistral-7B-v0.1-SFT-ultrachat-v2", adapter_name="reference")
```

接下来，设置DPO训练的超参。`DPOConfig` 是一个特定名字的类，用于配置 **Direct Preference Optimization（DPO）** 训练过程的参数。

```
training_arguments = DPOConfig(
        output_dir="./results",
        evaluation_strategy="steps",
        do_eval=True,
        optim="paged_adamw_8bit",
        per_device_train_batch_size=8,
        gradient_accumulation_steps=8,
        per_device_eval_batch_size=8,
        log_level="debug",
        save_steps=20,
        logging_steps=20,
        learning_rate=5e-7,
        eval_steps=20,
        max_steps=100,
        warmup_steps=20,
        lr_scheduler_type="linear",
        report_to='none'  
)
```

```
trainer = DPOTrainer(
    model,
    args=training_arguments,
    beta=0.1,
    model_adapter_name="DPO",
    ref_adapter_name="reference",
    train_dataset=dataset['train'],
    eval_dataset=dataset['test'],
    tokenizer=tokenizer,
)

trainer.train()
```

查看训练结果：

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/Comparison-of-Various-Fine-Tuning-Methods/images/2.png)

## 对DPO训练结果的解释

在使用 DPO（Direct Preference Optimization，直接偏好优化）方法进行模型训练时，我们涉及两个模型：


训练模型（Policy Model）：这是我们希望优化的模型，它的参数会在训练过程中更新。

参考模型（Reference Model）：这是一个固定的模型，其参数在训练过程中保持不变，用于提供参考和正则化。

训练目标：希望训练模型在给定 Prompt（输入）时，更倾向于生成 Chosen（偏好较高的回复），而避免生成 Rejected（偏好较低的回复）。

训练过程中的关键步骤和指标

#### 训练数据

每个训练样本包含：

- Prompt（输入）：模型需要回答的问题或指令。
- Chosen（理想回复）：我们希望模型生成的正确答案。
- Rejected（不理想回复）：我们希望模型避免生成的答案。

#### 模型评价

对于每个样本，我们需要计算以下内容：

- 训练模型 对 Prompt + Chosen 的评价。
- 训练模型 对 Prompt + Rejected 的评价。
- 参考模型 对 Prompt + Chosen 和 Prompt + Rejected 的评价（用于计算正则化项）。

#### 奖励值的计算

奖励函数（Reward Function）：用于评估训练模型对 Chosen 和 Rejected 的输出质量，给予数值评分（奖励值）。

- 奖励值的计算：
  - Rewards/chosen：训练模型对 Chosen 回复的奖励。
  - Rewards/rejected：训练模型对 Rejected 回复的奖励。

#### 损失函数的计算

损失函数的主要组成部分：

- 偏好损失：鼓励模型对 Chosen 回复给予更高的奖励，对 Rejected 回复给予更低的奖励。

- 正则化项：利用参考模型，限制训练模型不要偏离原始语言模型的分布过远。

  损失函数公式（简化版，用普通文本表示）：

```
损失 = -（训练模型对 Chosen 的奖励 - 训练模型对 Rejected 的奖励） + β * （训练模型与参考模型的差异）  
```

其中，β 是超参数，控制正则化项的权重。



### 通过示例解释训练过程和指标

 #### 示例训练数据


Prompt：

```
翻译以下英文句子为中文： "The quick brown fox jumps over the lazy dog."  
```


Chosen（理想回复）：

```
"敏捷的棕色狐狸跳过了懒狗。"  
```


Rejected（不理想回复）：

```
"我不知道如何翻译这个句子。"  
```

 **步骤 1：模型评价**

（a）训练模型对 Chosen 和 Rejected 的评价
计算对数概率（Logps）：

- Logps/chosen：训练模型对 Chosen 回复的对数概率之和。例如，假设计算得到 -50。

- Logps/rejected：训练模型对 Rejected 回复的对数概率之和。例如，计算得到 -70。

  （b）参考模型的输出
  参考模型 也会对 Chosen 和 Rejected 进行评价，计算对数概率（用于正则化项）。但这些值不直接出现在训练结果中。

**步骤 2：奖励值的计算**

（a）计算奖励值

- Rewards/chosen：

  ```
  Rewards/chosen = 训练模型对 Chosen 的对数概率 - β * （训练模型与参考模型在 Chosen 上的差异）  
  ```

  例如，假设训练模型和参考模型在 Chosen 上的差异为 5，β 为 0.1，则：

  ```
  Rewards/chosen = -50 - 0.1 * 5 = -50.5  
  ```

 

- Rewards/rejected：

  ```
  Rewards/rejected = 训练模型对 Rejected 的对数概率 - β * （训练模型与参考模型在 Rejected 上的差异）  
  ```

  例如，差异为 3，则：

  ```
  Rewards/rejected = -70 - 0.1 * 3 = -70.3  
  ```


（b）计算奖励差距（Rewards/margins）

```
Rewards/margins = Rewards/chosen - Rewards/rejected  
```

代入数值：

```
Rewards/margins = (-50.5) - (-70.3) = 19.8  
```

 

**步骤 3：计算损失**

```
损失 = -（Rewards/chosen - Rewards/rejected） = -（-50.5 - (-70.3)） = -19.8  
```

模型会根据这个损失值进行参数更新，目的是最小化损失，即最大化奖励差距。



**步骤 4：计算其他指标**

- Rewards/accuracies（奖励准确率）
  如果 Rewards/chosen > Rewards/rejected，则计为一次正确判断。在我们的例子中，-50.5 > -70.3，所以这是一次正确判断。
- Logits/chosen 和 Logits/rejected
  Logits 是在计算对数概率（Logps）之前的原始输出值。这些值反映了模型对每个词的未归一化的信心程度。



### 参考模型的作用

 

- 正则化：参考模型帮助计算正则化项，限制训练模型不要偏离原始语言模型的分布过远。
- 计算差异：通过比较训练模型和参考模型在 Chosen 和 Rejected 上的输出差异，调整奖励值。
- 隐式影响：虽然参考模型的输出不直接出现在训练结果中，但它在损失函数的计算中起到了关键作用，进而影响了训练模型的更新。

### 训练结果中的各指标


现在，我们将总结训练结果中的各个指标，以及它们是如何产生的：

- Rewards/chosen：由训练模型对 Chosen 回复的评价计算得到，考虑了参考模型的影响（通过正则化项）。
- Rewards/rejected：由训练模型对 Rejected 回复的评价计算得到，同样考虑了参考模型的影响。
- Rewards/accuracies：模型正确区分 Chosen 和 Rejected 的比例。
- Rewards/margins：Rewards/chosen 与 Rewards/rejected 之间的差值，反映了模型对偏好的区分能力。
- Logps/chosen 和 Logps/rejected：训练模型在 Chosen 和 Rejected 上的对数概率之和。
- Logits/chosen 和 Logits/rejected：训练模型在生成 Chosen 和 Rejected 时的未归一化输出值。

### 总结

- 参考模型的作用：虽然参考模型的输出不直接显示在训练结果的指标中，但它通过损失函数中的正则化项，间接地影响了训练模型的更新。
- 指标的产生：训练结果中的各项指标主要反映了训练模型的性能，通过对 Chosen 和 Rejected 的评价计算得到。
- 训练目标：通过最小化损失函数，使训练模型更倾向于生成 Chosen 回复，而避免生成 Rejected 回复，同时保持模型的语言能力不被破坏。

## DPO微调中的正则化与泛化

### 正则化


简单来说，正则化是一种防止模型过度拟合的方法。


过度拟合（Overfitting）是指模型在训练数据上表现很好，但在新数据（未见过的数据）上表现很差。这是因为模型“记住”了训练数据的细节和噪声，而不是学会了数据背后的一般规律。


正则化（Regularization）是为了解决过度拟合问题的一种技术。**它的作用是：**在训练过程中，对模型的某些特性进行约束，防止模型过于复杂，从而使模型在新数据上也能有良好的表现。


比喻：想象你在学习如何识别苹果和橘子。过度拟合的模型可能会记住训练集中每个苹果和橘子的具体特征，比如这个苹果有一个小斑点，那个橘子有一片叶子。正则化的模型则会关注苹果和橘子的一般特征，比如形状、颜色，而不会过分关注训练数据中特定的细节。

#### 在模型训练中的应用

- **限制模型复杂度**：正则化通过限制模型的参数，使模型不会变得过于复杂。例如，在神经网络中，可以对权重施加限制，不让它们变得过大。
- **防止参数过大**：如果模型的参数（如权重）变得过大，模型可能会过度关注训练数据的细节。正则化通过在损失函数中添加一个惩罚项，鼓励模型的参数保持较小的值。

在上面的训练中

- **参考模型的作用**：在 DPO（直接偏好优化）训练中，参考模型（Reference Model）用于正则化。

- 为什么需要正则化：

  - 平衡：我们希望模型既能学会用户的偏好，又不要失去原有的语言生成能力。
  - 防止过度偏离：如果只关注让模型更倾向于生成“正确”的回复，模型可能会过度调整，导致语言质量下降。

- 正则化的方法

  ：

  - 利用参考模型：通过在损失函数中加入一个项，测量训练模型与参考模型之间的差异（如 KL 散度）。
  - **作用**：这项差异作为正则化，防止模型过度偏离参考模型的行为，保持语言流畅性和多样性。

### 泛化


我们实际上已经在训练数据中提供了“应该怎么回答”和“不应该怎么回答”的示例，分别是 Chosen 和 Rejected 回复。但仅仅提供正确答案并让模型模仿是不够的，我们希望模型能够理解为什么某个回答是好的，某个回答是差的，从而在新的情况下做出正确的判断。

1. **模仿学习的局限性**

   - **直接模仿可能导致过度拟合**：
     如果模型只是机械地记住训练数据中的正确回答，它可能无法对新问题或稍有变化的输入做出良好的响应。
   - **缺乏区分能力**：
     模型可能不知道为什么一个回答是好的，另一个是差的，所以在面对新情况时，可能无法正确区分。

2. **引入不良回复的原因**

   - **明确模型的偏好**：
     通过提供 Rejected（不良回复），我们告诉模型哪些回答是不好的。
   - **对比学习**：
     模型通过对比 Chosen 和 Rejected，学会区分好的回复和坏的回复。这种方法能够让模型理解什么样的特征是好的，什么样的特征是坏的。

3. **提高模型的泛化能力**

   - **在新情况下做出正确判断**：
     我们希望模型不仅能记住正确的回答，还能在面对各种新问题时，基于学到的偏好，生成优质的回复。
   - **理解背后的原则**：
     通过对比好的和坏的回复，模型可以学到更深层次的模式和规律，而不是仅仅记住答案。

4. **防止偏差和不良行为**

   - **处理安全和道德问题**：
     通过明确标注不应该生成的回复，模型可以避免生成有害、偏见或不恰当的内容。
   - **强化正确的行为**：
     让模型学会哪些内容是应该避免的，哪些是应该鼓励的，提高模型的可靠性。

5. **举个例子**

   - **直接提供正确回答的风险**：
     如果我们只给模型一个问题和正确答案，例如：

     ```
     问题：请解释牛顿的第一定律。  
     回答：物体在没有受到外力作用时，会保持静止或匀速直线运动状态。  
     ```

     模型可能只能在遇到完全相同的问题时给出正确的回答。

   - **加入不良回复进行对比**：
     我们再提供一个不良回复：

     ```
     不良回答：我不知道牛顿的第一定律是什么。  
     ```

     通过对比，模型可以学到：

     - 好的回答应该是对问题的准确、完整的解释。
     - 不好的回答是无法回答问题或者提供错误信息。

     这样，模型在面对类似但不同的问题时，也能给出正确的回答：
     例如，遇到“请解释牛顿的第二定律”时，模型能够推断出应该提供该定律的解释，而不是说“我不知道”。

#### 总结

- **正则化**：
  是一种防止模型过度拟合的方法，确保模型在新数据上也有良好的表现。在您的训练中，参考模型用于正则化，防止训练模型过度偏离原有的语言模型分布，保持语言的质量和一致性。
- **训练方法的设计**：
  通过提供 Chosen 和 Rejected 回复，让模型学会区分好的和坏的回答。这种对比学习的方法让模型能够理解回答质量的差异，从而在新情况下也能生成优质的回复。直接让模型模仿正确答案，可能导致模型缺乏泛化能力，无法在新情境下做出正确的判断。
