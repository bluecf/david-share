# Comparison of Various Fine-Tuning Methods

本文将会对如下微调技术进行对比：SFT、ReFT、RHLF、RLAIF、DPO、PPO、TPO。



## 几种技术之间的关系

如果把复杂的问题简单理解，这些技术之间的关系大概是：

**ReFT（Reinforced Fine-Tuning，强化微调）：**

- **组成**：ReFT = SFT + PPO
- **过程**：在有监督微调（SFT）的基础上，使用 PPO（近端策略优化）进行强化学习。
- **评估方式**：通常通过**自动化程序**对模型输出进行评估，奖励信号来自程序的评价。

**RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）：**

- **组成**：RLHF = SFT + PPO + 人类反馈
- **过程**：在 SFT 的基础上，使用 PPO 进行强化学习，奖励信号来自**人类反馈**。
- **评估方式**：人类对模型输出进行评价，或者使用基于人类反馈训练的**奖励模型**来评估。

##### DPO 方法（Direct Preference Optimization，直接偏好优化）：

- **组成**：DPO 方法 = SFT + **参考模型** + DPO
- **过程**：在 SFT 的基础上，**引入参考模型**（通常是经过 SFT 的初始模型，参数固定不更新），使用 DPO（直接偏好优化）方法，利用参考模型和人类偏好数据，直接优化模型参数。
- **评估方式**：利用**人类偏好数据和参考模型**，构建损失函数，直接优化模型参数，使模型更倾向于生成被人类偏好的输出。


**RLAIF（Reinforcement Learning from AI Feedback，基于 AI 反馈的强化学习）：**

- **组成**：RLAIF = SFT + PPO + AI 反馈
- **过程**：在 SFT 的基础上，使用 PPO 进行强化学习，奖励信号来自**AI 模型的反馈**。
- **评估方式**：辅助的 AI 模型（可能是奖励模型）对模型输出进行评价，提供奖励信号。


**TPO（Thought Preference Optimization，思维偏好优化）：**

- **组成**：TPO = SFT + 思维生成（Thought Generation）+ DPO
- **过程**：在 SFT 的基础上，引入**思维生成**，即模型在输出回答前生成内部的思维过程。然后，使用 DPO 方法直接优化模型，偏好数据来自**AI 判别模型的反馈**。
- **评估方式**：利用**AI 判别模型**对模型输出的**回答部分**进行评价，形成偏好对（优选和劣选）。根据这些偏好对，使用 DPO 方法优化模型参数，提升模型性能。



**解释：**

- ReFT（强化微调）通过在监督微调后的模型上，使用PPO算法进行强化学习，奖励信号来自于自动化程序对模型输出与标准答案的比较。

- RLHF（基于人类反馈的强化学习）在SFT基础上，使用PPO算法进行强化学习，奖励信号来自人类对模型输出的评价。

- DPO方法（直接偏好优化）在SFT基础上，使用DPO算法直接优化模型参数以符合人类偏好，不使用PPO等传统强化学习算法。

- RLAIF（基于AI反馈的强化学习）类似于RLHF，但人类反馈替换为AI模型的反馈，使用PPO算法进行强化学习。




ReFT、RLHF、DPO和RLAIF。这些方法都是在监督微调（SFT）的基础上，进一步优化模型以提高性能，但它们在优化策略和反馈来源上有所不同。

1. **ReFT（Reinforced Fine-Tuning，强化微调）**：这是SFT和PPO（近端策略优化）的结合。在第一阶段，模型通过SFT在有标注的数据上进行训练，建立基本的语言理解和生成能力。第二阶段，引入PPO算法，对模型进行强化学习优化。此时，模型的输出由自动化程序进行评估，程序根据预设的规则或标准对模型的输出进行评价，并生成奖励信号。模型根据这些奖励信号，使用PPO算法调整自身参数，以产生更优的输出。ReFT的特点是评估过程自动化，无需人类参与，适用于有明确客观标准的任务，例如数学问题求解。

2. **RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）**：在SFT的基础上，结合PPO算法，但奖励信号来自人类反馈。具体而言，人类对模型的输出进行评价，指出更优的回答，或通过偏好对比的方式提供反馈。这些人类反馈可以直接用于指导模型优化，或者用于训练一个奖励模型，后续由奖励模型对模型输出进行评估。RLHF的优势在于引入了人类的主观判断，使模型的输出更符合人类偏好，适用于需要复杂评价和主观判断的任务。

3. **DPO（Direct Preference Optimization）方法：** 与前两种方法不同，DPO 不使用强化学习算法（如 PPO），而是采用监督学习的方法，直接优化模型。在 SFT（有监督微调）之后，**引入一个参考模型**（通常为经过 SFT 的初始模型，参数固定不更新），利用人类偏好数据和参考模型，构建损失函数，对模型进行微调。具体来说：

   - **收集人类偏好数据**：收集人类对模型输出的偏好数据，如在给定的多个回答中标注出人类更喜欢的那个，形成偏好对（首选和非首选响应）。

   - **参考模型的引入**：参考模型提供了一个稳定的概率基准，用于与当前模型的输出概率进行比较，防止模型在优化过程中过度偏离预训练的语言分布。

   - **构建损失函数**：设计一个损失函数，利用当前模型和参考模型对首选和非首选响应的对数概率，鼓励模型倾向于生成被人类偏好的输出。损失函数通常包含对数概率差和正则项，以确保训练的稳定性。

   - **直接优化模型参数**：通过最小化该损失函数，直接调整模型参数，使其更倾向于生成被人类偏好的输出。

     DPO 避免了强化学习中的试错过程，训练更稳定，效率更高，适用于有大量人类偏好数据的场景。**同时，参考模型的引入有助于保持模型生成质量的稳定性，防止模型偏离预训练分布过远。**

4. **RLAIF（Reinforcement Learning from AI Feedback，基于AI反馈的强化学习）**：这是SFT、PPO和AI反馈的结合。在SFT后，使用PPO进行强化学习，然而奖励信号不是来自人类，而是来自辅助的AI模型（如奖励模型）的反馈。AI模型对主模型的输出进行评估，提供奖励信号。这样的方法节省了人类评价的成本，但依赖于辅助AI模型的质量。

**总结：**四种方法中，ReFT、RLHF和RLAIF都使用了PPO作为强化学习算法，区别在于奖励信号的来源不同：ReFT来自自动化程序的评估，RLHF来自人类反馈，RLAIF来自AI模型的反馈。只有DPO方法使用了监督学习的方式，不采用PPO等强化学习算法，而是直接利用人类偏好数据优化模型。



**那么，DPO的意义是什么？**

强化学习方法（如PPO）需要模型在环境中自行探索，通过试错学习获得奖励信号，这个过程复杂，训练不稳定，且调参困难。相比之下，监督学习的方法更直接高效：通过人类提供的偏好数据，直接告诉模型什么是好的输出，构建损失函数，调整模型参数。这样避免了强化学习的复杂性，训练过程更稳定，效率更高，特别适用于有大量人类偏好数据的情况。

举个例子，使用强化学习的模型就像是在黑暗中摸索前进，需要不断试错；而使用监督学习的DPO方法，就像是有人直接给了一张地图，告诉你正确的前进路线。采用监督学习，可以更快地达到目标。

选择方法的依据：如果任务有明确的客观评价标准，适合使用ReFT，通过自动化程序评估模型输出。如果希望模型的输出更符合人类主观偏好，并有大量人类反馈数据，可以选择RLHF或DPO方法。RLHF使用强化学习算法，需要模型与环境交互，训练复杂；DPO则采用监督学习，训练更简单高效。若人类反馈成本高，可以考虑RLAIF，用辅助AI模型提供反馈信号。



## **ReFT简介**

先看ReFT论文中的流程图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/akGXyic486nUpgIIJic3v3UHiaRriaapjX2omaoJWqB4dr3dQyGEkDMjjR5JeI8LibRVX9icuCAiarOA0kMgPfWhoqiamg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如上图所示，ReFT，该框架结合了监督微调（Supervised Fine-Tuning, SFT）和强化微调（Reinforced Fine-Tuning, ReFT）的方法。以下是对图中各部分的详细解释：

1. **监督微调（Supervised Fine-Tuning）**：

   - **模型（Model）**：初始模型通过多个SFT周期（epochs）在训练数据上进行训练。训练数据包含问题（x），推理链（CoT，e）和答案（y）。
   - **SFT Epochs**：模型在训练数据上进行多个周期的训练，以学习如何从问题（x）和推理链（e）生成正确的答案（y）。
   - **不同阶段的模型**：图中展示了经过不同训练阶段后的模型表情变化，表示模型逐渐变得更好。

2. **强化微调（Reinforced Fine-Tuning）**：

   - **预热阶段（Warm-up）**：在进入强化学习之前，模型通过SFT进行预热。
   - **问题（question）**：模型接受一个输入问题（x）。
   - **On-Policy Sampling**：在策略内采样，模型生成一个推理链和答案（e', y'）。
   - **Golden Reward**：对生成的答案（y'）与正确答案（y）进行比较，给予奖励信号。如果答案正确，给予正奖励（√），否则给予负奖励（×）。
   - **强化学习（Reinforcement Learning）**：利用奖励信号来调整模型参数，以提高模型在相同数据上的表现。

3. **最终策略（Final Policy）**：

   - 经过SFT和ReFT训练后，模型形成最终策略，可以更准确地回答问题。

     图例说明了在GSM8K数据集上，一个问题（x）、推理链（e）和答案（y）的示例。通过多个SFT周期对训练数据进行迭代，并使用ReFT方法从SFT进行预热，然后在相同数据上进行强化学习训练。

## **TPO的流程**

先看Thought Preference Optimization（TPO）的流程：

![图片](https://mmbiz.qpic.cn/mmbiz_png/akGXyic486nUpgIIJic3v3UHiaRriaapjX2obrFTylPby9mlhw3NUiaJicSIAianv6WrYjTbaCe24w2nic8xrHLOUo4VLg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- TPO 方法由三个主要部分组成：
  1. SFT（有监督微调）：提供了模型的基础。
  2. 思维生成（Thought Generation）：让模型在回答前进行内部思考。
  3. DPO（直接偏好优化）：利用 AI 判别模型生成的偏好对，直接优化模型参数。
- TPO 的创新之处在于：
  - 引入了思维生成，增强模型的推理和规划能力。
  - 利用 AI 判别模型的反馈，降低了对人类偏好数据的依赖。
  - 使用 DPO 方法，简化了训练流程，提高了训练效率。



**TPO = SFT + 思维生成（Thought Generation）+ DPO（偏好优化）**

需要指出的是，**DPO 方法（Direct Preference Optimization，直接偏好优化）** 通常直接使用人类的偏好数据，并且 **引入了参考模型**。在传统的应用中，DPO 方法依赖于人类对模型输出的偏好反馈，并通过 **参考模型** 来构建损失函数，直接优化模型参数。然而，**TPO 方法（Thought Preference Optimization，思维偏好优化）** 将 DPO 的应用扩展到了使用 **AI 模型生成的偏好数据**，这一创新使得模型能够在缺乏人类偏好数据的情况下，仍然利用偏好优化方法进行训练。需要强调的是，对于 DPO 方法而言，**参考模型和偏好数据的结合是关键**。无论偏好数据的来源是人类还是 AI 模型，DPO 方法都通过 **参考模型** 和偏好数据来构建损失函数，直接优化模型，而 **不涉及强化学习算法**。 
**上面 TPO 方法的具体解释如下：**

**1. SFT（Supervised Fine-Tuning，有监督微调）：**

- **起点模型（Seed Model）**：TPO 方法以一个已经过有监督微调的预训练模型作为基础（即经过 SFT 的模型），例如 Llama-3-8B-Instruct。

  **2. 思维生成（Thought Generation）：**

- **引入思维过程**：在生成最终回答之前，模型通过特定的提示（Prompt），被引导生成内部的思维过程（Thought）。

- **思维与回答的分离**：生成的输出分为 **思维部分** 和 **回答部分**，思维部分在最终呈现给用户时被隐藏。

  **3. DPO（Direct Preference Optimization，直接偏好优化）：**

- **参考模型的引入**：使用经过 SFT 的模型作为 **参考模型**，参数固定不更新。

- **偏好数据来源**：偏好对（Preference Pairs）来自于 **AI 判别模型** 对模型输出的评估，而非传统的人类偏好数据。

- **AI 判别模型**：

  - **Self-Taught Evaluator（STE）**：基于大型语言模型（如 Llama-3-70B-Instruct），用于对两个回答进行比较，输出偏好结果。

  - **ArmoRM**：一个奖励模型，直接对单个回答进行评分。

    **优化过程：**

- **生成候选输出**：模型针对每个输入指令，生成多个包含 **思维和回答** 的候选输出。

- **评估回答部分**：将 **回答部分** 输入到 **AI 判别模型**，进行评分或偏好比较。

- **构建偏好对**：根据评估结果，选出 **最佳和最差回答**，形成偏好对（Chosen 和 Rejected）。

- **使用 DPO 优化**：利用这些偏好对，结合 **参考模型**，使用 DPO 方法构建损失函数，直接优化模型参数。

## **PPO和DPO的本质区别**

### 1. 强化学习的本质


强化学习就像训练一只小狗做动作。

**比方说：**

- **训练小狗**：当小狗做对了动作（比如坐下、握手），我们就给它一块小零食作为奖励；当它做错了，我们就不给奖励。

- **目的**：经过多次训练，小狗会明白，做对了就有奖励，于是它会更愿意做我们希望的动作。

  **在机器学习中：**

- **模型与环境互动**：模型（相当于小狗）在某个环境中，根据当前的情况，选择一个动作。

- **获得奖励或惩罚**：环境会根据模型的动作，给出一个奖励（奖励值可以是正的，也可以是负的）。

- **优化目标**：模型的目标是学习到一种策略，使得它在与环境的长期互动中，累积的奖励最大化。

  **简单理解：**

- **强化学习 = 试错学习 + 奖励机制**

- 模型通过不断尝试不同的动作，学习到哪些行为能够获得更多的奖励。

  **举个例子：**

- 游戏中的机器人：想象一个机器人在迷宫中寻找出口。

  - **到达出口**：奖励 +10 分。
  - **撞墙**：奖励 -1 分。
  - **其他情况**：奖励 0 分。

- 行动选择：

  - 每走一步，机器人会根据当前的位置（状态），选择向上、下、左、右移动（动作）。

- 模型目标：

  - 学会一条最优路径，快速找到出口，获得最高的累计奖励。

### 2. 偏好数据的直接优化（DPO）的本质

**直接偏好优化（DPO）** 是一种直接利用偏好数据和参考模型来优化模型的方法。

**比方说：**

- **模型生成两个回答**：对于同一个问题，当前模型（Policy Model）生成了回答 A 和回答 B。

- **获得偏好反馈**：我们请人类评估者或辅助的 AI 模型告诉我们，哪个回答更好。

  - 例如，人类评估者说：“回答 A 比回答 B 好。”

- **引入参考模型**：

  - **参考模型（Reference Model）**：通常是经过监督微调（SFT）的初始模型，其参数在 DPO 训练过程中保持固定不变。
  - **作用**：提供一个稳定的概率基准，防止当前模型在优化过程中偏离预训练的语言分布。

- **构建损失函数**：

  - **利用当前模型和参考模型**，对两个回答的对数概率进行计算。
  - **损失函数**：设计一个损失函数，使当前模型更倾向于生成被人类偏好的回答，同时限制模型的更新幅度，保持训练稳定性。

- **直接优化模型**：

  - 通过最小化损失函数，直接调整当前模型的参数，使其更倾向于生成被偏好的回答 A。

    **与强化学习的区别：**

- **不需要复杂的奖励函数**：DPO 不需要设计一个数值化的奖励函数或与环境交互的机制，只需利用偏好比较的信息和参考模型的概率基准。

- **不需要环境交互**：DPO 直接使用已有的偏好数据和参考模型进行优化，无需模型与环境持续交互，也不涉及即时奖励信号。

  **简单理解：**

- **DPO = 利用偏好比较和参考模型直接调整模型参数**

- 模型根据哪些输出被偏好，结合参考模型的概率信息，直接优化自身，倾向于生成更符合人类偏好的输出。

  **举个例子：**

- **训练聊天机器人**：

  1. **模型生成两个回答**：针对用户提出的问题，当前模型生成了两个候选回答。

     - **回答 1**：“好的，我会帮您查询相关信息。”
     - **回答 2**：“我不知道，你自己去查吧。”

  2. **获取偏好数据**：人类评估者比较两个回答，选择更好的一个。

     - 人类评估者表示更喜欢 **回答 1**，因为它更有礼貌、服务态度更好。

  3. **引入参考模型**：

     - 使用经过 SFT 的初始模型作为参考模型，其参数固定。

  4. **构建损失函数**：

     - 计算 **当前模型** 和 **参考模型** 对 **回答 1** 和 **回答 2** 的对数概率。
     - 设计损失函数，鼓励当前模型提高对被偏好回答（回答 1）的概率，降低对不被偏好回答（回答 2）的概率，同时参考模型的信息用于稳定训练。

  5. **直接优化模型**：

     - 通过最小化损失函数，更新当前模型的参数，使其更倾向于生成类似 **回答 1** 的内容。

       **总结：**

- **DPO 方法** 通过结合 **人类偏好数据** 和 **参考模型**，在监督学习的框架下直接优化模型参数。

- **参考模型** 的作用是提供一个稳定的基准，防止模型过度偏离预训练的语言分布，确保训练过程的稳定性和生成质量。

- **与强化学习的区别**：

  - DPO 不涉及与环境的交互和即时奖励信号，避免了强化学习中的复杂性和不稳定性。
  - DPO 的优化过程更直接、更稳定，适合在有大量偏好数据的场景下使用。

### **3. 关于 DPO 中的“交互”**

 
在 DPO 的描述中，模型需要生成多个回答，然后人类评估者或辅助的 AI 判别模型提供偏好反馈（**注：在 TPO 的 DPO 部分，偏好数据可能来自 AI 模型**）。这似乎也涉及到一些交互。那么，DPO 不是也有交互吗？

**的确，DPO 涉及到模型与人类评估者或 AI 判别模型之间的“交互”，因为需要收集偏好数据。然而，这种交互与强化学习中的交互有本质的不同。**

#### **1. DPO 中的“交互”**

- 数据收集阶段的交互：
  - 在 DPO 中，模型会为每个输入生成多个候选输出（如回答）。
  - **人类评估者或 AI 判别模型**对这些候选输出进行偏好标注，形成偏好对（首选和非首选响应）。
  - **特点**：这种交互是一次性的、离线的，用于构建训练数据集。数据收集完成后，进入模型训练阶段。
- 训练阶段的优化：
  - 一旦偏好数据收集完成，模型的训练过程就是基于这些偏好对，**结合参考模型**，使用监督学习的方法直接优化模型参数。
  - **训练过程中不再需要与人类评估者或环境进行交互**，参考模型的参数在训练中保持固定。

#### **2. 强化学习中的交互**



- 持续的环境交互：
  - 在强化学习（如 PPO）中，模型在训练过程中需要与环境进行持续、动态的交互。
  - 模型根据当前状态选择动作，环境反馈新的状态和奖励信号，模型根据这些信息更新策略。
- 训练过程的复杂性：
  - 强化学习的训练是在线的、迭代的，模型需要不断地试错来学习最优策略。
  - 涉及到 **状态、动作、奖励、价值函数、策略更新** 等复杂概念。

#### **3. 两者的区别**

- 交互的性质和阶段不同：

  - **DPO** 的交互发生在训练前的数据收集阶段，是 **静态的、离线的**。在训练过程中，模型只需利用已获得的偏好数据和参考模型进行优化。
  - **强化学习** 的交互发生在训练过程中，是 **动态的、在线的**。模型需要与环境持续交互，获取即时奖励信号。

- 训练方法不同：

  - **DPO** 使用监督学习方法，结合 **参考模型**，利用偏好对构建损失函数，直接优化模型参数。
  - **强化学习** 需要策略梯度或价值函数等方法，根据环境反馈的奖励信号更新策略，通常采用强化学习算法（如 PPO）。

- 复杂性和稳定性

  ：

  - **DPO** 的训练过程相对简单、稳定，因为它是基于监督学习，参考模型的引入进一步稳定了训练过程。
  - **强化学习** 的训练过程更复杂，可能存在不稳定性，需要精心调整超参数和训练策略。

#### **4. 举个例子**

 
**DPO 的例子：**

- **步骤 1：数据收集**

  - 模型生成两个回答

    ：

    - **回答 1**：“好的，我会帮您查询相关信息。”
    - **回答 2**：“我不知道，你自己去查吧。”

  - **人类评估者或 AI 判别模型**对这两个回答进行比较，偏好 **回答 1**。

- **步骤 2：模型优化**

  - **引入参考模型**：使用经过 SFT 的模型作为 **参考模型**，参数固定不更新。
  - 构建损失函数：
    - 利用 **当前模型** 和 **参考模型** 对 **回答 1** 和 **回答 2** 的对数概率进行计算。
    - 根据偏好数据，设计损失函数，鼓励当前模型提高对被偏好回答（回答 1）的概率，降低对不被偏好回答（回答 2）的概率。
  - **优化模型**：通过最小化损失函数，直接调整当前模型的参数，使其更倾向于生成被偏好的输出。

- **特点**：

  - **数据收集和模型训练是分开的**。

  - **训练过程中不需要与人类评估者或环境交互**，参考模型在训练中提供稳定的基准。

    **强化学习的例子：**

- **模型与环境持续交互**：

  - 模型在训练过程中，不断与环境交互，选择动作，观察反馈，获取即时奖励。
  - 例如，**训练一个游戏 AI**，模型需要在游戏环境中不断尝试不同的行动，根据得分（奖励）更新策略。

- **特点**：

  - **训练过程需要持续的、在线的交互**。
  - **模型的行为会影响后续的状态和奖励**，训练过程更加复杂，需要处理环境的不确定性。

#### **5. 总结**

- **直接偏好优化（DPO）**：
  - **交互发生在数据收集阶段**，是离线的、静态的。偏好数据收集完成后，训练过程不再需要交互。
  - **训练过程中使用监督学习方法**，结合参考模型，直接利用偏好数据优化模型。
  - **训练过程简单、稳定**，不涉及环境交互，参考模型的引入确保了训练的稳定性。
- **强化学习**：
  - **交互发生在训练过程中**，是在线的、动态的。模型需要与环境持续交互，获取即时奖励信号。
  - **模型通过与环境的持续交互**，基于奖励信号更新策略，需要处理环境的不确定性。
  - **训练过程复杂**，可能存在不稳定性，需要精心调整训练策略和超参数。

## 几种技术对比

| **比较维度**   | **SFT（有监督微调）**                            | **ReFT（强化微调）**                                         | **RLHF（基于人类反馈的强化学习）**                        | **DPO（直接偏好优化）**                                      | **PPO（近端策略优化）**                            | **RLAIF（基于 AI 反馈的强化学习）**                          | **TPO（思维偏好优化）**                                      |
| -------------- | ------------------------------------------------ | ------------------------------------------------------------ | --------------------------------------------------------- | ------------------------------------------------------------ | -------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **概念**       | 使用标注数据对预训练模型进行有监督的微调         | 在 SFT 基础上，**结合参考模型**，使用自动化评估和 DPO 方法直接优化模型 | 在 SFT 基础上，结合人类反馈和 PPO 算法进行强化学习        | 使用人类偏好数据和**参考模型**，直接优化模型参数，避免强化学习复杂性 | 一种强化学习算法，限制策略更新幅度，保持训练稳定性 | 在 SFT 基础上，使用 AI 模型的反馈，结合 PPO 算法进行强化学习 | 模型在回答前进行内部思考，结合**参考模型**和偏好数据优化模型参数 |
| **实现方法**   | 收集标注数据，最小化模型输出与目标输出之间的损失 | SFT 后，采样模型输出，利用自动化评估或偏好数据，**结合参考模型**使用 DPO 方法直接优化 | SFT 后，收集人类反馈，训练奖励模型，使用 PPO 算法优化策略 | 收集人类偏好数据，**结合参考模型**，构建损失函数，直接优化模型参数 | 与环境交互，计算优势函数，使用剪辑目标函数优化策略 | SFT 后，使用辅助 AI 模型评估，训练奖励模型，使用 PPO 优化策略 | 使用提示引导模型生成思维和回答，利用**参考模型**和偏好数据，使用 DPO 方法优化 |
| **数据需求**   | 大量高质量标注数据                               | 标注数据 + 自动化评估程序或偏好数据 + **参考模型**           | 标注数据 + 大量人类反馈 + 奖励模型                        | 大量人类偏好数据 + **参考模型**                              | 与环境交互产生的数据                               | 标注数据 + 辅助 AI 模型 + 奖励模型                           | 输入数据 + 判别模型 + **参考模型**                           |
| **人类参与**   | 高，需要人类标注数据                             | 低，无需额外人类反馈（如偏好数据来自自动化评估）             | 高，需要大量人类反馈                                      | 高，需要人类偏好数据                                         | 取决于任务，一般无需人类参与                       | 低，无需人类反馈，依赖 AI 模型                               | 低，无需人类思维标注，偏好数据可来自 AI 模型                 |
| **参考模型**   | **否**                                           | **是**，利用参考模型稳定训练                                 | **否**                                                    | **是，关键组成部分**                                         | **否**                                             | **是**，用于稳定训练和对比                                   | **是，关键组成部分**                                         |
| **思维过程**   | 否                                               | 否                                                           | 否                                                        | 否                                                           | 否                                                 | 否                                                           | **是**，模型在回答前生成思维                                 |
| **奖励机制**   | 基于损失函数，最小化预测输出与目标输出的差异     | 利用自动化评估或偏好数据，**结合参考模型构建损失函数**，直接优化 | 人类反馈训练的奖励模型评估输出，给予奖励                  | 基于人类偏好数据和**参考模型**构建损失函数，直接优化模型参数 | 环境提供奖励，计算优势函数，指导策略更新           | 辅助 AI 模型评估输出，训练奖励模型，给予奖励                 | 通过评估回答质量，利用**参考模型构建损失函数**，优化模型参数 |
| **训练复杂度** | 低                                               | 中，需结合参考模型和偏好数据进行优化，训练稳定               | 高，多阶段训练，需要人类反馈和强化学习                    | 中，避免了强化学习的复杂性，训练稳定                         | 中，需要调整超参数，训练稳定                       | 中，需要辅助 AI 模型、奖励模型和强化学习                     | 高，多次迭代训练，优化思维和回答，需考虑参考模型和偏好数据   |
| **优势**       | 简单直接，易于实现                               | 训练稳定，直接优化目标，适用于有自动化评估的任务             | 输出更符合人类期望，提高安全性和自然度                    | 训练稳定，直接优化目标，效率高，避免强化学习复杂性           | 训练稳定性高，样本效率高                           | 降低人类反馈成本，可规模化训练                               | 提升复杂任务表现，无需人类思维数据，多任务适用               |
| **缺点**       | 数据需求高，泛化能力有限                         | 依赖自动化评估或偏好数据的质量，参考模型选择需谨慎           | 人力成本高，训练复杂，需要多阶段训练                      | 依赖偏好数据质量和参考模型的设计                             | 超参数调节复杂，样本效率较低                       | 依赖辅助模型质量，可能引入偏差                               | 训练复杂度高，判别模型质量影响大，思维过程可能不可控         |
| **适用场景**   | 有大量标注数据的任务                             | 有明确评估标准，可自动计算评估或有偏好数据的任务             | 需要高质量输出，强调人类价值观和主观评价的任务            | 有大量人类偏好数据，需简化训练流程的任务                     | 强化学习任务，需与环境交互                         | 无法获得人类反馈但有可靠的 AI 模型评估的任务                 | 复杂任务，多步骤推理，缺乏人类思维标注数据的任务             |

**ReFT（Reinforced Fine-Tuning，强化微调）：**

- **组成**：ReFT = SFT + **参考模型** + DPO
- **过程**：在有监督微调（SFT）的基础上，引入**参考模型**（通常为经过 SFT 的初始模型，参数固定不更新），使用 DPO（直接偏好优化）方法，结合自动化评估或偏好数据，直接优化模型参数。
- **评估方式**：通常通过**自动化程序或偏好数据**对模型输出进行评估，利用**参考模型**和评估结果构建损失函数，直接优化模型。

 
**RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）：**

- **组成**：RLHF = SFT + PPO + 人类反馈
- **过程**：在 SFT 的基础上，使用 PPO（近端策略优化）进行强化学习，**引入奖励模型**，其奖励信号来自**人类反馈**。模型通过与环境交互，利用奖励信号更新策略。
- **评估方式**：人类对模型输出进行评价，构建偏好数据，训练**奖励模型**。在强化学习过程中，奖励模型对模型输出进行评估，提供奖励信号，指导模型优化。


**DPO 方法（Direct Preference Optimization，直接偏好优化）：**

- **组成**：DPO 方法 = SFT + **参考模型** + DPO
- **过程**：在 SFT 的基础上，引入**参考模型**（参数固定不更新），使用 DPO 方法，利用**人类偏好数据**和参考模型，直接优化模型参数。
- **评估方式**：利用人类偏好数据，结合**参考模型**，构建损失函数，直接优化模型参数，使模型倾向于生成被人类偏好的输出。


**RLAIF（Reinforcement Learning from AI Feedback，基于 AI 反馈的强化学习）：**

- **组成**：RLAIF = SFT + PPO + **AI 反馈** + 奖励模型
- **过程**：在 SFT 的基础上，使用 PPO 进行强化学习，**引入奖励模型**，其奖励信号来自**AI 模型的反馈**。模型通过与环境交互，利用 AI 模型评估的奖励信号更新策略。
- **评估方式**：辅助的 AI 模型对模型输出进行评价，生成偏好数据，训练**奖励模型**。在强化学习过程中，奖励模型对模型输出进行评估，提供奖励信号，指导模型优化。


**TPO（Thought Preference Optimization，思维偏好优化）：**

- **组成**：TPO = SFT + 思维生成（Thought Generation）+ **参考模型** + DPO
- **过程**：在 SFT 的基础上，引入**思维生成**，即模型在输出回答前生成内部的思维过程。然后，使用 DPO 方法，结合**参考模型**，直接优化模型参数，偏好数据来自**AI 判别模型的反馈**。
- **评估方式**：利用**AI 判别模型**对模型输出的**回答部分**进行评价，形成偏好对（优选和劣选）。结合**参考模型**，使用 DPO 方法构建损失函数，直接优化模型参数，提升模型性能。

