# Comparison of Various Fine-Tuning Methods

本文将会对如下微调技术进行对比：SFT、ReFT、RHLF、RLAIF、DPO、PPO、TPO。



## 几种技术之间的关系

如果把复杂的问题简单理解，这些技术之间的关系大概是：

**ReFT（Reinforced Fine-Tuning，强化微调）：**

- **组成**：ReFT = SFT + PPO
- **过程**：在有监督微调（SFT）的基础上，使用 PPO（近端策略优化）进行强化学习。
- **评估方式**：通常通过**自动化程序**对模型输出进行评估，奖励信号来自程序的评价。


**RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）：**

- **组成**：RLHF = SFT + PPO + 人类反馈
- **过程**：在 SFT 的基础上，使用 PPO 进行强化学习，奖励信号来自**人类反馈**。
- **评估方式**：人类对模型输出进行评价，或者使用基于人类反馈训练的**奖励模型**来评估。


**DPO 方法（Direct Preference Optimization，直接偏好优化）：**

- **组成**：DPO 方法 = SFT + DPO
- **过程**：在 SFT 的基础上，使用 DPO（直接偏好优化）方法直接优化模型。
- **评估方式**：利用**人类偏好数据**，构建损失函数，直接优化模型参数。


**RLAIF（Reinforcement Learning from AI Feedback，基于 AI 反馈的强化学习）：**

- **组成**：RLAIF = SFT + PPO + AI 反馈
- **过程**：在 SFT 的基础上，使用 PPO 进行强化学习，奖励信号来自**AI 模型的反馈**。
- **评估方式**：辅助的 AI 模型（可能是奖励模型）对模型输出进行评价，提供奖励信号。


**TPO（Thought Preference Optimization，思维偏好优化）：**

- **组成**：TPO = SFT + 思维生成（Thought Generation）+ DPO（偏好优化，偏好数据来自 AI 判别模型）
- **过程**：在 SFT 的基础上，引入**思维生成**，即模型在输出回答前生成内部的思维过程。然后，使用 DPO 方法直接优化模型，偏好数据来自**AI 判别模型的反馈**。
- **评估方式**：利用**AI 判别模型**对模型输出的**回答部分**进行评价，形成偏好对（优选和劣选）。根据这些偏好对，使用 DPO 方法优化模型参数，提升模型性能。



**解释：**

- ReFT（强化微调）通过在监督微调后的模型上，使用PPO算法进行强化学习，奖励信号来自于自动化程序对模型输出与标准答案的比较。

- RLHF（基于人类反馈的强化学习）在SFT基础上，使用PPO算法进行强化学习，奖励信号来自人类对模型输出的评价。

- DPO方法（直接偏好优化）在SFT基础上，使用DPO算法直接优化模型参数以符合人类偏好，不使用PPO等传统强化学习算法。

- RLAIF（基于AI反馈的强化学习）类似于RLHF，但人类反馈替换为AI模型的反馈，使用PPO算法进行强化学习。




ReFT、RLHF、DPO和RLAIF。这些方法都是在监督微调（SFT）的基础上，进一步优化模型以提高性能，但它们在优化策略和反馈来源上有所不同。

1. **ReFT（Reinforced Fine-Tuning，强化微调）**：这是SFT和PPO（近端策略优化）的结合。在第一阶段，模型通过SFT在有标注的数据上进行训练，建立基本的语言理解和生成能力。第二阶段，引入PPO算法，对模型进行强化学习优化。此时，模型的输出由自动化程序进行评估，程序根据预设的规则或标准对模型的输出进行评价，并生成奖励信号。模型根据这些奖励信号，使用PPO算法调整自身参数，以产生更优的输出。ReFT的特点是评估过程自动化，无需人类参与，适用于有明确客观标准的任务，例如数学问题求解。

2. **RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）**：在SFT的基础上，结合PPO算法，但奖励信号来自人类反馈。具体而言，人类对模型的输出进行评价，指出更优的回答，或通过偏好对比的方式提供反馈。这些人类反馈可以直接用于指导模型优化，或者用于训练一个奖励模型，后续由奖励模型对模型输出进行评估。RLHF的优势在于引入了人类的主观判断，使模型的输出更符合人类偏好，适用于需要复杂评价和主观判断的任务。

3. **DPO（Direct Preference Optimization，直接偏好优化）方法：**与前两种方法不同，DPO不用强化学习算法（如PPO），而是采用监督学习的方法直接优化模型。在SFT之后，利用人类偏好数据构建损失函数，直接对模型进行微调。具体来说，收集人类对模型输出的偏好数据，如在给定的多个回答中标注出人类更喜欢的那个。然后，设计一个损失函数，使模型倾向于生成被人类偏好的输出。通过最小化这个损失函数，直接调整模型参数。DPO避免了强化学习中的试错过程，训练更稳定，效率更高，适用于有大量人类偏好数据的场景。

4. **RLAIF（Reinforcement Learning from AI Feedback，基于AI反馈的强化学习）**：这是SFT、PPO和AI反馈的结合。在SFT后，使用PPO进行强化学习，然而奖励信号不是来自人类，而是来自辅助的AI模型（如奖励模型）的反馈。AI模型对主模型的输出进行评估，提供奖励信号。这样的方法节省了人类评价的成本，但依赖于辅助AI模型的质量。

**总结：**四种方法中，ReFT、RLHF和RLAIF都使用了PPO作为强化学习算法，区别在于奖励信号的来源不同：ReFT来自自动化程序的评估，RLHF来自人类反馈，RLAIF来自AI模型的反馈。只有DPO方法使用了监督学习的方式，不采用PPO等强化学习算法，而是直接利用人类偏好数据优化模型。



**那么，为什么DPO的意义是什么？**

强化学习方法（如PPO）需要模型在环境中自行探索，通过试错学习获得奖励信号，这个过程复杂，训练不稳定，且调参困难。相比之下，监督学习的方法更直接高效：通过人类提供的偏好数据，直接告诉模型什么是好的输出，构建损失函数，调整模型参数。这样避免了强化学习的复杂性，训练过程更稳定，效率更高，特别适用于有大量人类偏好数据的情况。

举个例子，使用强化学习的模型就像是在黑暗中摸索前进，需要不断试错；而使用监督学习的DPO方法，就像是有人直接给了一张地图，告诉你正确的前进路线。采用监督学习，可以更快地达到目标。

选择方法的依据：如果任务有明确的客观评价标准，适合使用ReFT，通过自动化程序评估模型输出。如果希望模型的输出更符合人类主观偏好，并有大量人类反馈数据，可以选择RLHF或DPO方法。RLHF使用强化学习算法，需要模型与环境交互，训练复杂；DPO则采用监督学习，训练更简单高效。若人类反馈成本高，可以考虑RLAIF，用辅助AI模型提供反馈信号。



## **ReFT简介**

先看ReFT论文中的流程图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/akGXyic486nUpgIIJic3v3UHiaRriaapjX2omaoJWqB4dr3dQyGEkDMjjR5JeI8LibRVX9icuCAiarOA0kMgPfWhoqiamg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如上图所示，ReFT，该框架结合了监督微调（Supervised Fine-Tuning, SFT）和强化微调（Reinforced Fine-Tuning, ReFT）的方法。以下是对图中各部分的详细解释：

1. **监督微调（Supervised Fine-Tuning）**：

   - **模型（Model）**：初始模型通过多个SFT周期（epochs）在训练数据上进行训练。训练数据包含问题（x），推理链（CoT，e）和答案（y）。
   - **SFT Epochs**：模型在训练数据上进行多个周期的训练，以学习如何从问题（x）和推理链（e）生成正确的答案（y）。
   - **不同阶段的模型**：图中展示了经过不同训练阶段后的模型表情变化，表示模型逐渐变得更好。

2. **强化微调（Reinforced Fine-Tuning）**：

   - **预热阶段（Warm-up）**：在进入强化学习之前，模型通过SFT进行预热。
   - **问题（question）**：模型接受一个输入问题（x）。
   - **On-Policy Sampling**：在策略内采样，模型生成一个推理链和答案（e', y'）。
   - **Golden Reward**：对生成的答案（y'）与正确答案（y）进行比较，给予奖励信号。如果答案正确，给予正奖励（√），否则给予负奖励（×）。
   - **强化学习（Reinforcement Learning）**：利用奖励信号来调整模型参数，以提高模型在相同数据上的表现。

3. **最终策略（Final Policy）**：

   - 经过SFT和ReFT训练后，模型形成最终策略，可以更准确地回答问题。

     图例说明了在GSM8K数据集上，一个问题（x）、推理链（e）和答案（y）的示例。通过多个SFT周期对训练数据进行迭代，并使用ReFT方法从SFT进行预热，然后在相同数据上进行强化学习训练。

## **ReFT第二阶段评估**

**评估过程中是否需要使用其他大模型**

在ReFT（强化微调，Reinforced Fine-Tuning）方法中，模型的评估通常通过自动化程序完成，即将模型的输出与已知的标准答案进行比较。那么，在这个评估过程中，是否需要使用其他大模型呢？

**1. 评估是否需要其他大模型**

- **无需其他大模型**：在ReFT方法的评估过程中，不需要引入其他大型语言模型来辅助。评估的核心是基于答案的正确性，将模型的输出与标准答案直接进行比较。

- **直接比较答案**：对于有标准答案的任务（如数学问题求解），可以直接将模型的输出与标准答案匹配，判断其正确性，而不需要其他模型的介入。

  **2. 第二阶段的评估由谁来做**

  在ReFT的第二阶段（强化学习阶段），评估和奖励的计算由程序自动完成：

- **正确答案**：如果模型的输出与标准答案匹配，给予正向奖励（例如，奖励值为1）。

- **错误答案**：如果不匹配，则不给予奖励或给予负向奖励。

- **部分奖励**：在某些情况下，对接近正确的答案给予部分奖励，鼓励模型朝正确方向改进。

  **评估流程如下：**

1. **模型生成解答**：模型针对每个输入问题生成解题过程（包括思维链，CoT）和最终答案。

2. **提取最终答案**：程序从模型的输出中提取最终答案部分，忽略推理过程中的无关信息。

3. **答案比较**：将提取的最终答案与预先存在的标准答案进行比较。

4. **奖励计算**：根据比较结果，自动计算奖励值，调整模型参数。

   整个评估和奖励计算过程是自动化的，无需人类干预或其他模型的参与。

   **3. 如何保证评估程序的客观性**

   为确保评估程序的客观性和公正性，需要注意以下几点：

- **明确的评估标准**：使用客观且可验证的标准，基于标准答案进行精确比较。

- **自动化的比较机制**：采用字符串匹配、数值比较等方式，自动判断模型输出与标准答案的一致性。

- **答案标准化处理**：对答案进行标准化处理，如去除空格、统一格式，确保比较的一致性。

- **考虑数值误差**：对于数值型答案，考虑允许的小误差范围，确保比较的准确性。

- **处理多样化的正确答案**：对于可能有多种正确答案的题目，评估程序应涵盖所有正确解，确保公平性。

- **避免主观因素**：评估程序只关注答案的正确性，不涉及语言风格、表达方式等主观评价。

- **统一的评估逻辑**：对所有样本应用一致的评估逻辑，避免人为偏见。

- **严格的测试和验证**：在正式使用前，充分测试评估程序，确保其在各种可能的输出情况下都能正确判断。

- **透明和可审计**：保持评估程序的透明度，代码可供审查，评估逻辑清晰明了。

- **处理异常输入**：应对边界情况和异常输入，增强程序的稳健性。

  **4. ReFT适合的微调场景和能力**

  **4.1 适合的微调场景**

- **答案可验证的任务**：输出有明确的正确答案，评估可自动完成。

- **需要复杂推理的任务**：如数学求解、逻辑推理、编程和代码生成、科学计算、定理证明等。

- **有明确评估标准的任务**：如选择题、填空题，答案可以精确匹配。

- **人类反馈成本高**：获取大量人类偏好数据成本高昂，ReFT可减少对人类反馈的依赖。

- **数据标注有限**：需要充分利用现有数据，提升模型性能。

- **增强模型泛化能力的任务**：模型可适应未见过的问题类型，探索创新解法。

  **4.2 适合微调的能力**

- **自我探索**：模型在强化学习阶段自主探索解答，增强学习能力。

- **奖励驱动学习**：通过奖励机制，模型学会识别正确的解答方向。

- **提高答案正确率**：增加模型输出正确答案的概率。

- **错误纠正能力**：模型能够从错误中学习，逐步改进。

- **处理新问题**：适应未见过的问题类型，具备更强的泛化能力。

- **创新解题方法**：探索新的解题思路，丰富解答方式。

- **多步骤推理**：提高多步骤问题上的推理深度，清晰地逐步解答。

- **逻辑严谨性**：增强推理过程的逻辑正确性，避免漏洞。

  **4.3 不太适用的场景**

- **无法自动评估的任务**：如创意写作，没有明确的评价标准。

- **多样性要求高的任务**：需要生成多样化的输出，ReFT可能限制多样性。

- **主观性强的任务**：如意见问答、情感分析，需要人类反馈和主观评价。

- **对话生成**：涉及情感、礼貌性、上下文理解等复杂因素。

  **4.4 数学问题求解的评估流程**

1. **模型生成**：针对给定的数学问题，生成详细的解题过程和最终答案。

2. **答案提取**：从模型的输出中提取最终答案，进行必要的标准化处理（如化简分数、统一单位）。

3. **答案比较**：将提取的答案与标准答案进行精确比较。

4. **奖励分配**：根据比较结果，给予相应的奖励，指导模型参数朝着生成正确答案的方向更新。

   **5. 总结**

   ReFT通过结合有监督微调和强化学习（尤其是使用PPO算法），引导模型自主探索和学习，增强了在复杂推理和多步骤任务上的能力。它适用于：

- **有明确正确答案的任务**：评估可自动化，避免主观偏见。

- **需要复杂推理的任务**：如数学、逻辑推理、代码生成等。

- **增强模型泛化和自主学习能力的场景**：适应新问题，探索创新解法。

  然而，对于需要大量人类评价、涉及主观判断或无法定义明确奖励机制的任务，ReFT可能并非最佳选择。这些情况下，可考虑其他方法，如基于人类反馈的强化学习（RLHF）或直接偏好优化（DPO）等。

  通过合理选择微调方法和评估机制，结合任务特点和需求，可最大程度提升模型性能，满足实际应用要求。

## **TPO的流程**

先看Thought Preference Optimization（TPO）的流程：

![图片](https://mmbiz.qpic.cn/mmbiz_png/akGXyic486nUpgIIJic3v3UHiaRriaapjX2obrFTylPby9mlhw3NUiaJicSIAianv6WrYjTbaCe24w2nic8xrHLOUo4VLg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- TPO 方法由三个主要部分组成：
  1. SFT（有监督微调）：提供了模型的基础。
  2. 思维生成（Thought Generation）：让模型在回答前进行内部思考。
  3. DPO（直接偏好优化）：利用 AI 判别模型生成的偏好对，直接优化模型参数。
- TPO 的创新之处在于：
  - 引入了思维生成，增强模型的推理和规划能力。
  - 利用 AI 判别模型的反馈，降低了对人类偏好数据的依赖。
  - 使用 DPO 方法，简化了训练流程，提高了训练效率。



**TPO = SFT + 思维生成（Thought Generation）+ DPO（偏好优化，偏好数据来自 AI 判别模型）。**

需要指出的是，DPO 通常直接使用人类的偏好数据。在传统的应用中，DPO 方法依赖于人类对模型输出的偏好反馈。然而，TPO 方法将 DPO 的应用扩展到了使用 AI 模型生成的偏好数据，这一创新使得模型能够在缺乏人类偏好数据的情况下，仍然利用偏好优化方法进行训练。对于 DPO 方法而言，偏好数据的来源并不是最关键的，关键在于 DPO 直接利用偏好数据（无论其来源）来优化模型，而不涉及强化学习算法。

上面TPO公式的具体解释如下：

1. SFT（Supervised Fine-Tuning，有监督微调）：
   - 起点模型（Seed Model）：TPO 方法以一个已经过有监督微调的预训练模型作为基础，例如 Llama-3-8B-Instruct。
2. 思维生成（Thought Generation）：
   - 引入思维过程：在生成最终回答之前，模型通过特定的提示（Prompt），被引导生成内部的思维过程（Thought）。
   - 思维与回答的分离：生成的输出分为思维部分和回答部分，思维部分在最终呈现给用户时被隐藏。
3. DPO（Direct Preference Optimization，直接偏好优化）：
   - 偏好数据来源：偏好对（Preference Pairs）来自于 AI 判别模型 对模型输出的评估，而非传统的人类偏好数据。
   - AI 判别模型：

​    Self-Taught Evaluator（STE）：基于大型语言模型（如 Llama-3-70B-Instruct），用于对两个回答进行比较，输出偏好结果。ArmoRM：一个奖励模型，直接对单个回答进行评分。

- 优化过程：
  - 生成候选输出：模型针对每个输入指令，生成多个包含思维和回答的候选输出。
  - 评估回答部分：将回答部分输入到 AI 判别模型，进行评分或偏好比较。
  - 构建偏好对：根据评估结果，选出最佳和最差回答，形成偏好对（Chosen 和 Rejected）。
  - 使用 DPO 优化：利用这些偏好对，使用 DPO 方法直接优化模型参数。

## **为什么TPO用的DPO不算强化学习?**

### 1. 强化学习的本质

 
强化学习就像训练一只小狗做动作。

**比方说：**

- **训练小狗**：当小狗做对了动作（比如坐下、握手），我们就给它一块小零食作为奖励；当它做错了，我们就不给奖励。

- **目的**：经过多次训练，小狗会明白，做对了就有奖励，于是它会更愿意做我们希望的动作。

  **在机器学习中：**

- **模型与环境互动**：模型（相当于小狗）在某个环境中，根据当前的情况，选择一个动作。

- **获得奖励或惩罚**：环境会根据模型的动作，给出一个奖励（奖励值可以是正的，也可以是负的）。

- **优化目标**：模型的目标是学习到一种策略，使得它在与环境的长期互动中，累积的奖励最大化。

  **简单理解：**

- **强化学习 = 试错学习 + 奖励机制**

- 模型通过不断尝试不同的动作，学习到哪些行为能够获得更多的奖励。

  **举个例子：**

- 游戏中的机器人：想象一个机器人在迷宫中寻找出口。

  - **到达出口**：奖励 +10 分。
  - **撞墙**：奖励 -1 分。
  - **其他情况**：奖励 0 分。

- 行动选择：

  - 每走一步，机器人会根据当前的位置（状态），选择向上、下、左、右移动（动作）。

- 模型目标：

  - 学会一条最优路径，快速找到出口，获得最高的累计奖励。



### 2. 偏好数据的直接优化（DPO）的本质

 
直接偏好优化（DPO）是一种直接利用偏好数据来优化模型的方法。

**比方说：**

- **模型生成两个回答**：对于同一个问题，模型生成了回答 A 和回答 B。

- 获得偏好反馈

  ：我们请人或者另一个模型告诉我们，哪个回答更好。

  - 例如，人类评估者说：“回答 A 比回答 B 好。”

- 直接优化模型：

  - 利用这个“回答 A 比回答 B 好”的信息，调整模型的参数，使其更倾向于生成类似回答 A 的内容。

    **与强化学习的区别：**

- **不需要复杂的奖励函数**：DPO 不需要设计一个数值化的奖励，只需要知道哪个更好。

- **不需要环境互动**：DPO 直接使用已有的偏好数据进行优化。

  **简单理解：**

- **DPO = 用偏好比较直接调整模型**

- 模型根据哪些输出被偏好，直接优化自身，倾向于生成更好的输出。

  **举个例子：**

- 训练聊天机器人：

  - 利用“回答 1 比回答 2 好”这条偏好信息，调整模型参数，使其更倾向于生成类似回答 1 的内容。
  - 人类评估者表示更喜欢回答 1，因为它更有礼貌、服务态度更好。
    - **回答 1**：“好的，我会帮您查询相关信息。”
    - **回答 2**：“我不知道，你自己去查吧。”

- 步骤：

  1. **模型生成两个回答**：针对用户提出的问题。
  2. **获取偏好数据**：人类评估者比较两个回答，选择更好的一个。
  3. **直接优化模型**：使用偏好信息，调整模型，使其倾向于生成被偏好的回答。

### 3. 关于 DPO 中的“交互”

 
在 DPO 的描述中，模型需要生成多个回答，然后人类评估者提供偏好反馈（**注：TPO 的 DPO 部分用的是模型**）。这似乎也存在一些交互。那么，DPO 不是也有交互吗？

DPO 的确涉及到系统与人类评估者之间的 **交互**，因为需要从人类收集偏好数据。然而，这种交互与强化学习中的交互有本质的不同。

#### 1. DPO 中的“交互”

- 数据收集阶段的交互：
  - 在 DPO 中，模型会为每个输入生成多个候选输出（如回答）。
  - 人类评估者在数据收集阶段对这些候选输出进行偏好标注，即指出哪个回答更好。
  - **特点**：这种交互是一次性的、离线的，用于构建训练数据集。
- 训练阶段的优化：
  - 一旦偏好数据收集完成，模型的训练过程就是基于这些偏好对，使用监督学习的方法直接优化模型参数。
  - **训练过程中不再需要与人类评估者或环境进行交互。**

#### 2. 强化学习中的交互

- 持续的环境交互：
  - 在强化学习（如 PPO）中，模型在训练过程中需要与环境进行持续、动态的交互。
  - 模型根据当前状态选择动作，环境反馈新的状态和奖励，模型根据这些信息更新策略。
- 训练过程的复杂性：
  - 强化学习的训练是在线的、迭代的，模型需要不断地试错来学习最优策略。
  - 涉及到 **状态、动作、奖励、价值函数、策略更新** 等复杂概念。

#### 3. 两者的区别

- 交互的性质和阶段不同：
  - **DPO** 的交互发生在训练前的数据收集阶段，是 **静态的、离线的**。
  - **强化学习** 的交互发生在训练过程中，是 **动态的、在线的**。
- 训练方法不同：
  - **DPO** 使用监督学习方法，直接利用偏好对构建损失函数，优化模型参数。
  - **强化学习** 需要策略梯度或价值函数等方法，根据环境反馈的奖励信号更新策略。
- 复杂性和稳定性：
  - **DPO** 的训练过程相对简单、稳定，因为它是监督学习，不涉及环境的不确定性。
  - **强化学习** 的训练过程更复杂，可能存在不稳定性，需要精心调参。

#### 4. 举个例子


**DPO 的例子：**

- 步骤 1：数据收集

  - 模型生成两个回答：
    - **回答 1**：“好的，我会帮您查询相关信息。”
    - **回答 2**：“我不知道，你自己去查吧。”
  - **人类评估者对回答进行比较，偏好回答 1。**

- 步骤 2：模型优化

  - 使用这些偏好对，直接构建损失函数，优化模型，使其倾向于生成类似回答 1 的输出。

- 特点：

  - **数据收集和模型训练是分开的。**

  - **训练过程中不需要与人类或环境交互。**

    **强化学习的例子：**

- 模型与环境持续交互：

  - 模型在训练过程中，不断与环境交互，选择动作，观察反馈，获取奖励。
  - 例如，**训练一个游戏 AI**，模型需要在游戏环境中不断尝试行动，根据得分（奖励）更新策略。

- 特点：

  - **训练过程需要持续的、在线的交互。**
  - **模型的行为会影响后续的状态和奖励，训练过程更加复杂。**

### 5. 总结

- 直接偏好优化（DPO）：
  - **交互发生在数据收集阶段**，是离线的、静态的。
  - **训练过程中使用监督学习方法**，直接利用偏好数据优化模型。
  - **训练过程简单、稳定**，不涉及环境交互。
- 强化学习：
  - **交互发生在训练过程中**，是在线的、动态的。
  - **模型通过与环境的持续交互**，基于奖励信号更新策略。
  - **训练过程复杂**，需要处理环境的不确定性，可能不稳定。



## 几种技术对比

| 比较维度       | SFT（有监督微调）                                | ReFT（强化微调）                                             | RLHF（基于人类反馈的强化学习）                        | DPO（直接偏好优化）                                    | PPO（近端策略优化）                                | RLAIF（基于 AI 反馈的强化学习）                              | TPO（思维偏好优化）                                        |
| -------------- | ------------------------------------------------ | ------------------------------------------------------------ | ----------------------------------------------------- | ------------------------------------------------------ | -------------------------------------------------- | ------------------------------------------------------------ | ---------------------------------------------------------- |
| **概念**       | 使用标注数据对预训练模型进行有监督的微调         | 在 SFT 基础上，使用 PPO 算法，通过自动化奖励信号进行强化学习 | 在 SFT 基础上，结合人类反馈和 PPO 算法进行强化学习    | 使用人类偏好数据，直接优化模型参数，避免强化学习复杂性 | 一种强化学习算法，限制策略更新幅度，保持训练稳定性 | 在 SFT 基础上，使用 AI 模型的反馈，结合 PPO 算法进行强化学习 | 模型在回答前进行内部思考，通过偏好优化提升性能             |
| **实现方法**   | 收集标注数据，最小化模型输出与目标输出之间的损失 | SFT 后，采样模型输出，用自动化程序评估，使用 PPO 优化策略    | SFT 后，收集人类反馈，训练奖励模型，使用 PPO 优化策略 | 收集人类偏好数据，构建损失函数，直接优化模型参数       | 与环境交互，计算优势函数，使用剪辑目标函数优化策略 | SFT 后，使用辅助 AI 模型评估，使用 PPO 优化策略              | 使用提示引导模型生成思维和回答，评估回答部分，优化模型参数 |
| **数据需求**   | 大量高质量标注数据                               | 标注数据 + 自动化评估程序                                    | 标注数据 + 大量人类反馈                               | 大量人类偏好数据                                       | 与环境交互产生的数据                               | 标注数据 + 辅助 AI 模型                                      | 输入数据 + 判别模型                                        |
| **人类参与**   | 高，需要人类标注数据                             | 低，无需额外人类反馈                                         | 高，需要大量人类反馈                                  | 高，需要人类偏好数据                                   | 取决于任务，一般无需人类参与                       | 低，无需人类反馈，依赖 AI 模型                               | 低，无需人类思维标注                                       |
| **思维过程**   | 否                                               | 否                                                           | 否                                                    | 否                                                     | 否                                                 | 否                                                           | **是**，模型在回答前生成思维                               |
| **奖励机制**   | 基于损失函数，最小化预测输出与目标输出的差异     | 自动化程序评估输出正确性，给予奖励或惩罚                     | 人类反馈训练的奖励模型评估输出，给予奖励              | 基于人类偏好数据构建损失函数，直接优化                 | 环境提供奖励，计算优势函数，指导策略更新           | 辅助 AI 模型评估输出，给予奖励                               | 通过评估回答质量，间接优化思维过程                         |
| **训练复杂度** | 低                                               | 高，需要实现强化学习算法，训练复杂                           | 高，多阶段训练，需要人类反馈和强化学习                | 中，避免了强化学习的复杂性                             | 中，需要调整超参数，训练稳定                       | 中，需要辅助 AI 模型和强化学习                               | 高，多次迭代训练，优化思维和回答                           |
| **优势**       | 简单直接，易于实现                               | 增强模型自主学习和推理能力，适合复杂任务                     | 输出更符合人类期望，提高安全性和自然度                | 训练稳定，直接优化目标，效率高                         | 训练稳定性高，样本效率高                           | 降低人类反馈成本，可规模化训练                               | 提升复杂任务表现，无需人类思维数据，多任务适用             |
| **缺点**       | 数据需求高，泛化能力有限                         | 训练复杂度高，奖励设计困难                                   | 人力成本高，训练复杂                                  | 依赖偏好数据质量，可能存在局部最优                     | 超参数调节复杂，样本效率较低                       | 依赖辅助模型质量，可能引入偏差                               | 训练复杂度高，判别模型质量影响大，思维过程不可控           |
| **适用场景**   | 有大量标注数据的任务                             | 有明确评估标准，可自动计算奖励的复杂推理任务                 | 需要高质量输出，强调人类价值观的任务                  | 有大量人类偏好数据，需简化训练流程的任务               | 强化学习任务，需与环境交互                         | 无人类反馈，辅助模型可靠的任务                               | 复杂任务，多步骤推理，缺乏思维标注数据的任务               |

**ReFT（Reinforced Fine-Tuning，强化微调）：**

- **组成**：ReFT = SFT + PPO
- **过程**：在有监督微调（SFT）的基础上，使用 PPO（近端策略优化）进行强化学习。
- **评估方式**：通常通过**自动化程序**对模型输出进行评估，奖励信号来自程序的评价。


**RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）：**

- **组成**：RLHF = SFT + PPO + 人类反馈
- **过程**：在 SFT 的基础上，使用 PPO 进行强化学习，奖励信号来自**人类反馈**。
- **评估方式**：人类对模型输出进行评价，或者使用基于人类反馈训练的**奖励模型**来评估。


**DPO 方法（Direct Preference Optimization，直接偏好优化）：**

- **组成**：DPO 方法 = SFT + DPO
- **过程**：在 SFT 的基础上，使用 DPO（直接偏好优化）方法直接优化模型。
- **评估方式**：利用**人类偏好数据**，构建损失函数，直接优化模型参数。


**RLAIF（Reinforcement Learning from AI Feedback，基于 AI 反馈的强化学习）：**

- **组成**：RLAIF = SFT + PPO + AI 反馈
- **过程**：在 SFT 的基础上，使用 PPO 进行强化学习，奖励信号来自**AI 模型的反馈**。
- **评估方式**：辅助的 AI 模型（可能是奖励模型）对模型输出进行评价，提供奖励信号。


**TPO（Thought Preference Optimization，思维偏好优化）：**

- **组成**：TPO = SFT + 思维生成（Thought Generation）+ DPO（偏好优化，偏好数据来自 AI 判别模型）
- **过程**：在 SFT 的基础上，引入**思维生成**，即模型在输出回答前生成内部的思维过程。然后，使用 DPO 方法直接优化模型，偏好数据来自**AI 判别模型的反馈**。
- **评估方式**：利用**AI 判别模型**对模型输出的**回答部分**进行评价，形成偏好对（优选和劣选）。根据这些偏好对，使用 DPO 方法优化模型参数，提升模型性能。
